{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing rows:   2%|▏         | 21/1000 [1:46:49<83:00:15, 305.23s/it]\n",
      "Processing rows:  14%|█▍        | 142/1000 [2:01:11<14:42:00, 61.68s/it]HTTP Error 504 thrown while requesting GET https://huggingface.co/datasets/howey/unarXive/resolve/daf6d547b5290c0f783a6bc8573078c0a60771cd/data/train-00084-of-00198-2667d4e6238c850b.parquet\n",
      "Retrying in 1s [Retry 1/5].\n",
      "Processing rows:  36%|███▌      | 355/1000 [5:28:45<9:57:18, 55.56s/it] \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Initialize an empty DataFrame to store the filtered rows\n",
    "filtered_df = pd.DataFrame(columns=[\"id\", \"text\"])\n",
    "\n",
    "\n",
    "# Function to check if both keywords are in the abstract\n",
    "def contains_keywords(text):\n",
    "    if isinstance(text, list) and len(text) > 1:\n",
    "        abstract = \" \".join(text[1]).lower()\n",
    "        return \"natural language processing\" in abstract and \"translation\" in abstract\n",
    "    return False\n",
    "\n",
    "\n",
    "# Load the dataset in streaming mode\n",
    "dataset = load_dataset(\"howey/unarXive\", split=\"train\", streaming=True)\n",
    "\n",
    "# Initialize a counter for the total number of rows\n",
    "total_rows = 0\n",
    "\n",
    "# Initialize tqdm progress bar\n",
    "progress_bar = tqdm(total=1000, desc=\"Processing rows\")\n",
    "\n",
    "# Buffer to store rows temporarily\n",
    "buffer = []\n",
    "\n",
    "# Iterate over the dataset in chunks\n",
    "for example in dataset:\n",
    "    buffer.append(example)\n",
    "\n",
    "    # Process the buffer when it reaches 100 rows\n",
    "    if len(buffer) >= 100000:\n",
    "        # Convert the buffer to a DataFrame\n",
    "        buffer_df = pd.DataFrame(buffer)\n",
    "\n",
    "        # Filter the rows based on the criteria\n",
    "        filtered_chunk = buffer_df[buffer_df[\"text\"].apply(contains_keywords)]\n",
    "\n",
    "        # Append the filtered rows to the DataFrame\n",
    "        filtered_df = pd.concat([filtered_df, filtered_chunk], ignore_index=True)\n",
    "\n",
    "        # Update the total number of rows\n",
    "        total_rows += len(filtered_chunk)\n",
    "\n",
    "        # Update the progress bar\n",
    "        progress_bar.update(len(filtered_chunk))\n",
    "\n",
    "        # Clear the buffer\n",
    "        buffer = []\n",
    "\n",
    "        # Break the loop if we have 1000 rows\n",
    "        if total_rows >= 1000:\n",
    "            break\n",
    "\n",
    "# Process any remaining rows in the buffer\n",
    "if buffer:\n",
    "    buffer_df = pd.DataFrame(buffer)\n",
    "    filtered_chunk = buffer_df[buffer_df[\"text\"].apply(contains_keywords)]\n",
    "    filtered_df = pd.concat([filtered_df, filtered_chunk], ignore_index=True)\n",
    "    total_rows += len(filtered_chunk)\n",
    "    progress_bar.update(len(filtered_chunk))\n",
    "\n",
    "# Ensure the DataFrame has exactly 1000 rows\n",
    "filtered_df = filtered_df.head(1000)\n",
    "\n",
    "# Close the progress bar\n",
    "progress_bar.close()\n",
    "\n",
    "filtered_df.to_parquet(\"filtered_translation_data.parquet\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "streem-wind-prod-forecast-jFLq_jOy-py3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
