{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from sklearn.metrics import classification_report\n",
    "from helper_functions import calculate_results\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from keras.models import Sequential\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_papers = pd.read_parquet(\"./data/cleaned_df_arxiv.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.config.set_visible_devices([], \"GPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 ['0' '1']\n",
      "24.318397827562798\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-08 15:33:23.792043: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text:\n",
      "These results show that coevolution between class I and TAP genes can explain the presence of a single dominantly expressed class I molecule in common chicken MHC haplotypes.\n",
      "\n",
      "Length of text: 28\n",
      "\n",
      "Vectorized text:\n",
      "[[  23   34   65    8 5452   27  392  298    4 4190  101   48 1856    2\n",
      "   236    3    7  174 3715  243  392  298 1301    5  215 1042  730 1008\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0]]\n",
      "Number of words in vocabulary: 11218\n",
      "Most common words in the vocabulary: ['', '[UNK]', 'the', 'of', 'and']\n",
      "Least common words in the vocabulary: ['0027', '0021', '001p009', '00174', '0006']\n",
      "Sentence before Vectorization : \n",
      "These results show that coevolution between class I and TAP genes can explain the presence of a single dominantly expressed class I molecule in common chicken MHC haplotypes.\n",
      "\n",
      "Sentence After vectorization :\n",
      " [[  23   34   65    8 5452   27  392  298    4 4190  101   48 1856    2\n",
      "   236    3    7  174 3715  243  392  298 1301    5  215 1042  730 1008\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0]]\n",
      "\n",
      "Embedding Sentence :\n",
      "[[[ 0.01901401 -0.00968255 -0.03151058 ... -0.00212149 -0.04945121\n",
      "   -0.04761567]\n",
      "  [-0.03568133  0.01109082  0.04481303 ...  0.00399438  0.03392669\n",
      "    0.00172525]\n",
      "  [-0.03797189 -0.02380383  0.04575273 ... -0.010275   -0.00779869\n",
      "    0.02616126]\n",
      "  ...\n",
      "  [-0.02012571  0.00634167 -0.01339721 ... -0.00010381  0.01883029\n",
      "   -0.03618325]\n",
      "  [-0.02012571  0.00634167 -0.01339721 ... -0.00010381  0.01883029\n",
      "   -0.03618325]\n",
      "  [-0.02012571  0.00634167 -0.01339721 ... -0.00010381  0.01883029\n",
      "   -0.03618325]]]\n",
      "\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 1)]               0         \n",
      "                                                                 \n",
      " text_vectorization (TextVec  (None, 55)               0         \n",
      " torization)                                                     \n",
      "                                                                 \n",
      " embedding (Embedding)       (None, 55, 128)           1435904   \n",
      "                                                                 \n",
      " conv1d (Conv1D)             (None, 55, 64)            41024     \n",
      "                                                                 \n",
      " global_max_pooling1d (Globa  (None, 64)               0         \n",
      " lMaxPooling1D)                                                  \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 64)                0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 2)                 130       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,477,058\n",
      "Trainable params: 1,477,058\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/15\n",
      "9/9 [==============================] - 1s 37ms/step - loss: 1.4003 - accuracy: 0.7326 - val_loss: 1.2238 - val_accuracy: 0.8194\n",
      "Epoch 2/15\n",
      "9/9 [==============================] - 0s 16ms/step - loss: 1.0949 - accuracy: 0.8194 - val_loss: 0.9616 - val_accuracy: 0.8194\n",
      "Epoch 3/15\n",
      "9/9 [==============================] - 0s 18ms/step - loss: 0.9143 - accuracy: 0.8021 - val_loss: 0.8157 - val_accuracy: 0.8194\n",
      "Epoch 4/15\n",
      "9/9 [==============================] - 0s 15ms/step - loss: 0.8204 - accuracy: 0.7847 - val_loss: 0.7123 - val_accuracy: 0.8194\n",
      "Epoch 5/15\n",
      "9/9 [==============================] - 0s 16ms/step - loss: 0.6686 - accuracy: 0.8229 - val_loss: 0.6381 - val_accuracy: 0.8194\n",
      "Epoch 6/15\n",
      "9/9 [==============================] - 0s 15ms/step - loss: 0.6631 - accuracy: 0.7847 - val_loss: 0.5851 - val_accuracy: 0.8194\n",
      "Epoch 7/15\n",
      "9/9 [==============================] - 0s 15ms/step - loss: 0.5804 - accuracy: 0.8090 - val_loss: 0.5485 - val_accuracy: 0.8194\n",
      "Epoch 8/15\n",
      "9/9 [==============================] - 0s 17ms/step - loss: 0.5625 - accuracy: 0.7951 - val_loss: 0.5227 - val_accuracy: 0.8194\n",
      "Epoch 9/15\n",
      "9/9 [==============================] - 0s 16ms/step - loss: 0.4625 - accuracy: 0.8576 - val_loss: 0.4992 - val_accuracy: 0.8194\n",
      "Epoch 10/15\n",
      "9/9 [==============================] - 0s 14ms/step - loss: 0.5570 - accuracy: 0.7743 - val_loss: 0.4833 - val_accuracy: 0.8194\n",
      "Epoch 11/15\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.5894 - accuracy: 0.6875WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 135 batches). You may need to use the repeat() function when building your dataset.\n",
      "9/9 [==============================] - 0s 14ms/step - loss: 0.5390 - accuracy: 0.7273 - val_loss: 0.4932 - val_accuracy: 0.8212\n"
     ]
    }
   ],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "one_hot_encoder = OneHotEncoder(sparse=False)\n",
    "\n",
    "claim_data_dir = \"./Claim-Extraction-dataset/\"\n",
    "claim_filenames = [claim_data_dir + filename for filename in os.listdir(claim_data_dir)]\n",
    "\n",
    "claim_train_data = pd.read_json(claim_filenames[1], lines=True)\n",
    "claim_test_data = pd.read_json(claim_filenames[0], lines=True)\n",
    "claim_val_data = pd.read_json(claim_filenames[2], lines=True)\n",
    "\n",
    "\n",
    "def sentences_label(claim_train_data):\n",
    "    ID = []\n",
    "    sentences = []\n",
    "    labels = []\n",
    "    list_df = []\n",
    "    claim_sentences = claim_train_data[\"sentences\"]\n",
    "    claim_labels = claim_train_data[\"labels\"]\n",
    "    paper_id = claim_train_data[\"paper_id\"]\n",
    "    for i in range(len(claim_sentences)):\n",
    "        for j in range(len(claim_sentences[i])):\n",
    "            sentences.append(claim_sentences[i][j])\n",
    "            labels.append(claim_labels[i][j])\n",
    "            ID.append(paper_id[i])\n",
    "            list_df.append([paper_id[i], claim_labels[i][j], claim_sentences[i][j]])\n",
    "    df = pd.DataFrame(list_df, columns=[\"Paper Ids\", \"Label\", \"Sentences\"])\n",
    "    return df\n",
    "\n",
    "\n",
    "claim_train_sentences_df = sentences_label(claim_train_data)\n",
    "claim_train_sentences = claim_train_sentences_df[\"Sentences\"].tolist()\n",
    "claim_train_labels_one_hot = one_hot_encoder.fit_transform(\n",
    "    claim_train_sentences_df[\"Label\"].to_numpy().reshape(-1, 1)\n",
    ")\n",
    "claim_label_encoder = LabelEncoder()\n",
    "claim_train_labels_encoded = claim_label_encoder.fit_transform(\n",
    "    claim_train_sentences_df[\"Label\"].to_numpy()\n",
    ")\n",
    "\n",
    "claim_test_sentences_df = sentences_label(claim_test_data)\n",
    "claim_test_sentences = claim_test_sentences_df[\"Sentences\"].tolist()\n",
    "claim_test_labels_one_hot = one_hot_encoder.fit_transform(\n",
    "    claim_test_sentences_df[\"Label\"].to_numpy().reshape(-1, 1)\n",
    ")\n",
    "claim_test_labels_encoded = claim_label_encoder.fit_transform(\n",
    "    claim_test_sentences_df[\"Label\"].to_numpy()\n",
    ")\n",
    "\n",
    "claim_val_sentences_df = sentences_label(claim_val_data)\n",
    "claim_val_sentences = claim_val_sentences_df[\"Sentences\"].tolist()\n",
    "claim_val_labels_one_hot = one_hot_encoder.fit_transform(\n",
    "    claim_val_sentences_df[\"Label\"].to_numpy().reshape(-1, 1)\n",
    ")\n",
    "claim_val_labels_encoded = claim_label_encoder.fit_transform(\n",
    "    claim_val_sentences_df[\"Label\"].to_numpy()\n",
    ")\n",
    "\n",
    "\n",
    "# Get class names and number of classes from LabelEncoder instance\n",
    "num_classes = len(claim_label_encoder.classes_)\n",
    "class_names = claim_label_encoder.classes_\n",
    "print(num_classes, class_names)\n",
    "\n",
    "sen_len = [len(sentences.split()) for sentences in claim_train_sentences]\n",
    "avg_sen_len = np.mean(sen_len)\n",
    "print(avg_sen_len)\n",
    "\n",
    "\n",
    "def count_words(claim_train_sentences):\n",
    "    total = []\n",
    "    total_unique = []\n",
    "    for i in range(len(claim_train_sentences)):\n",
    "        text_tokens = word_tokenize(claim_train_sentences[i])\n",
    "        # tokens_without_sw = [word for word in text_tokens if not word in stopwords.words()]\n",
    "        total.append(text_tokens)\n",
    "    for i in range(len(total)):\n",
    "        for j in range(len(total[i])):\n",
    "            total_unique.append(total[i][j])\n",
    "    words = list(set((total_unique)))\n",
    "    n_words = len(words)\n",
    "    return n_words\n",
    "\n",
    "\n",
    "claim_max_token = count_words(claim_train_sentences)\n",
    "\n",
    "claim_text_vectorizer = TextVectorization(\n",
    "    max_tokens=claim_max_token,\n",
    "    standardize=\"lower_and_strip_punctuation\",\n",
    "    output_sequence_length=55,\n",
    ")\n",
    "# Adapt text vectorizer to training sentences\n",
    "claim_text_vectorizer.adapt(claim_train_sentences)\n",
    "\n",
    "# viewing vectorize training sentences\n",
    "target_sentence = random.choice(claim_train_sentences)\n",
    "print(f\"Text:\\n{target_sentence}\")\n",
    "print(f\"\\nLength of text: {len(target_sentence.split())}\")\n",
    "print(f\"\\nVectorized text:\\n{claim_text_vectorizer([target_sentence])}\")\n",
    "\n",
    "\n",
    "# Getting the vocabulary and showing most frequent and least frequest words in the vocabulary\n",
    "claim_text_vocab = claim_text_vectorizer.get_vocabulary()\n",
    "most_common = claim_text_vocab[:5]\n",
    "least_common = claim_text_vocab[-5:]\n",
    "print(f\"Number of words in vocabulary: {len(claim_text_vocab)}\"),\n",
    "print(f\"Most common words in the vocabulary: {most_common}\")\n",
    "print(f\"Least common words in the vocabulary: {least_common}\")\n",
    "\n",
    "# Get the config of our text vectorizer\n",
    "claim_text_vectorizer.get_config()\n",
    "\n",
    "token_embed = layers.Embedding(\n",
    "    input_dim=len(claim_text_vocab), output_dim=128, mask_zero=True, input_length=55\n",
    ")\n",
    "\n",
    "\n",
    "print(f\"Sentence before Vectorization : \\n{target_sentence}\\n\")\n",
    "vec_sentence = claim_text_vectorizer([target_sentence])\n",
    "print(f\"Sentence After vectorization :\\n {vec_sentence}\\n\")\n",
    "embed_sentence = token_embed(vec_sentence)\n",
    "print(f\"Embedding Sentence :\\n{embed_sentence}\\n\")\n",
    "\n",
    "# Turn our data into TensorFlow Dataset\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(\n",
    "    (claim_train_sentences, claim_train_labels_one_hot)\n",
    ")\n",
    "valid_dataset = tf.data.Dataset.from_tensor_slices(\n",
    "    (claim_val_sentences, claim_val_labels_one_hot)\n",
    ")\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices(\n",
    "    (claim_test_sentences, claim_test_labels_one_hot)\n",
    ")\n",
    "\n",
    "# Take the TensorSliceDataset's and turn them into prefetched batches\n",
    "train_dataset = train_dataset.batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "valid_dataset = valid_dataset.batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "test_dataset = test_dataset.batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "inputs = layers.Input(shape=(1,), dtype=tf.string)\n",
    "text_vector = claim_text_vectorizer(inputs)\n",
    "embed = token_embed(text_vector)\n",
    "x = layers.Conv1D(\n",
    "    filters=64,\n",
    "    kernel_size=5,\n",
    "    padding=\"same\",\n",
    "    activation=\"relu\",\n",
    "    kernel_regularizer=tf.keras.regularizers.L2(0.01),\n",
    ")(embed)\n",
    "x = layers.GlobalMaxPool1D()(x)\n",
    "x = layers.Dropout(0.2)(x)\n",
    "outputs = layers.Dense(num_classes, activation=\"softmax\")(x)\n",
    "\n",
    "model = tf.keras.Model(inputs, outputs)\n",
    "model.summary()\n",
    "\n",
    "model.compile(optimizer=\"Adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "model_1_history = model.fit(\n",
    "    train_dataset,\n",
    "    steps_per_epoch=int(0.1 * len(train_dataset)),\n",
    "    epochs=15,\n",
    "    validation_data=valid_dataset,\n",
    "    validation_steps=int(0.1 * len(valid_dataset)),\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "streem-wind-prod-forecast-jFLq_jOy-py3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
