{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.0.2 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"<frozen runpy>\", line 189, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 148, in _get_module_details\n",
      "  File \"<frozen runpy>\", line 112, in _get_module_details\n",
      "  File \"/Users/omar/opt/anaconda3/envs/triplets/lib/python3.11/site-packages/spacy/__init__.py\", line 6, in <module>\n",
      "    from .errors import setup_default_warnings\n",
      "  File \"/Users/omar/opt/anaconda3/envs/triplets/lib/python3.11/site-packages/spacy/errors.py\", line 3, in <module>\n",
      "    from .compat import Literal\n",
      "  File \"/Users/omar/opt/anaconda3/envs/triplets/lib/python3.11/site-packages/spacy/compat.py\", line 4, in <module>\n",
      "    from thinc.util import copy_array\n",
      "  File \"/Users/omar/opt/anaconda3/envs/triplets/lib/python3.11/site-packages/thinc/__init__.py\", line 5, in <module>\n",
      "    from .config import registry\n",
      "  File \"/Users/omar/opt/anaconda3/envs/triplets/lib/python3.11/site-packages/thinc/config.py\", line 5, in <module>\n",
      "    from .types import Decorator\n",
      "  File \"/Users/omar/opt/anaconda3/envs/triplets/lib/python3.11/site-packages/thinc/types.py\", line 25, in <module>\n",
      "    from .compat import cupy, has_cupy\n",
      "  File \"/Users/omar/opt/anaconda3/envs/triplets/lib/python3.11/site-packages/thinc/compat.py\", line 35, in <module>\n",
      "    import torch\n",
      "  File \"/Users/omar/opt/anaconda3/envs/triplets/lib/python3.11/site-packages/torch/__init__.py\", line 1477, in <module>\n",
      "    from .functional import *  # noqa: F403\n",
      "  File \"/Users/omar/opt/anaconda3/envs/triplets/lib/python3.11/site-packages/torch/functional.py\", line 9, in <module>\n",
      "    import torch.nn.functional as F\n",
      "  File \"/Users/omar/opt/anaconda3/envs/triplets/lib/python3.11/site-packages/torch/nn/__init__.py\", line 1, in <module>\n",
      "    from .modules import *  # noqa: F403\n",
      "  File \"/Users/omar/opt/anaconda3/envs/triplets/lib/python3.11/site-packages/torch/nn/modules/__init__.py\", line 35, in <module>\n",
      "    from .transformer import TransformerEncoder, TransformerDecoder, \\\n",
      "  File \"/Users/omar/opt/anaconda3/envs/triplets/lib/python3.11/site-packages/torch/nn/modules/transformer.py\", line 20, in <module>\n",
      "    device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n",
      "/Users/omar/opt/anaconda3/envs/triplets/lib/python3.11/site-packages/torch/nn/modules/transformer.py:20: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_numpy.cpp:84.)\n",
      "  device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n",
      "Collecting en-core-web-lg==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-3.8.0/en_core_web_lg-3.8.0-py3-none-any.whl (400.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m400.7/400.7 MB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: en-core-web-lg\n",
      "Successfully installed en-core-web-lg-3.8.0\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_lg')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "\n",
    "def get_logger(log_name, log_file=\"processing_log.txt\"):\n",
    "    \"\"\"Create and return a logger that writes to a log file.\n",
    "\n",
    "    Parameters:\n",
    "        log_name (str): Name of the logger (e.g., \"citations\", \"abbreviations\", \"coref\").\n",
    "        log_file (str): Path to the log file. Default is \"processing_log.txt\".\n",
    "\n",
    "    Returns:\n",
    "        logging.Logger: Configured logger for writing logs.\n",
    "    \"\"\"\n",
    "    logger = logging.getLogger(log_name)\n",
    "    logger.setLevel(logging.DEBUG)\n",
    "\n",
    "    # Create file handler to log to a file\n",
    "    file_handler = logging.FileHandler(log_file)\n",
    "    file_handler.setLevel(logging.DEBUG)\n",
    "\n",
    "    # Create formatter and add it to the handler\n",
    "    formatter = logging.Formatter(\n",
    "        \"%(asctime)s - %(name)s - %(levelname)s - %(message)s\"\n",
    "    )\n",
    "    file_handler.setFormatter(formatter)\n",
    "\n",
    "    # Add the file handler to the logger\n",
    "    logger.addHandler(file_handler)\n",
    "\n",
    "    return logger\n",
    "\n",
    "\n",
    "def write_log(\n",
    "    old_text, new_text, line_count, start_positions, end_positions, action, logger\n",
    "):\n",
    "    \"\"\"Write a log entry indicating a specific text transformation.\n",
    "\n",
    "    Parameters:\n",
    "        old_text (str): Original text before transformation.\n",
    "        new_text (str): Transformed text.\n",
    "        line_count (int): Line number where the change occurred.\n",
    "        start_positions (list): List of start positions for the changes.\n",
    "        end_positions (list): List of end positions for the changes.\n",
    "        action (str): Description of the action performed (e.g., \"Removing citations\").\n",
    "        logger (logging.Logger): The logger to write the log entry.\n",
    "\n",
    "    \"\"\"\n",
    "    logger.info(f\"Action: {action}\")\n",
    "    logger.info(f\"Line: {line_count}\")\n",
    "    logger.info(f\"Old Text: {old_text.strip()}\")\n",
    "    logger.info(f\"New Text: {new_text.strip()}\")\n",
    "    logger.info(f\"Start Positions: {start_positions}\")\n",
    "    logger.info(f\"End Positions: {end_positions}\")\n",
    "    logger.info(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11/09/2024 20:34:36 - INFO - \t missing_keys: []\n",
      "11/09/2024 20:34:36 - INFO - \t unexpected_keys: []\n",
      "11/09/2024 20:34:36 - INFO - \t mismatched_keys: []\n",
      "11/09/2024 20:34:36 - INFO - \t error_msgs: []\n",
      "11/09/2024 20:34:36 - INFO - \t Model Parameters: 90.5M, Transformer: 82.1M, Coref head: 8.4M\n"
     ]
    }
   ],
   "source": [
    "from fastcoref import spacy_component\n",
    "from abbreviations import schwartz_hearst\n",
    "\n",
    "import re\n",
    "import time\n",
    "import tqdm\n",
    "import spacy\n",
    "import pandas as pd\n",
    "from spacy.tokens import Doc\n",
    "\n",
    "spacy.prefer_gpu()\n",
    "\n",
    "# Load spacy pipeline\n",
    "nlp = spacy.load(\"en_core_web_lg\", exclude=[\"parser\", \"ner\", \"lemmatizer\", \"textcat\"])\n",
    "nlp.add_pipe(\"fastcoref\")\n",
    "\n",
    "\n",
    "def remove_citations(texts, logger_citations):\n",
    "    \"\"\"Remove citations through a rule based heuristic\n",
    "\n",
    "    Parameters:\n",
    "        texts (list[str]): list of texts\n",
    "        log_file (str): path to the log file\n",
    "    Return:\n",
    "        new_texts (list[str]): list of texts with the citations removed\n",
    "\n",
    "    \"\"\"\n",
    "    new_texts = []\n",
    "    for text in texts:\n",
    "        new_text = \"\"\n",
    "        line_count = 0\n",
    "        for line in text.split(\"\\n\"):\n",
    "            # Brackets with only a number inside are removed\n",
    "            # Brackets with a year inside are removed\n",
    "            # Brackets with a number inside and other text, e.g. [llm2], are not removed\n",
    "            re_expression = \"\\[[0-9]{4}[a-zA-Z0-9 .,!/\\-\\\"']*\\]|\\[[0-9]+\\]|\\[[a-zA-Z0-9 .,!/\\-\\\"']*[0-9]{4}\\]|\\([a-zA-Z0-9 .,!/\\-\\\"']*[0-9]{4}\\)|\\([0-9]{4}[a-zA-Z0-9 .,!/\\-\\\"']*\\)|\\([0-9]+\\)\"\n",
    "            if re.search(re_expression, line):\n",
    "                # get starting and ending position of citation. If there are multiple citations in one line, store starting and ending position of each in a list\n",
    "                new_line = re.sub(re_expression, \"\", line)\n",
    "                start_pos, end_pos = [], []\n",
    "                for match in re.finditer(re_expression, line):\n",
    "                    start_pos.append(match.start())\n",
    "                    end_pos.append(match.end())\n",
    "\n",
    "                write_log(\n",
    "                    line,\n",
    "                    new_line,\n",
    "                    line_count,\n",
    "                    start_pos,\n",
    "                    end_pos,\n",
    "                    \"Removing citations\",\n",
    "                    logger_citations,\n",
    "                )\n",
    "            else:\n",
    "                new_line = line\n",
    "            line_count += 1\n",
    "            new_text += new_line + \"\\n\"\n",
    "        new_texts.append(new_text)\n",
    "    return new_texts\n",
    "\n",
    "\n",
    "def expand_abbreviations(texts, logger_abbr):\n",
    "    \"\"\"Expand the abbreviations using the Schwartz-Hearst algorithm\n",
    "\n",
    "    Parameters:\n",
    "        texts (list[str]): list of texts\n",
    "        log_file (str): path to the log file\n",
    "\n",
    "    Return:\n",
    "        new_texts (list[str]): list of texts with the abbreviations expanded\n",
    "        pairs (dict): dictionary with the abbreviations as keys and the definitions as values\n",
    "    \"\"\"\n",
    "\n",
    "    new_texts = []\n",
    "    errors_with_abbreviations = set()\n",
    "    for text in texts:\n",
    "        pairs = schwartz_hearst.extract_abbreviation_definition_pairs(doc_text=text)\n",
    "        # Add the fully lowercased versions of the abbreviations as keys\n",
    "        pairs_copy = pairs.copy()\n",
    "        for abbrev, definition in pairs_copy.items():\n",
    "            if abbrev.lower() != abbrev:\n",
    "                pairs[abbrev.lower()] = definition\n",
    "        # iterate over the lines in the text file and replace the abbreviations\n",
    "        # split by \\n to get the lines\n",
    "\n",
    "        sentences = text.split(\"\\n\")\n",
    "        new_sentences = []\n",
    "        for i, sentence in enumerate(sentences):\n",
    "            old_sentence = sentence\n",
    "            start_pos, end_pos = [], []\n",
    "            replacements = []\n",
    "            for abbrev, definition in pairs.items():\n",
    "                # check whether the abbreviation is in the sentence\n",
    "                if abbrev in sentence:\n",
    "                    # we have to make sure that the abbreviation is not inside a word, e.g. \"in\" in \"within\". It is allowed to have punctuation before and after the abbreviation, e.g. AI, or AI.\n",
    "                    # We add a \"try\" since the abbreviation might contain a backslash, which would cause an error. If there is an error, we skip the abbreviation\n",
    "                    try:\n",
    "                        for m in re.finditer(abbrev, old_sentence):\n",
    "                            # check whether there is a letter before and after the abbreviation\n",
    "                            if m.start() > 0:\n",
    "                                if sentence[m.start() - 1].isalpha():\n",
    "                                    continue\n",
    "                            if m.end() < len(sentence):\n",
    "                                if sentence[m.end()].isalpha():\n",
    "                                    continue\n",
    "                            replacements.append(((m.start(), m.end()), definition))\n",
    "                    except:\n",
    "                        errors_with_abbreviations.add(abbrev)\n",
    "                        continue\n",
    "            # Now we want to make sure that the replacements do not overlap. We do this by sorting the replacements by their start index and then iterating over them and only keeping the first replacement that does not overlap with the previous replacements\n",
    "            replacements = sorted(replacements, key=lambda x: x[0][0])\n",
    "            replacements_to_keep = []\n",
    "            for replacement in replacements:\n",
    "                if len(replacements_to_keep) == 0:\n",
    "                    replacements_to_keep.append(replacement)\n",
    "                else:\n",
    "                    # check whether the replacement overlaps with the previous replacements\n",
    "                    overlap = False\n",
    "                    for replacement_to_keep in replacements_to_keep:\n",
    "                        if replacement[0][0] <= replacement_to_keep[0][1]:\n",
    "                            overlap = True\n",
    "                            break\n",
    "                    if not overlap:\n",
    "                        replacements_to_keep.append(replacement)\n",
    "            # Now we can replace the abbreviations with their definitions\n",
    "            sorted_replacements_to_keep = sorted(\n",
    "                replacements_to_keep, key=lambda x: x[0][0], reverse=True\n",
    "            )\n",
    "            for replacement in sorted_replacements_to_keep:\n",
    "                sentence = (\n",
    "                    sentence[: replacement[0][0]]\n",
    "                    + replacement[1]\n",
    "                    + sentence[replacement[0][1] :]\n",
    "                )\n",
    "                start_pos.append(replacement[0][0])\n",
    "                end_pos.append(replacement[0][1])\n",
    "            new_sentences.append(sentence)\n",
    "            if len(replacements_to_keep) > 0:\n",
    "                write_log(\n",
    "                    old_sentence,\n",
    "                    sentence,\n",
    "                    i,\n",
    "                    start_pos,\n",
    "                    end_pos,\n",
    "                    \"Abbreviation replacement\",\n",
    "                    logger_abbr,\n",
    "                )\n",
    "        # Get new_text by joining the sentences\n",
    "        new_text = \"\\n\".join(new_sentences)\n",
    "        new_texts.append(new_text)\n",
    "    return new_texts, errors_with_abbreviations\n",
    "\n",
    "\n",
    "def get_span_noun_indices(doc, cluster):\n",
    "    \"\"\"Get the indices of the spans that contain a noun\n",
    "\n",
    "    Parameters:\n",
    "        doc (Doc): spacy document\n",
    "        cluster (list[tuple]): list of tuples with the start and end position of the spans\n",
    "\n",
    "    Return:\n",
    "        span_noun_indices (list[int]): list of indices of the spans that contain a noun\n",
    "    \"\"\"\n",
    "\n",
    "    spans = [doc.text[span[0] : span[1] + 1] for span in cluster]\n",
    "    # We now want to know which tokens are in the spans and whether they are nouns\n",
    "    span_noun_indices = []\n",
    "    for idx, span in enumerate(spans):\n",
    "        has_noun = False\n",
    "        for token in doc:\n",
    "            if token.text in span and token.pos_ in [\"NOUN\", \"PROPN\"]:\n",
    "                has_noun = True\n",
    "                break\n",
    "        if has_noun:\n",
    "            span_noun_indices.append(idx)\n",
    "    return span_noun_indices\n",
    "\n",
    "\n",
    "def is_containing_other_spans(span, all_spans):\n",
    "    \"\"\"Check whether a span is containing other spans\n",
    "\n",
    "    Parameters:\n",
    "        span (tuple): tuple with the start and end position of the span\n",
    "        all_spans (list[tuple]): list of tuples with the start and end position of the spans\n",
    "\n",
    "    Return:\n",
    "        bool: whether the span is containing other spans\n",
    "    \"\"\"\n",
    "    return any([s[0] >= span[0] and s[1] <= span[1] and s != span for s in all_spans])\n",
    "\n",
    "\n",
    "def get_cluster_head(doc: Doc, cluster, noun_indices):\n",
    "    \"\"\"Get the head of the cluster\n",
    "\n",
    "    Parameters:\n",
    "        doc (Doc): spacy document\n",
    "        cluster (list[tuple]): list of tuples with the start and end position of the spans\n",
    "        noun_indices (list[int]): list of indices of the spans that contain a noun\n",
    "\n",
    "    Return:\n",
    "        head_span (str): head of the cluster\n",
    "        head_start_end (tuple): tuple with the start and end position of the head\n",
    "    \"\"\"\n",
    "    head_idx = noun_indices[0]\n",
    "    head_start, head_end = cluster[head_idx]\n",
    "    head_span = doc.text[head_start : head_end + 1]\n",
    "    return head_span, (head_start, head_end)\n",
    "\n",
    "\n",
    "def replace_corefs(doc, logger_coref, clusters):\n",
    "    \"\"\"Replace the coreferences in the text\n",
    "\n",
    "    Parameters:\n",
    "        doc (Doc): spacy document\n",
    "        PATH_LOG (str): path to the log file\n",
    "        clusters (list[list[tuple]]): list of clusters, where each cluster is a list of tuples with the start and end position of the spans\n",
    "\n",
    "    Return:\n",
    "        new_text (str): text with the coreferences replaced\n",
    "    \"\"\"\n",
    "\n",
    "    all_spans = [span for cluster in clusters for span in cluster]\n",
    "    # initialize new text being equal to old text\n",
    "    new_text = doc.text\n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "    all_replacements = []\n",
    "    for cluster in clusters:\n",
    "        noun_indices = get_span_noun_indices(doc, cluster)\n",
    "        if len(noun_indices) > 0:\n",
    "            mention_span, mention = get_cluster_head(doc, cluster, noun_indices)\n",
    "            for coref in cluster:\n",
    "                if coref != mention and not is_containing_other_spans(coref, all_spans):\n",
    "                    # Execute the replacement\n",
    "                    start_pos, end_pos = coref\n",
    "                    # Replace the coref\n",
    "                    start_positions.append(coref[0])\n",
    "                    end_positions.append(coref[1])\n",
    "                    # Store the replacement in a way that we can do it later\n",
    "                    all_replacements.append((coref, mention_span))\n",
    "\n",
    "\n",
    "def process_dataframe(df: pd.DataFrame):\n",
    "    \"\"\"Process the rows of the DataFrame containing the titles, abstracts, and bodies\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): DataFrame with 'id', 'title', 'abstract', 'body' columns\n",
    "\n",
    "    Return:\n",
    "        df (pd.DataFrame): DataFrame with processed text columns\n",
    "    \"\"\"\n",
    "    texts = []\n",
    "    logger_citations = get_logger(\"citations\")\n",
    "    logger_abbr = get_logger(\"abbreviations\")\n",
    "    logger_coref = get_logger(\"coref\")\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        title, abstract, body = row[\"title\"], row[\"abstract\"], row[\"body\"]\n",
    "\n",
    "        # Remove citations\n",
    "        abstract, body = (\n",
    "            remove_citations([abstract], logger_citations)[0],\n",
    "            remove_citations([body], logger_citations)[0],\n",
    "        )\n",
    "\n",
    "        # Expand abbreviations\n",
    "        abstract, errors_abbr = expand_abbreviations([abstract], logger_abbr)\n",
    "        body, errors_abbr_body = expand_abbreviations([body], logger_abbr)\n",
    "        abstract, body = abstract[0], body[0]\n",
    "\n",
    "        # Apply coreference resolution\n",
    "        doc_abstract = nlp(abstract)\n",
    "        doc_body = nlp(body)\n",
    "        clusters_abstract = doc_abstract._.coref_clusters\n",
    "        clusters_body = doc_body._.coref_clusters\n",
    "\n",
    "        abstract = replace_corefs(doc_abstract, logger_coref, clusters_abstract)\n",
    "        body = replace_corefs(doc_body, logger_coref, clusters_body)\n",
    "\n",
    "        # Store the processed text\n",
    "        texts.append(\n",
    "            {\"id\": row[\"id\"], \"title\": title, \"abstract\": abstract, \"body\": body}\n",
    "        )\n",
    "\n",
    "    # Return the DataFrame with processed text\n",
    "    return pd.DataFrame(texts)\n",
    "\n",
    "\n",
    "# Now, `processed_df` will contain the processed titles, abstracts, and bodies with citations removed, abbreviations expanded, and coreferences resolved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "arxiv_df = pd.read_parquet('filtered_df_arxiv.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11/09/2024 20:36:18 - INFO - \t Action: Removing citations\n",
      "11/09/2024 20:36:18 - INFO - \t Line: 0\n",
      "11/09/2024 20:36:18 - INFO - \t Old Text: Introduction When a glass-forming liquid is cooled rapidly, its viscosity increases dramatically and it eventually transforms into an amorphous solid, called a glass, whose physical properties are profoundly different from those of ordered crystalline solids [1]. At even lower temperature, around 1K, the specific heat of a disordered solid is much larger than that of its crystalline counterpart as it scales linearly rather than cubically with temperature. Similarly, the temperature evolution of the thermal conductivity in glasses is quadratic, rather than cubic [2], [3], [4], [5], [6], [7], [8], [9], [10], [11]. A theoretical framework rationalizing such anomalous behavior was provided by Anderson, Halperin and Varma [12] and by Phillips [13], [14]. They argued that the energy landscape of amorphous solids contains many nearly-degenerate minima, connected by localized motions of a few atoms, that can act as tunneling defects, called two-level systems (TLS). Understanding the microscopic origin of TLS and how to control their density and physical properties has gained significant interest, not only because they provide a large contribution to the specific heat and to the thermal conductivity, but also because their presence may impact the performance of certain quantum devices [15]. Unfortunately, understanding their microscopic nature and their almost ubiquitous presence in low-temperature glasses remains a challenge [16], [17], [18], [19], [20]. With the development of the swap Monte Carlo algorithm [21], [22] it has become possible to create computer glasses at unprecedentedly low temperatures. Combined with landscape exploration algorithms [23], [24], [25], [26], [27], [28], [29], [30], [31], [32] this provides a method to investigate the nature of TLS in materials prepared under conditions comparable to experimental studies [33], [34]. These tools have enabled computational studies that have confirmed the experimental observation [7], [8], [10], [35], [11] that as the kinetic stability of a glass increases, the density of tunneling defects is strongly depleted [33], [36]. The direct detection of TLS revealed some of their microscopic features, namely that the participation ratio decreases in more stable glasses [33], and that TLS do not seem to be in a one-to-one correspondence with soft harmonic [37], [33], [34] or localized [38], [34] modes. The main issue that limits the applicability of the direct landscape exploration method is that it remains computationally very expensive, and it is thus hard to construct the large library of TLS needed to enable a robust statistical analysis of their physical properties. After accumulating a large number of inherent structures (IS), one must run an expensive algorithm to find the relaxation pathway connecting pairs of IS to then determine if this pair forms a proper TLS (namely, has an energy splitting within thermal energy at 1K). Due to the large number of IS pairs detected, it is impossible to characterize all of them. In previous work, some ad hoc filtering rules were introduced [33], [34], [36] but the success rate of such filters is poor. After a significant exploration effort, it was possible to identify about 60 TLS starting from the identification of $\\sim 10^8$  IS. It is then obvious that most of the computational effort has been wasted in the study of pairs that form defects which do not tunnel at low temperatures. In this paper we show that it is possible to predict with enhanced accuracy whether a pair of inherent structures forms a TLS by using machine learning techniques. Recently, machine learning [39], [40], [41], [42], [43], [44], [45], [46], [47], [48] has been shown to be extremely effective in using structural indicators to predict structural, dynamical or mechanical properties of glassy systems. In a similar vein, we use supervised learning to streamline the process of TLS identification. Our study has two goals: (i) develop a faster way to identify TLS compared to the standard approach outlined in Ref. [33] and described in Sec. , in order to collect a statistically significant number of TLS; (ii) understand what are the structural and dynamical features characterizing TLS, and if/how they change with the preparation temperature. To address (i) we show that our machine learning model can be trained in a few minutes using a small quantity of data, after which the model is able to identify candidate TLS with high speed and accuracy. To address (ii) we show which static features are the most important for the model prediction and we show that states which are dynamically distant can nevertheless be TLS. We conclude by explaining how the ML model distinguishes TLS from non-TLS and how it is able to identify glasses prepared at different temperatures. Standard approach to TLS identification The standard procedure [30], [31], [33] to identify TLS is sketched in Fig. REF . It consists of the following first steps which aim at identifying potential candidates for TLS:  Equilibrate the system at the preparation temperature $T_f$ . Glasses with lower $T_f$  have a larger glass stability. Run molecular dynamics to sample configurations along a dynamical trajectory at the exploration temperature $T<T_f$ . Perform energy minimization from the sampled configurations to produce a time series of energy minima, or inherent structures (IS). Analyze the transition rates between pairs of IS, and select the pairs of IS that are explored consecutively. Step 4 was necessary because it is computationally impossible to analyze all pairs of IS, as the number of pairs scales quadratically with the number of minima. The filter defined in step 4 was physically motivated by the fact that TLS tend to originate from IS that are not too distinct in order to have a reasonable tunneling probability. As such it is likely that those pairs of IS get explored one after the other during the exploration dynamics in step 2. Overall, given $N_{IS}$  inherent structures, this procedure selects for $\\mathcal {O}(N_{IS})$  pairs to be analyzed. However, many pairs of IS can be close but not sampled consecutively during the dynamics, owing to the complex structure of the potential energy landscape. Instead, our machine learning approach allows to consider all pairs of IS. As shown below, our approach is able to detect TLS which are otherwise excluded by the above step 4. Once potential candidates are selected, the procedure continues as follows:  For each selected pair of IS, look for the minimum energy path and the classical barrier between them by running a minimum energy path finding algorithm, such as nudge elastic band (NEB) [49], [50], [51]. This provides the value of the potential along the minimum energy path between the pair $V(\\xi )$ , where $0\\le \\xi \\le 1$  is the reaction coordinate. Select pairs whose energy profile $V(\\xi )$  has the form of a double well (DW), i.e., exclude paths with multiple wells. Solve the one-dimensional Schrödinger equation: $-\\frac{\\hbar ^2}{2md^2\\epsilon }\\partial ^2_{\\xi } \\Psi (\\xi )+V(\\xi )\\Psi (\\xi )=\\mathcal {E}\\Psi (\\xi ),$  where $\\xi $  is a normalized distance along the reaction path $\\xi =x/d$  and energy is normalized by a Lennard-Jones energy scale $\\epsilon $ , the effective mass $m$  and the distance $d$  are calculated as in Ref. [33]. We obtain the quantum splitting (QS) $E_{qs}=\\mathcal {E}_2-\\mathcal {E}_1$  from the first two energy levels $\\mathcal {E}_1$  and $\\mathcal {E}_2$ . The quantum splitting is the most relevant parameter because when $E_{qs} \\sim T$  the system can go from one state to the other via quantum tunneling [13], creating what is called a two-level system (TLS). In particular since we choose to report the data in units that correspond to Argon [33], a double well excitation will be an active TLS at $T=1$ K when $E_{qs} < 0.0015 \\epsilon $ , where $\\epsilon $  sets the energy scale of the pair interactions in the simulated model. Overall, since at low temperature the landscape exploration dynamics is slow, one would like to spend most of the computational time by doing steps 2-3 to construct a large library of pairs of IS. A first problem is that when the library of IS grows larger it takes a lot of time to perform steps 5-7. Moreover, the main bottleneck lies in the fact that most of the pairs that go through the full procedure are not found to be TLS in the end, and so this large computational time is effectively wasted. Machine learning approach to TLS identification As in any machine learning (ML) approach, we distinguish two phases: training and deployment. Our supervised training approach, detailed in the next section, takes just a few hours of training on a single CPU. It requires an initial dataset of $\\mathcal {O}(10^4)$  full NEB calculations, whose collection is the most time consuming part of the training phase. Once training is complete, the ML model can be deployed to identify new TLS. Its workflow is similar to the standard one, with some major improvements. It proceeds with the following steps:  - 3. The first 3 steps are similar to the standard procedure to obtain a collection of inherent structures from a dynamical exploration. Apply the ML model to all possible pairs of IS to predict which pairs form a DW potential. Apply the ML model to predict the quantum splitting (QS) for all predicted DW and filter out the configurations that are not predicted to be TLS by the ML model. - 8. Run NEB, select pairs that form DW potential and solve the one-dimensional Schrödinger equation for the TLS candidates only in order to obtain the exact value of the quantum splitting. It is possible to use steps 4-5 as a single shot or as an iterative training approach (see Sec. REF ). In the Supplementary Information (SI) we provide details on how steps 1-3 are performed: glass preparation, exploration of the potential energy landscape via molecular dynamics simulations and minimization procedure, as well as NEB computation, see Sec. . Noticeably, if the model is well-trained then the ML approach has two significant advantages over the standard approach. First, $\\mathcal {O}(N_{IS}^2)$  pairs of IS are scanned to identify TLS, compared to a much smaller number $\\mathcal {O}(N_{IS})$  in the standard procedure. Second, if a pair of IS passes step 5 and goes through the full procedure it is very likely to be a real TLS. As a consequence, by using the ML approach one can spend more time doing steps 2-3 to produce new IS, since fewer pairs pass step 5. At the same time, for any given number of IS, the ML approach can analyze all possible pairs and is therefore able to identify many more TLS, as we demonstrate below. Machine learning model In Refs. [33], [34], the authors analyze a library of 14202, 23535 and 117370 pairs of inherent structures for a continuously polydisperse system of soft repulsive particles, equilibrated at reduced temperatures $T = 0.062$ , 0.07, and 0.092, respectively. The standard approach (Sec. ) leads to the identification of 61, 291 and 1008 TLS for the three temperatures, respectively. Notice that this approach uses pairs of IS that are selected by the dynamical information contained in the transition matrix between pairs of IS [33]. This was done to filter out all non double well potential. For all pairs in this small subset, the quantum splitting was then calculated. Instead, the ML approach starts by independently evaluating the relevant information contained in each IS and constructs all possible combinations, even for pairs that are not dynamically connected. Following the steps discussed in Sec. the model is then able to predict the quantum splitting of all the pairs, that were predicted to form a DW, very accurately. From a quantitative perspective, this means that the same trajectories now contain many more TLS candidates in the ML approach compared to the standard approach. Figure: Flowchart of the machine learning approach. The first block represents the construction of the dataset by comparing all the pairs of inherent structures, focusing on the MM particles that displace the most. Then, required features are extracted to construct the input vector XX. We then train a classifier to predict whether a pair of IS is a DW or not. The DW are finally processed using a multi-layer stacking strategy to predict the quantum splitting energy. Our pipeline analyses a given pair of IS in about ∼10 -4 \\sim 10^{-4}s.In this section we describe the flowchart of the model summarised in Fig. REF . We first discuss how we construct the dataset and extract the relevant features (Sec. REF ). We then explain the two main blocks consisting in a DW classifier followed by a quantum splitting predictor, both of which have similar model architecture and inputs. Finally we evaluate the performance of this model by showing its ability to accurately predict the quantum splitting. We conclude by introducing the iterative training technique that represents the optimal way to efficiently expand the library of TLS. Dataset and features construction The first step is the evaluation of a set of static quantities for all the available IS. This set consists of: energy, particle positions, averaged bond-orientational order parameters [52] determined via a Voronoi tessellation, from $q_2$  to $q_{12}$ , and finally particle radii. The cost of this operation scales as the number of available states $N_{IS}$ , but we use these quantities to calculate the features of $\\sim N_{IS}^2$  pairs. A detailed analysis (see Sec. in the SI) shows that the bond orientational parameters and the particle sizes are not very useful for the ML model. Since their calculation is slower than all the other features, we do not include them in the final version of the ML approach. To construct the input features for each pair of IS we combine the information of the two states evaluating the following:  Energy splitting $\\Delta E$ : energy difference between the two IS. Displacements $\\Delta \\vec{r}_i$ : displacement vector of particle $i$  between the two configurations. Total displacement $d$ : total distance between the two IS defined as $d^2=\\sum _i |\\Delta \\vec{r}_i|^2$ . Participation ratio $PR$ : defined as $PR=(d^2)^2/\\left( \\sum _i |\\Delta \\vec{r}_i|^4\\right)$ . Distance from the displacement center $|\\vec{r}_0-\\vec{r}_i|$ : we measure the average distance of particle $i$  from the center of displacement $\\vec{r}_0$ , identified as the average position of the particle that moves the most. This quantity identifies the typical size of the region of particles that rearrange. Transition matrix $T_{ij}$  (and $T_{ji}$ ): number of times that the exploration dynamics traverses IS$_j$  just after IS$_i$  (and vice versa). The crucial step of the feature construction is that we can reduce the number of features by considering only the $M$  particles whose displacement is the largest between pairs of IS. We make this assumption because we expect that the low temperature dynamics is characterized by localised rearrangements involving only a small fraction of the particles [7], [8], [10], [35], [11], [33]. In Sec. of the SI, we confirm this assumption by showing that the ML model achieves optimal performances even when $M$  is very small. So, the choice of $M \\ll N$  makes the ML model computationally effective without any performance drop. Double well classifier A necessary condition in order for a pair of IS to be a TLS is that the transition between the pair forms a double well (DW) potential. A DW is defined when the minimum energy path between the two IS resembles a quartic potential, as sketched in Fig. REF (c). The final goal of the ML model is to predict the quantum splitting of the pair to identify pairs with low values of the QS. The first obstacle in the identification of TLS is that DW represent only a small subgroup of all IS pairs. For instance in Ref. [33], only $\\sim 0.5\\%$  of all the IS pairs are DW at the lowest temperature. It is then mandatory to filter out pairs that are not likely to be a DW. In the machine learning field there are usually many different models that can be trained to achieve similar performances, with complexity ranging from polynomial regression to deep neural networks. Here, we perform model ensembling and use ensembles both for DW classification and QS prediction. Model ensembling consists in averaging the output of different ML models to achieve better predictions compared to each of them separately. We do so using the publicly available AutoGluon library [53]. In this approach, we train in a few minutes a single-stack ensemble that is able to classify DW with $>95\\%$  accuracy. In Sec. of the SI we justify this choice of ML model and provide details on performances and hyperparameters. Overall, since the DW classifier is accurate and rapid, we use it to filter out the pairs that do not require the attention of the QS predictor because they cannot be TLS anyway. Quantum splitting predictor We want to predict the quantum splitting of a pair of IS for which the features discussed in Sec. REF  have been computed. We need this prediction to be very precise, because we know that a pair can be considered a TLS when $E_{qs}<0.0015 \\epsilon $ , but $E_{qs}$  can vary significantly so errors may be large. In the SI (see Sec. ) we show that models such as deep neural networks and regression are not stable or powerful enough to achieve satisfying results. We thus perform model ensembling by using the AutoGluon library [53]. This achieves superior performances while also allowing us to compare and rank single models. We perform two types of model ensembling: (i) stacking: different layers of models are applied in series creating a stack (schematized in Fig. REF ), and (ii) bagging: we divide the data in subsets that we use to train multiple instances of the same models and we later combine all their predictions. As shown in the SI, the best results are obtained while using ensembles of gradient boosting methods (in particular CatBoost [54]), which have proven to be the optimal choice in similar semi-empirical quantum-mechanical calculations [55]. Overall, the ensemble structure of our final model relieves us from hyperparameter optimization and makes the results more stable. Figure: (a)-(c) Quantum splitting and (d)-(f) energy barrier predicted by the ML model compared to the exact value, reported using a double logarithmic scale. We report predictions for IS pairs that the ML model has not seen during the training. (a) and (d) correspond to T f =0.062T_f=0.062, with training for 7000 samples and information of the M=3M=3 particles with largest displacements. (b) and (e) correspond to T f =0.07T_f=0.07, 10000 samples and M=3M=3. (c) and (f) correspond to T f =0.092T_f=0.092, 30000 samples and M=3M=3. All models have been trained for ∼10\\sim 10 hours of single CPU time.In order to train the model we first collect a set of $E_{qs}$  examples. The size of this training set is discussed in the SI (see Fig. REF ) where we find that the minimum number is around $10^4$ . We can use some of the data already collected in previous work in Ref. [33] for the training. Moreover, since we are interested in estimating with more precision the lowest values of $E_{qs}$  we train the model to minimize the following loss function $\\mathcal {L} = \\frac{ \\sum _{i=1}^n w_i \\left( E_{qs,\\mathrm {true}} -E_{qs,\\mathrm {predicted}} \\right)^2 }{n \\sum _{i=1}^n w_i },$  which is a weighted mean-squared error. The weights correspond to $w_i = 1/E_{qs,\\mathrm {true}}$  in order to give more importance to low $E_{qs}$  values. We thus train our model to provide a very accurate prediction of the value $E_{qs}$  for any given pair. Once the model is trained it takes only $\\sim 10^{-4}$ s to predict the QS of a new pair (compared to 1 minute to run the standard procedure). If we predict a value $E_{qs}<0.0015\\epsilon $ , then we have identified a TLS much faster. To showcase the performance of the ML model we report in Fig. REF (a)-(c) the exact quantum splitting calculated from the NEB procedure, compared with the value predicted by the model. We have trained three independent models to work at the three different temperatures. As explained before (Sec. REF ), the model needs the information about only the $M \\ll N$  particles that are displaced the most to achieve the excellent precision demonstrated in Fig. REF . In the Fig. REF  (SI) we find that the optimal value is $M=3$ , confirming that the participation ratio in TLS is quite low, because only three particles are needed for the model to identify TLS. Furthermore, the models have been trained using the smallest number of samples, randomly selected from all the IS pairs available, that allows the model to reach its top performance. We have also performed an analysis of the optimal training time. Details on these points are provided in Sec. of the SI. The performances presented in Fig. REF  are achieved by training the model for $\\sim 10$  hours of single CPU time, but we also show in the SI that it is possible to already achieve $>90\\%$  of this performance by training the ensemble for only 10 minutes. The ML approach that we have just introduced is also easily generalizable to target any state-to-state transition, like excitations and higher energy effects. Here we modified the quantum splitting predictor to instead predict the classical energy barrier between two IS states. If the minimal energy path between two IS forms a DW, we define the classical energy barrier as the maximum value of the energy along this path. In Fig. REF (d)-(f) we report the value of the energy barrier predicted by the ML model (y-axis) compared to the exact value calculated from the NEB procedure (x-axis). The hyperparameters and the features are the ones used for the quantum splitting predictor. Such a high performance demonstrates that our ML approach can predict other types of transitions between states. Iterative training procedure We finally introduce an approach to optimally employ our ML model to process new data: the iterative training procedure. In previous sections and in Fig. REF  we trained the model once using a subset of the already available data. This is a natural way to proceed when the goal is to process new data that are very similar to the training set, and the training set is itself large enough. However, since the goal of the proposed ML model is to ultimately drive the landscape exploration and collect new samples, the single-training approach may encounter two types of problems. First, at the beginning there may be not enough data and second the findings of the model do not provide any additional feedback. To solve both problems we introduce the iterative training procedure. The idea of iterative training is to use the predictive power of ML to create and expand its own training set, consequently enhancing its performance by iteratively retraining over the new data. Details on the method and parameters are discussed in Sec. of the SI. In practice, we start from a training set of $K_0 \\sim 10^3-10^4$  randomly selected pairs to have an initial idea of the relation between input and output. We then use the ML approach outlined in Fig. REF  to predict the $K_i=500$  pairs with the lowest QS. For these TLS candidates, we perform the full procedure to calculate the true QS and determine whether the pair is a DW or a TLS. In Fig. REF  (SI), we report the result of this procedure when we process a new set of trajectories from the same polydisperse soft sphere potential as in Ref. [33]. In general the first few iterations of iterative training have a poor performance. In fact we find that $>70\\%$  of the first $K_i$  pairs are actually non-DW. After collecting additional $K_i$  measurements, we retrain the model. We report in Tab. REF  the average time for each step of the ML procedure. The retraining can be done in $\\sim 10$  min, after which the model is able to predict the next $K_i$  pairs with lowest QS. Overall, to process $N_\\mathrm {IS pairs}$  we estimate that the computational time of the iterative approach is $t_{i}=\\left[ K_0\\cdot 10^{2} + N_{iter}\\left(K_i\\cdot 10^{2} + 10^{3} + N_\\mathrm {IS pairs}\\cdot 10^{-5} \\right) \\right]s$ . If $N_\\mathrm {IS pairs}>10^{9}$  it is possible to significantly reduce $t_i$  by permanently discarding the worst pairs, but this is not needed here. We iterate this procedure $N_{iter}$  times, until the last batch of $K_i$  candidates contains less than $1\\%$  of the total number of TLS. We believe that continuing this iterative procedure would lead to the identification of even more TLS/DW, but this is out of the scope of this paper. Table: Computational time needed to perform our ML approach, on a standard laptop. Results and discussion We now use ML in order to speed up the TLS search. This highly efficient method allows us to collect a library of TLS of unprecedented size, generated from numerical simulations with the same interaction potential as in Ref. [33]. First of all, we reprocess the data produced to obtain the results presented in Ref. [33] with our new ML method, obtaining new information about the connection between TLS and dynamics. Next, we perform ML-guided exploration to collect as many TLS as possible. This sizable library of TLS allows us to perform for the first time a detailed statistical analysis of TLS and compare their distribution to the distribution of double wells. We perform this analysis for glasses of three different stabilities. Finally, we discuss the microscopic features of TLS not only by looking at their statistics, but also by analyzing what the ML model has learned, and how it expresses its prediction. Capturing elusive TLS with machine learning Prior to this paper, it was not possible to evaluate all the IS pairs collected in Ref. [33]. For this reason the authors discarded a priori all pairs where the number of forward and backward jumps between IS is such that $\\min \\left(T_{ij},T_{ji}\\right)<4$ , based on the assumption that high transition rates during the dynamic landscape exploration is a good indicator that two IS form a DW. This reduced the number of pairs to 14202, 21109 and 117339 for glasses prepared at $T_f=0.062$ , 0.07 and $0.092$  respectively. In order to have comparable data at the three temperatures, for $T_f=0.092$  we only consider a subset of glasses corresponding to 30920 IS pairs. The results of the TLS search are summarized in the red columns of Tab. REF . Overall the standard procedure reaches a rate of TLS found over calculations performed of $4 \\cdot 10^{-3}$ , $13\\cdot 10^{-3}$ , and $8\\cdot 10^{-3}$  for the three temperatures, respectively. We compare this with our iterative training procedure applied on the same data, whose results are reported in the green columns of Tab. REF . We immediately notice two major improvements: the overall number of TLS that we find from the same data set is more than twice larger and the ratio of TLS per calculation is more than 15 times larger, corresponding to $62\\cdot 10^{-3}$ , $211\\cdot 10^{-3}$ , and $194\\cdot 10^{-3}$  for the same three temperatures. We conclude that the iterative ML approach is much more efficient than the standard procedure, and also that TLS do not necessarily have a large dynamical transition rate, since the dynamical-filtering approach misses more than half of them. Table: Analysis of data collected in Ref. . We compare the standard procedure with our ML approach using iterative training. We report results for different glass stabilities, decreasing from top to bottom, using Argon units (Ar)  and NiP metallic glass parameters (NiP) . The standard procedure finds less than half of the TLS and is computationally much more expensive. Differences between DW and TLS With our ML-driven exploration of the energy landscape we can restrict the numerical effort to DW and favorable TLS candidates, while processing a larger number and/or longer exploration trajectories. This allows us to consider a larger set of independent glasses of the same type as those treated in Ref. [33], which is particularly relevant for ultrastable glasses generated at the lowest temperature $T_f=0.062$ . While in Ref. [33] the collection of 61 TLS required $>14000$  NEB calculations, we are able to identify 864 TLS running 11 iterations of iterative training using only a total of 5500 NEB calculations in addition to the $\\sim 6000$  used for pretraining. In the next section we analyze these results to discuss the nature of TLS. Figure: Results of ML driven exploration, leading to a library of TLS of unprecedented size. (a) We compare the model predictions to the calculated values at the end of our iterative training, at T f =0.062T_f=0.062. We color coded in the background the confusion matrix. The black dashed horizontal lines report the percentage of TLS that are predicted below that value of quantum splitting, showing that more than 95%95\\% of the TLS are within twice the TLS threshold of 0.00150.0015. (b) Cumulative distribution of energy splitting n(E qs )n(E_{qs}) divided by E qs E_{qs}. (c) Histograms of the number of TLS and DW per glass, at the three preparation temperatures. In total we have considered 237, 30, 5 glasses equilibrated at T f =0.062T_f=0.062, 0.07, and 0.092 respectively.The database of glasses that we analyze with iterative training contains 5 times more minima than in Ref. [33], but we are able to find 15 times more TLS running around half of the NEB calculations. Furthermore, in Fig. REF (a) we have color-coded the confusion matrix and we highlight that we can capture almost all of the TLS if we consider only the pairs that are predicted to be within twice the quantum splitting threshold of TLS. In Fig. REF (b) we report the saturation plateau of the cumulative density of TLS quantum splitting $n(E_{qs})$ , which scales as $n(E_{qs}) \\sim n_0 E_{qs}$  at low $E_{qs}$ . The ML approach allows us to collect significantly better statistics compared to Ref. [33], confirming that the TLS density $n_0$  decreases by several orders of magnitude from hyperquenched to ultrastable glasses. Lastly, in Fig. REF (c) we report the histograms of the number of TLS and DW per glass at the three temperatures. We see that when the glasses are ultrastable ($T_f=0.062$ ) most of the glasses have very few TLS. Conversely, poorly annealed ($T_f=0.092$ ) glasses show a very unbalanced distribution, with few glasses that contains most of the DW and TLS. Interpretation of the ML model It is possible to use the ML model to gain information about the distinctive structure of TLS. First, the present and previous works [7], [8], [10], [35], [11], [33], [36] find that the density of TLS decreases upon increasing glass stability, which in our simulations is controlled by the preparation temperature. Thus, one may also expect temperature-dependent TLS features. In the SI we show that when the ML model is trained at $T_\\mathrm {train}$  and deployed at $T_\\mathrm {prediction} \\ne T_\\mathrm {train}$  there is only a minor performance drop and the model is able to perform reasonably well. This implies that the model captures distinctive signatures of TLS that do not depend strongly on the preparation temperature. Yet, we also show in the SI that it is very easy to train another ML model to predict the temperature itself and eventually add it to the pipeline. Overall, the ML model is not only able to capture the different microscopic features of TLS, but it can also suggest what is the specific influence of each feature. To interpret this information we calculate their Shapley values [56] and we report them in Fig. REF . The features are ranked from the most important (top) to the less important (bottom) reporting the impact that they have on the model output (SHAP value), so that a positive SHAP value predicts on average a high value of the QS. We see that the most important feature is the classical energy splitting $\\Delta E$  and that a large splitting (red) corresponds to a large QS. The second most important feature is the largest single particle displacement $\\Delta \\vec{r}_0$ , which has to be larger than a threshold corresponding to $0.3\\sigma $  in order to predict a low QS. The total displacement $d$  is the third most important and shows a similar effect. All the remaining features have a less clear and much smaller effect on the model prediction and they only collaborate collectively to the final QS prediction. In the SI we show that it is possible to obtain very good performance even when removing some of the features with the largest Shapley values, which means that the ML interpretation is not unique. Figure: Importance of the different features for the ML model for the prediction of the quantum splitting, evaluated from the Shapley values  at T f =0.062T_f=0.062. The features are ranked from the most important (top) to the least important, where the impact that they have on the model output corresponds to their SHAP value. Each point corresponds to a single IS pair. The color coding shows the impact from high (red) to low (green) values of each specific input.According to this Shapley analysis we explain the ML prediction in the following way: the energy difference $\\Delta E$  between two IS is the main predictor for the quantum splitting, and it has to be small for TLS. Then, the largest particle displacement $\\Delta \\vec{r}_0$  is necessary to understand if the two IS are similar and what is their stability (we show in the SI that $\\Delta \\vec{r}_0$  is the most important feature to identify the glass stability). Then the total displacement $d$  complements this information and gives local information about the displacements of the other particles. Lastly, all the other inputs provide fine tuning to refine the final prediction and are discussed in more detail in the SI. Interestingly, in the SI we also show that even without the two most important features, the ML approach can still reasonably reveal TLS candidates. Microscopic features of TLS We have shown that by following a ML-driven approach it is possible to collect a significant library of TLS for any preparation temperature. However it may be useful to discuss alternative strategies to rapidly identify TLS. In general, since TLS are extremely rare objects [7], [8], [10], [35], [11] a filtering rule is necessary in order to reduce the number of possible candidates. In particular, Ref. [33], [34], [36] proposed to use the transition matrix to exclude pairs that do not get explored consecutively. This is based on the assumption that DW (and consequently TLS) correspond to IS pairs that are close to each other and therefore during an exploration run should be detected successively, hence have a non zero transition rates. Instead, we prove in Fig. REF  that a filter based on dynamical information only is a poor predictor. In Fig. REF (a) we report the distribution of dynamical transitions between two inherent structures $i$  and $j$  for TLS, DW and all the pairs, measured at $T_f=0.062$ . While the slowly decaying tail of TLS and DW suggests that they often exhibit a large transition rate, actually most of the TLS and DW are characterized by no transition at all between them. Our interpretation is that even though the transition towards the other side of the double well is favourable, the landscape has such a large dimensionality ($3N$ ) that even very favorable transitions may never take place in a finite exploration time. This issue can become more severe when the trajectories are shorter, for example if the exploration is performed in parallel. We confirmed this observation by the results that are already reported in Tab. REF , where we have used our iterative training approach to re-analyze the data of Ref. [33], including pairs with no transition, thus finding many more TLS. We conclude that even though the transition rates are the most important single characteristics for DW prediction (see Fig. REF ), a filter based solely on them is still missing a lot of interesting pairs and therefore is not the most efficient. In Fig. REF (b) we focus on the distribution of classical splitting $\\Delta E$ . When $\\Delta E$  is large we rarely see DW and TLS (red region). On the other hand, there are many pairs that show a very small $\\Delta E$  value (yellow region), but they are not more likely to be TLS. Ultimately we find a `sweet spot' (green region), where TLS are more frequent. The ML model also captures this feature, as we can see from the SHAP parameter of $\\Delta E$  in Fig. REF . The next most important feature according to the ML model is the largest particle displacement $\\Delta \\vec{r}_0$ , reported in Fig. REF (c). When it is larger than $\\sim 0.8\\sigma $  we rarely find TLS and DW, but we do not find them also when $\\Delta \\vec{r}_0<0.3\\sigma $ . The SHAP parameter in Fig. REF  confirms that the ML model has discovered this feature. Finally in Fig. REF (d) we report the total displacement $d$ . If $d>0.9\\sigma $  the pair is so different that it is not likely to be a TLS or DW, while this probability increases for smaller $d$ . Overall, if it is necessary to identify TLS with a `quick and dirty' method, we propose to use a transition matrix to filter DW from non-DW, and then select a sweet spot for the energy splitting and the displacement for selecting optimal TLS candidates. Figure: Microscopic features of TLS and DW for ultrastable glasses at T f =0.062T_f=0.062. We report the probability distribution functions of: (a) number of transitions between the two inherent structures T ij T_{ij} and T ji T_{ji}, (b) classical energy splitting ΔE\\Delta E, (c) largest particle displacement Δr → 0 \\Delta \\vec{r}_0 and (d) total displacement dd. We color coded in red the regions of parameters where we do not expect to find TLS, which instead concentrate in the green regions. These green regions could serve as an alternative to rapidly identify TLS. Conclusion In this paper we have introduced a machine-learning approach to explore complex energy landscapes, with the goal of efficiently locating double wells (DW) and two-level systems (TLS). We demonstrate that it is possible to use ML to rapidly estimate the quantum splitting of a pair of inherent structures (IS) and accurately predict if a DW is a TLS or not. We also show that our ML approach can be used to predict very accurately the energy barrier between pairs of IS. Overall, this approach allows us to collect a TLS library of unprecedented size that would be impossible to obtain without the support of ML. The ML model uses as input information calculated from a pair of inherent structures. After just a few minutes of supervised training it is able to infer with high accuracy the quantum splitting of any new pair of inherent structures. We establish that the ML model that we develop using the Autogluon library is fast and precise. Its efficiency allows us to introduce an iterative training procedure, where we perform a small batch of prediction and then retrain the model. After performing statistical analysis over the unprecedented number of TLS collected with our method, we have discovered that many DW and TLS are not consecutively explored during the dynamics. We then reanalyzed the data of Ref. [33] finding that more than half of the TLS had been missed, because the analysis of Ref. [33] was based on this dynamic assumption. Our ML approach not only finds more than twice the number of TLS from the same data, but it also requires significantly fewer calculations. Overall we conclude that ML significantly improves the existing approaches. It also shows that if it is not possible to use the ML procedure that we outline, an effective `quick and dirty' way to predict TLS should be: a) use $T_{ij},T_{ji}$  for predicting DW; b) for predicted DW use energy splitting between two IS for predicting TLS. We also discuss the microscopic nature of DW and TLS. We perform a Shapley analysis to dissect the ML model and understand what it learns, and we compare this with the extended statistics of TLS that we are able to collect. We find that the quantum splitting is mostly related to the classical energy splitting and the displacements of the particles. Overall, the Shapley analysis suggests that TLS are characterized by one particle that displaces between $0.3$  and $0.9$  of its size, while the total displacement and the energy difference between the two states remains small. The local structure around the particle is not as important, nor is the number of times we actually see this transition during the exploration dynamics. Lastly we investigate the effect that glass stability (equivalent to the preparation temperature in our simulations) has on double wells and TLS. The ML model learns that at higher temperatures all the pairs are characterized by more collective rearrangements, but TLS are similar for any preparation temperature. Ultimately, since our ML approach is extremely efficient in exploring the energy landscape and is easy to generalize to target any type of state-to-state transition (as we show for the energy barriers), we hope that our method will be used in the future to analyze not only TLS, but also many other examples of phenomena related to specific transitions between states in complex classical and quantum settings. We thank E. Flenner, G. Folena, M. Ozawa, J. Sethna, S. Elliott, G. Ruocco and W. Schirmacher for useful discussions. This work was granted access to the HPC resources of MesoPSL financed by the Region Ile de France and the project Equip@Meso (reference ANR-10-EQPX-29-01) of the programme Investissements d’Avenir supervised by the Agence Nationale pour la Recherche. This project received funding from the European Research Council (ERC) under the European Union’s Horizon 2020 research and innovation program, Grant No. 723955 – GlassUniversality (FZ), and from the Simons Foundation (#454933, LB, #454955, FZ, #454951 DR) and by a Visiting Professorship from the Leverhulme Trust (VP1-2019-029, LB). CS acknowledges support from the Herchel Smith Fund and Sidney Sussex College, University of Cambridge. Model We study a three-dimensional polydisperse mixture of particles interacting via the potential $v(r_{ij}) = \\epsilon (\\sigma _{ij}/r_{ij})^{12}+\\epsilon F(r_{ij}/\\sigma _{ij}),$  only if $r_{ij}=1.25\\sigma _{ij}$ . We use non-additive interactions $\\sigma _{ij} = 0.5(\\sigma _i + \\sigma _j ) (1 - 0.2|\\sigma _i - \\sigma _j |)$ . The function $F$  is a fourth-order polynomial ensuring continuity of $v(r_{ij})$ , its first and second derivatives, at the interaction cutoff. The particle diameters $\\sigma _i$  are drawn from the normalized distribution $P(0.73 < \\sigma <1.62) \\propto 1/\\sigma ^3$ . This model glassformer is efficiently equilibrated with the particle-swap Monte Carlo algorithm, with a fluid robust against fractionation and crystallization down to low temperature [21]. We employ the swap Monte Carlo algorithm implemented in the LAMMPS package, with the optimal parameters provided in [22]. With such parameters supercooled liquid configurations can be generated down to $T =$  0.062, below the experimental glass transition $T_g=0.067$ . In this work, we use configurations prepared in equilibrium conditions at temperatures $T_f = 0.092, 0.07, 0.062$ . MD dynamics To explore the energy landscape of glasses generated at $T_f$ , we use classical Molecular Dynamics (MD) simulations. Glasses are first thermalized to $T_{MD}$  = 0.04 using a Berendsen thermostat. Such temperature is low enough to suppress diffusion, but sufficiently high for quick exploration of the energy landscape. We then run MD simulations in the NVE ensemble, using an integration time step of $dt =$  0.01 (LJ time units). Configurations along the MD trajectory are used as the starting point for energy minimization via a conjugate gradient algorithm, which brings them to their inherent structure (IS). MD configurations are minimized every 20, 10, 5 time steps for $T_f$  = 0.062, 0.07, 0.092 respectively. The values are a compromise between too frequent minimization which sample the same IS many times consecutively, and too infrequent which would miss intermediate IS. For each initial configuration we perform 100,100,200 MD runs with different initial velocities for $T_f$  = 0.062, 0.07, 0.092, respectively. Each run lasts 40000, 100000, 10000 time steps. We performed our calculations on 200, 50, 5 glasses for $T_f$  = 0.062, 0.07, 0.092. For $T_f=0.092$  glasses we used a subset of the original data obtained in [33]. NEB To analyze the transition between two IS we compute the multi-dimensional minimum energy path separating them, along with its one-dimensional potential energy profile. This is done by the nudged elastic band (NEB) method [49], [50] implemented in the LAMMPS package. We use 40 images to interpolate the minimal energy path, that are connected by springs of constant $\\kappa =1.0$  $\\epsilon \\sigma ^{-2}$ , and use the FIRE algorithm in the minimization procedure [49], [51]. Comparison between ensembled and non-ensembled models In the main manuscript we discuss how to transform the problem of computing the quantum splitting of two inherent structures into a supervised learning regression problem. Still, many machine learning models can in principle answer this question. We opted for the AutoGluon library [53] which is based on model ensembling to find an optimal solution. Model ensembling consists in averaging the output of different ML models and create an ensemble-averaged model that outperforms each one of its components. In particular, the AutoGluon library performs model ensembling in the form of stacking (stacking different layers of models that are applied in series) and bagging (training multiple instances of the same model on different subsets of data and then combining their prediction). In Fig. REF (a) we report the most performant ensembling after 20 hours of training, at $T_f=0.062$ , $M=5$  and $N_\\mathrm {samples}=10^4$ . The R2-score reported in the figure shows that the most performant model is a bagged ensemble of CatBoost, a gradient boosting method [54], which then corresponds to the final WeightedEnsemble for this training instance. Notice that in the main paper, we always use the best ensembled model. In addition to CatBoost, very good performances are also usually achieved by LightGBM, another gradient boosting method [57], and ExtraTrees, which is an extremely randomized tree ensemble [58]. Overall, gradient boosting methods typically achieve the highest R2-score. Thus, in the main text when we report results obtained with the `best models', we refer to ensembles of gradient boosting methods assembled with Autogluon. Figure: Comparison of different ML model ensembles to predict the quantum splitting at T f =0.062T_f=0.062 using M=5M=5 and N  samples  =7·10 4 N_\\mathrm {samples}=7\\cdot 10^4. (a) R2-score of the test set obtained for an ensemble of models. The full collection was trained for 20 hours of CPU time. In the main manuscript we use the WeightedEnsemble. (b) An optimal Neural Network (MLP with 10 hidden layers of size 1.1·N  features  1.1\\cdot N_\\mathrm {features}) does not produce good predictions (R2<0R2<0).We motivate our choice of complex ensemble model constructed with Autogluon by showing that simpler models do not perform at the same level. For the system at $T_f=0.062$  and in the optimal condition corresponding to $M=5$  and $N_\\mathrm {samples}=7000$  the simplest model to at least capture some correlation was a Multi-layer-perceptron (MLP) with 10 hidden layers of size $1.1\\cdot N_\\mathrm {features}$ , using the same input and output structure as in the main text. A first disadvantage of this approach is that several additional hyperparameters have to be selected, such as learning rate, number and size of hidden layers, activation function, etc. We varied these parameters and report in Fig. REF (b) results for the model with the best performance. This optimal MLP was trained until the iteration per epoch was consistently below $10^{-10}$ , corresponding to $\\sim 10$  hours. Still, the results in Fig. REF (b) demonstrate that the MLP performance is much worse than that of the ensemble models used in the main text. In addition, it is much slower to train. While in this case the predictions could be scaled by a constant to achieve significant improvement, the problem with such an approach is that it is not reliable, and this scaling constant is not known a priori adding an additional layer of complication. For this reason we prefer to use more complex machine-learning models. Parameters to predict the quantum splitting A significant advantage of the model ensemble approach is that most of the hyperparameters are automatically optimized by the ensembling. Still, some degrees of freedom have to be fixed. In particular we need to fix the number of particles to be considered ($M$ ), how many samples to use ($N_\\mathrm {samples}$ ) and for how long to train the ensemble. In the next sections we motivate the choices reported in the main text. Figure: Optimizing the Quantum splitting predictor. Effect of (a) the number of particles MM, (b) number of samples N  samples  N_\\mathrm {samples} and (c) training time on performance, while other parameters assume their optimal value. We report the R2-score after training on the data of Ref. . Overall we conclude that optimal performances can be achieved for M=3M=3, ∼10 4 \\sim 10^4 samples in just 10 minutes of training. $M$ : input particles – In Fig. REF (a) we report the effect of $M$ , the number of particles considered in the ML procedure. A large $M$  hinders the performance of the ML model because it has to process a larger input vector. The peak performance is achieved at $M=3$ , which is a relatively low number of particles to define a TLS. This confirms that the participation rate in TLS is low [33]. $N_\\mathrm {samples}$ : number of samples – To evaluate the optimal value of $N_\\mathrm {samples}$  we report in Fig. REF (b) the variation of the R2-score upon using more samples for training. Notice that we keep the other parameters in the optimal condition for each temperature. The shape of the curves let us conclude that a good number of training samples is $7000, 10000, 30000$  over a total of $14202, 23535, 117370$  for $T_f=0.062, 0.07, 0.092$  respectively. Notice that the results depend on the specific quality of the samples provided for training. It is possible to achieve better predictions by selecting better initial samples, more uniformly distributed, which are different from the random choice we make for simplicity. Still, the values reported consistently produce good results, independently from the initial sample composition. Training time – In Fig. REF (c) we report the effect of the training time on performance. The R2-score approaches the plateau in 10 minutes of training on a single CPU and reaches the final value after $10^3$  minutes ($\\sim 16$  hours). Overall, we find that in iterative training, where we retrain the model several times, a good balance between performance and speed can be reached by training for $\\sim 10$  minutes. Instead, in a more classic single training approach we suggest to train for $>10^3$  minutes. Performing the training in parallel can further reduce the training time. Parameters to identify double wells The ML model has to process a large number of unfiltered minima obtained during the exploration simulation. While we can anticipate that only a small fraction of inherent structure pairs will be a TLS, we also know that the minimum energy path connecting any pair of IS will not form a DW and as it is likely to contain intermediate minima. To exclude those non-DW pairs, we train a classifier using as input the same quantities that we measure to predict the QS, as described in the main text. In Fig. REF  we report the results of a hyperparameter scan at $T_f=0.062$ . These results are similar to those obtained for the QS prediction suggesting that at $T_f=0.062$  we can use (a) $M=3$ , (b) $10^2$ s of training and (c) $\\sim 10^4$  samples in order to achieve $>95\\%$  accuracy, measured over a validation set. We conclude that we can use the same hyperparameters for the DW classifier and the QS predictor. Removing irrelevant features After finding the optimal parameters to train the ML model, we can extract the specific effect and overall importance of each input parameter, as well as the score drop when each of them is removed. Since each additional feature corresponds to additional computational time and memory, we investigate which features are crucial, and which ones can be excluded to make the ML pipeline faster and more efficient without affecting performance. Figure: Importance of different features on the quantum splitting predictor. (a) Performance drop when each specific feature is removed, normalized to the first most important. Different colors correspond to different glass preparation temperatures. (b) Shapley values calculated at T f =0.062T_f=0.062. In general, at any temperature the most important feature is the energy difference (ΔE\\Delta E) of the two IS, which is followed by the value of the displacement of the MM particles. The Shapley values on the right also report the effect of the specific features: the splitting ΔE\\Delta E has to be small (green) in order to predict low QS (i.e. negative SHAP), while instead the displacements have to be large (red) in order to point towards low QS.In Fig. REF (a) we report the feature importance (in log scale) after a single iteration of training containing all the data from Ref. [33]. Different colors refer to glasses prepared at different temperatures. Independently from the temperature, the most important features are the energy difference $\\Delta E$  between the two IS, followed by the total displacement of particles $\\sum _i \\Delta \\vec{r}_i$ . The third most important information is the positions of the particles that displaced the most $\\sum _i | \\vec{r}_0 -\\vec{r}_i|$ . After them, we find that the arrangement of the first shell of neighbors represented by the $q$  parameters and the particle sizes $\\sigma _i$  are insignificant (and so is their variation between the two IS of the pair $\\Delta \\cdot $  reported in Fig. REF ), since their importance is more than two orders of magnitude smaller than the one of the energy and displacements. Since the $q$  parameters are also the slowest to compute, we decided to not include them in the final pipeline reported in the main manuscript. Next, to quantify the role of those inputs, we calculate their Shapley values [56] shown in Fig. REF (b) for $T_f=0.062$ . The color codes for the value taken by the specific feature (red when the value is high, green when low) and reports its scaled impact on the model output. The x axis reports the Shapley value, where SHAP$>0$  implies that the specific feature is pushing towards predicting a high QS, while SHAP$<0$  indicates that the feature promotes low QS values. While no input feature alone can predict a TLS (because $|$ SHAP$|$  is small) there are some visible trends: (i) the energy asymmetry is the most important feature and should be small in order to predict a low QS, (ii) the particle displacements have to be larger than a threshold in order to predict low QS. According to these results we rationalize the ML prediction. The energy difference between two IS is the main predictor for the quantum splitting, which is never too large for DW, then the displacements are necessary to understand if the two IS are similar and what their stability is (see the temperature classification section in the SI). The displacement nucleus size complements the information contained in the displacements and gives local information about the participation ratio. Finally, all the other features do not provide any improvement. Since the bond order parameters are computationally expensive to calculate, we do not include them in the final ML approach we propose in the main text. The performance reported in the main text confirms that our choice is justified. Reducing the number of features We justify the choice of features discussed in sec. IVa and Fig. 2 by presenting the performance of our ML model as a function of feature number. In Tab. REF  we rank the features according to their Shapley values (see Fig. 6). We report (blue line) in Fig. REF  the accuracy of the DW classifier (a) and the QS predictor (b) as a function of number of features, following the Shapley ranking of Tab. REF . The DW classifier reaches its best accuracy with six features. In the initial study Ref. [33], [36], the matrix $\\mathbf {T}$  was the only information used to analyze the pair of IS. Here, the classifier already reaches $\\sim 90\\%$  accuracy using the transition matrix only (two features). We add (orange line) the performances when we exclude $\\Delta E, T_{ij}$  and $T_{ji}$ , which are the most important features overall. Surprisingly, the DW classifier still reaches its maximum accuracy with two features ($\\Delta \\vec{r}_2$  and $|\\vec{r}_0-\\vec{r}_2|$ ) as highlighted in the inset. The QS predictor shows instead very good predictions even using a single feature ($\\Delta E$  in the blue curve, or $\\Delta \\vec{r}_0$  in the orange). Table: Ranking the features used in the main manuscript by their Shapley values.Figure: Effect of the number of features. Accuracy of the DW classifier (a) and Pearson correlation score of the QS predictor (b), as a function of the number of features used. Along the blue curves the features are ranked by their importance using their Shapley values, so the first features to be used are the most important. For the orange curve we exclude ΔE\\Delta E, T ij T_{ij} and T ji T_{ji} to evaluate the performances of the ML model without the features with the best Shapley values. Iterative training The standard supervised learning approach consists in collecting a significant amount of data, then using them to train the ML model and finally use the model predictions. While overall very powerful, this scheme is not optimal when the goal of the model is to drive the exploration in a space much larger than the available data. This is the case of our main study where we employ the ML model to identify IS pairs with low QS. In order to make our approach more efficient and generalizable we developed the iterative training procedure. Figure: Performance of the iterative training procedure at T f =0.062T_f=0.062 for the data collected in the main manuscript. Starting from K 0 =5000K_0=5000 samples we perform iterations of K i =500K_i=500 predictions for 11 iterations. In (a) we report the cumulative number of double wells (DW), two-level systems (TLS) and non double-wells (Non-DW). In (b) we break down this result by color coding the confusion matrix of the ML model at each iteration of iterative training.We start the iterative training procedure from a sample of $K_0$  pairs for which we calculate the QS. We empirically find that $K_0\\sim 5000$  achieves a good balance between precision and time. This sample has to be balanced between DW and non-DW, but there is no need to include TLS in the initial sample. It is possible to start from a smaller $K_0$ , at the cost of a performance drop in the first few iterations of training. From the initial sample we perform a first training that takes 10 minutes for the DW classifier and 10 minutes for the QS predictor. We then use the model to predict the $K_i=500$  IS pairs with the lowest QS. We report the cumulative number of TLS, DW, and non-DW at each iteration in Fig. REF (a). From Fig. REF (b) we see that during the first iteration most of the $K_i=500$  best candidates are actually non-DW, so the model is performing poorly. This is the reason why we suggest to perform only $K_i=500$  NEBs for each iterations, otherwise the first iteration would lead to wasting time to perform calculations over non-DW. On the other hand, in the first iteration 72 TLS are already found by running 500 NEBs. This is to be compared with Ref. [33] in which 61 TLS were found by running $>14000$  NEBs. After the first retraining (during iteration 2) the performance of the ML model is already excellent and less than $30\\%$  of the 500 best pairs are non-DW, while 134 are newly found TLS. We also used our iterative training to reprocess the data of Ref. [33], and we report the results in Table 1 (main text). We show in Fig. REF  a detailed analysis of the procedure at $T_f=0.062$ . While the standard approach was able to identify 61 TLS running $>14000$  NEB+Schrödinger calculations, iterative training finds 156 TLS, by running only 2500 NEBs. This confirms that more than half of the total TLS were hidden among the pairs discarded by Ref. [33]. In details, Fig. REF (a) shows the cumulative number of DW and TLS that we find, while Fig. REF (b) reports the confusion matrix for the different steps of iterative training. Figure: Reprocessing the data of Ref. at T f =0.062T_f=0.062 with iterative training. In (a) we report the cumulative number of double wells (DW), two-level systems (TLS) and non double-wells (Non-DW). While Ref. (red dashed line) identified 61 TLS running >14000>14000 NEBs, iterative training finds 156 TLS from 2500 NEBs. In (b) we report the confusion matrix of the 5 steps of iterative training that we run.Overall, these results demonstrate that the ML approach is not only faster than a manual filtering rule based on the transition matrix, but also much more effective. The pool of IS pairs excluded from the analysis in the original approach effectively contains a significant number of TLS. In conclusion, a ML driven exploration is not only more efficient than manual filtering, but also necessary to capture the correct statistics. Temperature and stability Our ML approach is able to predict the quantum splitting of a pair of IS from static information. It is known that TLS have different features depending on their preparation temperature, or glass stability [33]. Here we address the following questions: (i) how difficult is it for a machine to distinguish data corresponding to different temperatures, (ii) what are the most important features for this task, and (iii) does our ML approach learn temperature-independent features? Temperature classification To understand how TLS and IS features evolve with temperature, we trained a multi-layer-perceptron (MLP) to classify the temperature corresponding to a specific pair. We find that it is possible to rapidly train a classifier reaching accuracy $>95\\%$ . Depending on the value of $M$ , the input layer is composed by $N_\\mathrm {features}$  neurons, where the features are explained in the main text. After the input layer, there are $n$  hidden fully connected layers, all of the same size $s$ . The activation function for each neuron-neuron connection is a ReLu function. The last layer is composed of 3 neurons that represent the probability that a given input belongs to one of the three classes: $T_f=0.062$  or $T_f=0.07$  or $T_f=0.092$ . The performance of the MLP are evaluated by measuring the accuracy, which is the percentage of the corrected predictions. In Fig. REF (a) we report the effect of increasing the size of the hidden layers, while in Fig. REF (b) we report the effect of a larger number of hidden layers. Figure: Temperature classifier accuracy as a function of number of hidden layers (a) and their size (b). We report results for M=5M=5. Results suggest that already with 2 hidden layers of 60 neurons it is possible to achieve an accuracy above 95%95\\%.Our results show that a MLP with 2 hidden layers of 60 neurons has a $95\\%$  accuracy after less than 1h of supervised training. The answer to question (i), is that one can identify the temperature at which a pair of IS was obtained. Static signatures of glass stability We answer question (ii) by measuring the Shapley (SHAP) values from the temperature classifier. In Fig. REF  (a) we report the SHAP value of the most important input features for the MLP prediction that considers only the $M=5$  particles that displaced the most. The smallest ($i=(M-1)=4$ ) and the largest ($i=0$ ) particle displacements emerge as the most important input, noted $\\Delta \\vec{r}_4$  and $\\Delta \\vec{r}_0$ , respectively. Their average effect on the model prediction is shown in Fig.REF (b,c). The particle that displaced the most ($i=0$ ) shows a large displacement at low preparation temperature, while the particle that displaced the least ($i=M-1$ ) exhibits a large displacement at high temperature. Our results suggest that higher temperatures are characterized by more collective rearrangements, leading to large $\\Delta \\vec{r}_4$ . In glasses prepared at low temperature instead, transitions are characterized by a particle displacing significantly more than the others. Figure: Microscopic differences originating from different temperature preparation/glass stability, quantified using the Shapley values for the temperature classifier. (a) Summary of the SHAP values for the 9 most important input features, measured for a subset of 1000 pairs processed by the TT-classifier. Predicted glass preparation temperature as a function of (b) Δr → 4 \\Delta \\vec{r}_4, and (c) Δr → 0 \\Delta \\vec{r}_0, with all other inputs taking their average value. Transferability and crossvalidation We address question (iii) by testing how our model performs when trained at $T_\\mathrm {train}$  and deployed to predict the quantum splitting of pairs at $T_\\mathrm {predict}\\ne T_\\mathrm {train}$ . In Fig. REF  each column corresponds to a training temperature: $T_\\mathrm {train}=0.092$  (left), $0.07$  (middle) and $0.062$  (right). Then, computed quantum splittings are compared with ML prediction made on samples obtained at $T_\\mathrm {predict}$ , decreasing from top to bottom. We see in Fig. REF  that the predictive power of a model trained at a different temperature is slightly lower compared to the model trained at the same temperature (Fig. REF  of main). Still, the transferability of the model is good, especially when the model is trained at $T_\\mathrm {train}>T_\\mathrm {predict}$ . This situation is the most useful, since it is easier to obtain data at higher $T$ . We conclude that if not enough data is available at low temperature, it is possible to rely on model transferability. This relies on the fact that TLS share some general temperature-independent features. In summary, we have seen that IS and TLS have different microscopic features when generated from different stabilities. It is then optimal to train the ML model at a fixed temperature only. If no easier way to measure $T$ /stability are available, it is possible to use ML to classify the stability of each IS by adding another block to the workflow, as reported in the main manuscript. Alternatively, at the cost of a finite accuracy drop it is even possible to transfer the model predictions at different temperatures and thus train where data collection is fast and easy and apply it where it is not. Figure: Transferability and crossvalidation of the machine learning model. Exact quantum splitting against ML prediction. The quantum splitting predictor is trained at T  train  T_\\mathrm {train} and deployed at T  prediction  ≠T  train  T_\\mathrm {prediction}\\ne T_\\mathrm {train}. From left to right: T  train  =0.092,0.07,0.062T_\\mathrm {train} = 0.092, 0.07, 0.062. From top to bottom, decreasing T  prediction  T_\\mathrm {prediction}. Performances are not on par with Fig. , but good in particular when T  train  >T  predict  T_\\mathrm {train}>T_\\mathrm {predict}, which is of practical interest.\n",
      "11/09/2024 20:36:18 - INFO - \t New Text: Introduction When a glass-forming liquid is cooled rapidly, its viscosity increases dramatically and it eventually transforms into an amorphous solid, called a glass, whose physical properties are profoundly different from those of ordered crystalline solids . At even lower temperature, around 1K, the specific heat of a disordered solid is much larger than that of its crystalline counterpart as it scales linearly rather than cubically with temperature. Similarly, the temperature evolution of the thermal conductivity in glasses is quadratic, rather than cubic , , , , , , , , , . A theoretical framework rationalizing such anomalous behavior was provided by Anderson, Halperin and Varma  and by Phillips , . They argued that the energy landscape of amorphous solids contains many nearly-degenerate minima, connected by localized motions of a few atoms, that can act as tunneling defects, called two-level systems (TLS). Understanding the microscopic origin of TLS and how to control their density and physical properties has gained significant interest, not only because they provide a large contribution to the specific heat and to the thermal conductivity, but also because their presence may impact the performance of certain quantum devices . Unfortunately, understanding their microscopic nature and their almost ubiquitous presence in low-temperature glasses remains a challenge , , , , . With the development of the swap Monte Carlo algorithm ,  it has become possible to create computer glasses at unprecedentedly low temperatures. Combined with landscape exploration algorithms , , , , , , , , ,  this provides a method to investigate the nature of TLS in materials prepared under conditions comparable to experimental studies , . These tools have enabled computational studies that have confirmed the experimental observation , , , ,  that as the kinetic stability of a glass increases, the density of tunneling defects is strongly depleted , . The direct detection of TLS revealed some of their microscopic features, namely that the participation ratio decreases in more stable glasses , and that TLS do not seem to be in a one-to-one correspondence with soft harmonic , ,  or localized ,  modes. The main issue that limits the applicability of the direct landscape exploration method is that it remains computationally very expensive, and it is thus hard to construct the large library of TLS needed to enable a robust statistical analysis of their physical properties. After accumulating a large number of inherent structures (IS), one must run an expensive algorithm to find the relaxation pathway connecting pairs of IS to then determine if this pair forms a proper TLS (namely, has an energy splitting within thermal energy at 1K). Due to the large number of IS pairs detected, it is impossible to characterize all of them. In previous work, some ad hoc filtering rules were introduced , ,  but the success rate of such filters is poor. After a significant exploration effort, it was possible to identify about 60 TLS starting from the identification of $\\sim 10^8$  IS. It is then obvious that most of the computational effort has been wasted in the study of pairs that form defects which do not tunnel at low temperatures. In this paper we show that it is possible to predict with enhanced accuracy whether a pair of inherent structures forms a TLS by using machine learning techniques. Recently, machine learning , , , , , , , , ,  has been shown to be extremely effective in using structural indicators to predict structural, dynamical or mechanical properties of glassy systems. In a similar vein, we use supervised learning to streamline the process of TLS identification. Our study has two goals: (i) develop a faster way to identify TLS compared to the standard approach outlined in Ref.  and described in Sec. , in order to collect a statistically significant number of TLS; (ii) understand what are the structural and dynamical features characterizing TLS, and if/how they change with the preparation temperature. To address (i) we show that our machine learning model can be trained in a few minutes using a small quantity of data, after which the model is able to identify candidate TLS with high speed and accuracy. To address (ii) we show which static features are the most important for the model prediction and we show that states which are dynamically distant can nevertheless be TLS. We conclude by explaining how the ML model distinguishes TLS from non-TLS and how it is able to identify glasses prepared at different temperatures. Standard approach to TLS identification The standard procedure , ,  to identify TLS is sketched in Fig. REF . It consists of the following first steps which aim at identifying potential candidates for TLS:  Equilibrate the system at the preparation temperature $T_f$ . Glasses with lower $T_f$  have a larger glass stability. Run molecular dynamics to sample configurations along a dynamical trajectory at the exploration temperature $T<T_f$ . Perform energy minimization from the sampled configurations to produce a time series of energy minima, or inherent structures (IS). Analyze the transition rates between pairs of IS, and select the pairs of IS that are explored consecutively. Step 4 was necessary because it is computationally impossible to analyze all pairs of IS, as the number of pairs scales quadratically with the number of minima. The filter defined in step 4 was physically motivated by the fact that TLS tend to originate from IS that are not too distinct in order to have a reasonable tunneling probability. As such it is likely that those pairs of IS get explored one after the other during the exploration dynamics in step 2. Overall, given $N_{IS}$  inherent structures, this procedure selects for $\\mathcal {O}(N_{IS})$  pairs to be analyzed. However, many pairs of IS can be close but not sampled consecutively during the dynamics, owing to the complex structure of the potential energy landscape. Instead, our machine learning approach allows to consider all pairs of IS. As shown below, our approach is able to detect TLS which are otherwise excluded by the above step 4. Once potential candidates are selected, the procedure continues as follows:  For each selected pair of IS, look for the minimum energy path and the classical barrier between them by running a minimum energy path finding algorithm, such as nudge elastic band (NEB) , , . This provides the value of the potential along the minimum energy path between the pair $V(\\xi )$ , where $0\\le \\xi \\le 1$  is the reaction coordinate. Select pairs whose energy profile $V(\\xi )$  has the form of a double well (DW), i.e., exclude paths with multiple wells. Solve the one-dimensional Schrödinger equation: $-\\frac{\\hbar ^2}{2md^2\\epsilon }\\partial ^2_{\\xi } \\Psi (\\xi )+V(\\xi )\\Psi (\\xi )=\\mathcal {E}\\Psi (\\xi ),$  where $\\xi $  is a normalized distance along the reaction path $\\xi =x/d$  and energy is normalized by a Lennard-Jones energy scale $\\epsilon $ , the effective mass $m$  and the distance $d$  are calculated as in Ref. . We obtain the quantum splitting (QS) $E_{qs}=\\mathcal {E}_2-\\mathcal {E}_1$  from the first two energy levels $\\mathcal {E}_1$  and $\\mathcal {E}_2$ . The quantum splitting is the most relevant parameter because when $E_{qs} \\sim T$  the system can go from one state to the other via quantum tunneling , creating what is called a two-level system (TLS). In particular since we choose to report the data in units that correspond to Argon , a double well excitation will be an active TLS at $T=1$ K when $E_{qs} < 0.0015 \\epsilon $ , where $\\epsilon $  sets the energy scale of the pair interactions in the simulated model. Overall, since at low temperature the landscape exploration dynamics is slow, one would like to spend most of the computational time by doing steps 2-3 to construct a large library of pairs of IS. A first problem is that when the library of IS grows larger it takes a lot of time to perform steps 5-7. Moreover, the main bottleneck lies in the fact that most of the pairs that go through the full procedure are not found to be TLS in the end, and so this large computational time is effectively wasted. Machine learning approach to TLS identification As in any machine learning (ML) approach, we distinguish two phases: training and deployment. Our supervised training approach, detailed in the next section, takes just a few hours of training on a single CPU. It requires an initial dataset of $\\mathcal {O}(10^4)$  full NEB calculations, whose collection is the most time consuming part of the training phase. Once training is complete, the ML model can be deployed to identify new TLS. Its workflow is similar to the standard one, with some major improvements. It proceeds with the following steps:  - 3. The first 3 steps are similar to the standard procedure to obtain a collection of inherent structures from a dynamical exploration. Apply the ML model to all possible pairs of IS to predict which pairs form a DW potential. Apply the ML model to predict the quantum splitting (QS) for all predicted DW and filter out the configurations that are not predicted to be TLS by the ML model. - 8. Run NEB, select pairs that form DW potential and solve the one-dimensional Schrödinger equation for the TLS candidates only in order to obtain the exact value of the quantum splitting. It is possible to use steps 4-5 as a single shot or as an iterative training approach (see Sec. REF ). In the Supplementary Information (SI) we provide details on how steps 1-3 are performed: glass preparation, exploration of the potential energy landscape via molecular dynamics simulations and minimization procedure, as well as NEB computation, see Sec. . Noticeably, if the model is well-trained then the ML approach has two significant advantages over the standard approach. First, $\\mathcal {O}(N_{IS}^2)$  pairs of IS are scanned to identify TLS, compared to a much smaller number $\\mathcal {O}(N_{IS})$  in the standard procedure. Second, if a pair of IS passes step 5 and goes through the full procedure it is very likely to be a real TLS. As a consequence, by using the ML approach one can spend more time doing steps 2-3 to produce new IS, since fewer pairs pass step 5. At the same time, for any given number of IS, the ML approach can analyze all possible pairs and is therefore able to identify many more TLS, as we demonstrate below. Machine learning model In Refs. , , the authors analyze a library of 14202, 23535 and 117370 pairs of inherent structures for a continuously polydisperse system of soft repulsive particles, equilibrated at reduced temperatures $T = 0.062$ , 0.07, and 0.092, respectively. The standard approach (Sec. ) leads to the identification of 61, 291 and 1008 TLS for the three temperatures, respectively. Notice that this approach uses pairs of IS that are selected by the dynamical information contained in the transition matrix between pairs of IS . This was done to filter out all non double well potential. For all pairs in this small subset, the quantum splitting was then calculated. Instead, the ML approach starts by independently evaluating the relevant information contained in each IS and constructs all possible combinations, even for pairs that are not dynamically connected. Following the steps discussed in Sec. the model is then able to predict the quantum splitting of all the pairs, that were predicted to form a DW, very accurately. From a quantitative perspective, this means that the same trajectories now contain many more TLS candidates in the ML approach compared to the standard approach. Figure: Flowchart of the machine learning approach. The first block represents the construction of the dataset by comparing all the pairs of inherent structures, focusing on the MM particles that displace the most. Then, required features are extracted to construct the input vector XX. We then train a classifier to predict whether a pair of IS is a DW or not. The DW are finally processed using a multi-layer stacking strategy to predict the quantum splitting energy. Our pipeline analyses a given pair of IS in about ∼10 -4 \\sim 10^{-4}s.In this section we describe the flowchart of the model summarised in Fig. REF . We first discuss how we construct the dataset and extract the relevant features (Sec. REF ). We then explain the two main blocks consisting in a DW classifier followed by a quantum splitting predictor, both of which have similar model architecture and inputs. Finally we evaluate the performance of this model by showing its ability to accurately predict the quantum splitting. We conclude by introducing the iterative training technique that represents the optimal way to efficiently expand the library of TLS. Dataset and features construction The first step is the evaluation of a set of static quantities for all the available IS. This set consists of: energy, particle positions, averaged bond-orientational order parameters  determined via a Voronoi tessellation, from $q_2$  to $q_{12}$ , and finally particle radii. The cost of this operation scales as the number of available states $N_{IS}$ , but we use these quantities to calculate the features of $\\sim N_{IS}^2$  pairs. A detailed analysis (see Sec. in the SI) shows that the bond orientational parameters and the particle sizes are not very useful for the ML model. Since their calculation is slower than all the other features, we do not include them in the final version of the ML approach. To construct the input features for each pair of IS we combine the information of the two states evaluating the following:  Energy splitting $\\Delta E$ : energy difference between the two IS. Displacements $\\Delta \\vec{r}_i$ : displacement vector of particle $i$  between the two configurations. Total displacement $d$ : total distance between the two IS defined as $d^2=\\sum _i |\\Delta \\vec{r}_i|^2$ . Participation ratio $PR$ : defined as $PR=(d^2)^2/\\left( \\sum _i |\\Delta \\vec{r}_i|^4\\right)$ . Distance from the displacement center $|\\vec{r}_0-\\vec{r}_i|$ : we measure the average distance of particle $i$  from the center of displacement $\\vec{r}_0$ , identified as the average position of the particle that moves the most. This quantity identifies the typical size of the region of particles that rearrange. Transition matrix $T_{ij}$  (and $T_{ji}$ ): number of times that the exploration dynamics traverses IS$_j$  just after IS$_i$  (and vice versa). The crucial step of the feature construction is that we can reduce the number of features by considering only the $M$  particles whose displacement is the largest between pairs of IS. We make this assumption because we expect that the low temperature dynamics is characterized by localised rearrangements involving only a small fraction of the particles , , , , , . In Sec. of the SI, we confirm this assumption by showing that the ML model achieves optimal performances even when $M$  is very small. So, the choice of $M \\ll N$  makes the ML model computationally effective without any performance drop. Double well classifier A necessary condition in order for a pair of IS to be a TLS is that the transition between the pair forms a double well (DW) potential. A DW is defined when the minimum energy path between the two IS resembles a quartic potential, as sketched in Fig. REF (c). The final goal of the ML model is to predict the quantum splitting of the pair to identify pairs with low values of the QS. The first obstacle in the identification of TLS is that DW represent only a small subgroup of all IS pairs. For instance in Ref. , only $\\sim 0.5\\%$  of all the IS pairs are DW at the lowest temperature. It is then mandatory to filter out pairs that are not likely to be a DW. In the machine learning field there are usually many different models that can be trained to achieve similar performances, with complexity ranging from polynomial regression to deep neural networks. Here, we perform model ensembling and use ensembles both for DW classification and QS prediction. Model ensembling consists in averaging the output of different ML models to achieve better predictions compared to each of them separately. We do so using the publicly available AutoGluon library . In this approach, we train in a few minutes a single-stack ensemble that is able to classify DW with $>95\\%$  accuracy. In Sec. of the SI we justify this choice of ML model and provide details on performances and hyperparameters. Overall, since the DW classifier is accurate and rapid, we use it to filter out the pairs that do not require the attention of the QS predictor because they cannot be TLS anyway. Quantum splitting predictor We want to predict the quantum splitting of a pair of IS for which the features discussed in Sec. REF  have been computed. We need this prediction to be very precise, because we know that a pair can be considered a TLS when $E_{qs}<0.0015 \\epsilon $ , but $E_{qs}$  can vary significantly so errors may be large. In the SI (see Sec. ) we show that models such as deep neural networks and regression are not stable or powerful enough to achieve satisfying results. We thus perform model ensembling by using the AutoGluon library . This achieves superior performances while also allowing us to compare and rank single models. We perform two types of model ensembling: (i) stacking: different layers of models are applied in series creating a stack (schematized in Fig. REF ), and (ii) bagging: we divide the data in subsets that we use to train multiple instances of the same models and we later combine all their predictions. As shown in the SI, the best results are obtained while using ensembles of gradient boosting methods (in particular CatBoost ), which have proven to be the optimal choice in similar semi-empirical quantum-mechanical calculations . Overall, the ensemble structure of our final model relieves us from hyperparameter optimization and makes the results more stable. Figure: (a)-(c) Quantum splitting and (d)-(f) energy barrier predicted by the ML model compared to the exact value, reported using a double logarithmic scale. We report predictions for IS pairs that the ML model has not seen during the training. (a) and (d) correspond to T f =0.062T_f=0.062, with training for 7000 samples and information of the M=3M=3 particles with largest displacements. (b) and (e) correspond to T f =0.07T_f=0.07, 10000 samples and M=3M=3. (c) and (f) correspond to T f =0.092T_f=0.092, 30000 samples and M=3M=3. All models have been trained for ∼10\\sim 10 hours of single CPU time.In order to train the model we first collect a set of $E_{qs}$  examples. The size of this training set is discussed in the SI (see Fig. REF ) where we find that the minimum number is around $10^4$ . We can use some of the data already collected in previous work in Ref.  for the training. Moreover, since we are interested in estimating with more precision the lowest values of $E_{qs}$  we train the model to minimize the following loss function $\\mathcal {L} = \\frac{ \\sum _{i=1}^n w_i \\left( E_{qs,\\mathrm {true}} -E_{qs,\\mathrm {predicted}} \\right)^2 }{n \\sum _{i=1}^n w_i },$  which is a weighted mean-squared error. The weights correspond to $w_i = 1/E_{qs,\\mathrm {true}}$  in order to give more importance to low $E_{qs}$  values. We thus train our model to provide a very accurate prediction of the value $E_{qs}$  for any given pair. Once the model is trained it takes only $\\sim 10^{-4}$ s to predict the QS of a new pair (compared to 1 minute to run the standard procedure). If we predict a value $E_{qs}<0.0015\\epsilon $ , then we have identified a TLS much faster. To showcase the performance of the ML model we report in Fig. REF (a)-(c) the exact quantum splitting calculated from the NEB procedure, compared with the value predicted by the model. We have trained three independent models to work at the three different temperatures. As explained before (Sec. REF ), the model needs the information about only the $M \\ll N$  particles that are displaced the most to achieve the excellent precision demonstrated in Fig. REF . In the Fig. REF  (SI) we find that the optimal value is $M=3$ , confirming that the participation ratio in TLS is quite low, because only three particles are needed for the model to identify TLS. Furthermore, the models have been trained using the smallest number of samples, randomly selected from all the IS pairs available, that allows the model to reach its top performance. We have also performed an analysis of the optimal training time. Details on these points are provided in Sec. of the SI. The performances presented in Fig. REF  are achieved by training the model for $\\sim 10$  hours of single CPU time, but we also show in the SI that it is possible to already achieve $>90\\%$  of this performance by training the ensemble for only 10 minutes. The ML approach that we have just introduced is also easily generalizable to target any state-to-state transition, like excitations and higher energy effects. Here we modified the quantum splitting predictor to instead predict the classical energy barrier between two IS states. If the minimal energy path between two IS forms a DW, we define the classical energy barrier as the maximum value of the energy along this path. In Fig. REF (d)-(f) we report the value of the energy barrier predicted by the ML model (y-axis) compared to the exact value calculated from the NEB procedure (x-axis). The hyperparameters and the features are the ones used for the quantum splitting predictor. Such a high performance demonstrates that our ML approach can predict other types of transitions between states. Iterative training procedure We finally introduce an approach to optimally employ our ML model to process new data: the iterative training procedure. In previous sections and in Fig. REF  we trained the model once using a subset of the already available data. This is a natural way to proceed when the goal is to process new data that are very similar to the training set, and the training set is itself large enough. However, since the goal of the proposed ML model is to ultimately drive the landscape exploration and collect new samples, the single-training approach may encounter two types of problems. First, at the beginning there may be not enough data and second the findings of the model do not provide any additional feedback. To solve both problems we introduce the iterative training procedure. The idea of iterative training is to use the predictive power of ML to create and expand its own training set, consequently enhancing its performance by iteratively retraining over the new data. Details on the method and parameters are discussed in Sec. of the SI. In practice, we start from a training set of $K_0 \\sim 10^3-10^4$  randomly selected pairs to have an initial idea of the relation between input and output. We then use the ML approach outlined in Fig. REF  to predict the $K_i=500$  pairs with the lowest QS. For these TLS candidates, we perform the full procedure to calculate the true QS and determine whether the pair is a DW or a TLS. In Fig. REF  (SI), we report the result of this procedure when we process a new set of trajectories from the same polydisperse soft sphere potential as in Ref. . In general the first few iterations of iterative training have a poor performance. In fact we find that $>70\\%$  of the first $K_i$  pairs are actually non-DW. After collecting additional $K_i$  measurements, we retrain the model. We report in Tab. REF  the average time for each step of the ML procedure. The retraining can be done in $\\sim 10$  min, after which the model is able to predict the next $K_i$  pairs with lowest QS. Overall, to process $N_\\mathrm {IS pairs}$  we estimate that the computational time of the iterative approach is $t_{i}=\\left[ K_0\\cdot 10^{2} + N_{iter}\\left(K_i\\cdot 10^{2} + 10^{3} + N_\\mathrm {IS pairs}\\cdot 10^{-5} \\right) \\right]s$ . If $N_\\mathrm {IS pairs}>10^{9}$  it is possible to significantly reduce $t_i$  by permanently discarding the worst pairs, but this is not needed here. We iterate this procedure $N_{iter}$  times, until the last batch of $K_i$  candidates contains less than $1\\%$  of the total number of TLS. We believe that continuing this iterative procedure would lead to the identification of even more TLS/DW, but this is out of the scope of this paper. Table: Computational time needed to perform our ML approach, on a standard laptop. Results and discussion We now use ML in order to speed up the TLS search. This highly efficient method allows us to collect a library of TLS of unprecedented size, generated from numerical simulations with the same interaction potential as in Ref. . First of all, we reprocess the data produced to obtain the results presented in Ref.  with our new ML method, obtaining new information about the connection between TLS and dynamics. Next, we perform ML-guided exploration to collect as many TLS as possible. This sizable library of TLS allows us to perform for the first time a detailed statistical analysis of TLS and compare their distribution to the distribution of double wells. We perform this analysis for glasses of three different stabilities. Finally, we discuss the microscopic features of TLS not only by looking at their statistics, but also by analyzing what the ML model has learned, and how it expresses its prediction. Capturing elusive TLS with machine learning Prior to this paper, it was not possible to evaluate all the IS pairs collected in Ref. . For this reason the authors discarded a priori all pairs where the number of forward and backward jumps between IS is such that $\\min \\left(T_{ij},T_{ji}\\right)<4$ , based on the assumption that high transition rates during the dynamic landscape exploration is a good indicator that two IS form a DW. This reduced the number of pairs to 14202, 21109 and 117339 for glasses prepared at $T_f=0.062$ , 0.07 and $0.092$  respectively. In order to have comparable data at the three temperatures, for $T_f=0.092$  we only consider a subset of glasses corresponding to 30920 IS pairs. The results of the TLS search are summarized in the red columns of Tab. REF . Overall the standard procedure reaches a rate of TLS found over calculations performed of $4 \\cdot 10^{-3}$ , $13\\cdot 10^{-3}$ , and $8\\cdot 10^{-3}$  for the three temperatures, respectively. We compare this with our iterative training procedure applied on the same data, whose results are reported in the green columns of Tab. REF . We immediately notice two major improvements: the overall number of TLS that we find from the same data set is more than twice larger and the ratio of TLS per calculation is more than 15 times larger, corresponding to $62\\cdot 10^{-3}$ , $211\\cdot 10^{-3}$ , and $194\\cdot 10^{-3}$  for the same three temperatures. We conclude that the iterative ML approach is much more efficient than the standard procedure, and also that TLS do not necessarily have a large dynamical transition rate, since the dynamical-filtering approach misses more than half of them. Table: Analysis of data collected in Ref. . We compare the standard procedure with our ML approach using iterative training. We report results for different glass stabilities, decreasing from top to bottom, using Argon units (Ar)  and NiP metallic glass parameters (NiP) . The standard procedure finds less than half of the TLS and is computationally much more expensive. Differences between DW and TLS With our ML-driven exploration of the energy landscape we can restrict the numerical effort to DW and favorable TLS candidates, while processing a larger number and/or longer exploration trajectories. This allows us to consider a larger set of independent glasses of the same type as those treated in Ref. , which is particularly relevant for ultrastable glasses generated at the lowest temperature $T_f=0.062$ . While in Ref.  the collection of 61 TLS required $>14000$  NEB calculations, we are able to identify 864 TLS running 11 iterations of iterative training using only a total of 5500 NEB calculations in addition to the $\\sim 6000$  used for pretraining. In the next section we analyze these results to discuss the nature of TLS. Figure: Results of ML driven exploration, leading to a library of TLS of unprecedented size. (a) We compare the model predictions to the calculated values at the end of our iterative training, at T f =0.062T_f=0.062. We color coded in the background the confusion matrix. The black dashed horizontal lines report the percentage of TLS that are predicted below that value of quantum splitting, showing that more than 95%95\\% of the TLS are within twice the TLS threshold of 0.00150.0015. (b) Cumulative distribution of energy splitting n(E qs )n(E_{qs}) divided by E qs E_{qs}. (c) Histograms of the number of TLS and DW per glass, at the three preparation temperatures. In total we have considered 237, 30, 5 glasses equilibrated at T f =0.062T_f=0.062, 0.07, and 0.092 respectively.The database of glasses that we analyze with iterative training contains 5 times more minima than in Ref. , but we are able to find 15 times more TLS running around half of the NEB calculations. Furthermore, in Fig. REF (a) we have color-coded the confusion matrix and we highlight that we can capture almost all of the TLS if we consider only the pairs that are predicted to be within twice the quantum splitting threshold of TLS. In Fig. REF (b) we report the saturation plateau of the cumulative density of TLS quantum splitting $n(E_{qs})$ , which scales as $n(E_{qs}) \\sim n_0 E_{qs}$  at low $E_{qs}$ . The ML approach allows us to collect significantly better statistics compared to Ref. , confirming that the TLS density $n_0$  decreases by several orders of magnitude from hyperquenched to ultrastable glasses. Lastly, in Fig. REF (c) we report the histograms of the number of TLS and DW per glass at the three temperatures. We see that when the glasses are ultrastable ($T_f=0.062$ ) most of the glasses have very few TLS. Conversely, poorly annealed ($T_f=0.092$ ) glasses show a very unbalanced distribution, with few glasses that contains most of the DW and TLS. Interpretation of the ML model It is possible to use the ML model to gain information about the distinctive structure of TLS. First, the present and previous works , , , , , ,  find that the density of TLS decreases upon increasing glass stability, which in our simulations is controlled by the preparation temperature. Thus, one may also expect temperature-dependent TLS features. In the SI we show that when the ML model is trained at $T_\\mathrm {train}$  and deployed at $T_\\mathrm {prediction} \\ne T_\\mathrm {train}$  there is only a minor performance drop and the model is able to perform reasonably well. This implies that the model captures distinctive signatures of TLS that do not depend strongly on the preparation temperature. Yet, we also show in the SI that it is very easy to train another ML model to predict the temperature itself and eventually add it to the pipeline. Overall, the ML model is not only able to capture the different microscopic features of TLS, but it can also suggest what is the specific influence of each feature. To interpret this information we calculate their Shapley values  and we report them in Fig. REF . The features are ranked from the most important (top) to the less important (bottom) reporting the impact that they have on the model output (SHAP value), so that a positive SHAP value predicts on average a high value of the QS. We see that the most important feature is the classical energy splitting $\\Delta E$  and that a large splitting (red) corresponds to a large QS. The second most important feature is the largest single particle displacement $\\Delta \\vec{r}_0$ , which has to be larger than a threshold corresponding to $0.3\\sigma $  in order to predict a low QS. The total displacement $d$  is the third most important and shows a similar effect. All the remaining features have a less clear and much smaller effect on the model prediction and they only collaborate collectively to the final QS prediction. In the SI we show that it is possible to obtain very good performance even when removing some of the features with the largest Shapley values, which means that the ML interpretation is not unique. Figure: Importance of the different features for the ML model for the prediction of the quantum splitting, evaluated from the Shapley values  at T f =0.062T_f=0.062. The features are ranked from the most important (top) to the least important, where the impact that they have on the model output corresponds to their SHAP value. Each point corresponds to a single IS pair. The color coding shows the impact from high (red) to low (green) values of each specific input.According to this Shapley analysis we explain the ML prediction in the following way: the energy difference $\\Delta E$  between two IS is the main predictor for the quantum splitting, and it has to be small for TLS. Then, the largest particle displacement $\\Delta \\vec{r}_0$  is necessary to understand if the two IS are similar and what is their stability (we show in the SI that $\\Delta \\vec{r}_0$  is the most important feature to identify the glass stability). Then the total displacement $d$  complements this information and gives local information about the displacements of the other particles. Lastly, all the other inputs provide fine tuning to refine the final prediction and are discussed in more detail in the SI. Interestingly, in the SI we also show that even without the two most important features, the ML approach can still reasonably reveal TLS candidates. Microscopic features of TLS We have shown that by following a ML-driven approach it is possible to collect a significant library of TLS for any preparation temperature. However it may be useful to discuss alternative strategies to rapidly identify TLS. In general, since TLS are extremely rare objects , , , ,  a filtering rule is necessary in order to reduce the number of possible candidates. In particular, Ref. , ,  proposed to use the transition matrix to exclude pairs that do not get explored consecutively. This is based on the assumption that DW (and consequently TLS) correspond to IS pairs that are close to each other and therefore during an exploration run should be detected successively, hence have a non zero transition rates. Instead, we prove in Fig. REF  that a filter based on dynamical information only is a poor predictor. In Fig. REF (a) we report the distribution of dynamical transitions between two inherent structures $i$  and $j$  for TLS, DW and all the pairs, measured at $T_f=0.062$ . While the slowly decaying tail of TLS and DW suggests that they often exhibit a large transition rate, actually most of the TLS and DW are characterized by no transition at all between them. Our interpretation is that even though the transition towards the other side of the double well is favourable, the landscape has such a large dimensionality ($3N$ ) that even very favorable transitions may never take place in a finite exploration time. This issue can become more severe when the trajectories are shorter, for example if the exploration is performed in parallel. We confirmed this observation by the results that are already reported in Tab. REF , where we have used our iterative training approach to re-analyze the data of Ref. , including pairs with no transition, thus finding many more TLS. We conclude that even though the transition rates are the most important single characteristics for DW prediction (see Fig. REF ), a filter based solely on them is still missing a lot of interesting pairs and therefore is not the most efficient. In Fig. REF (b) we focus on the distribution of classical splitting $\\Delta E$ . When $\\Delta E$  is large we rarely see DW and TLS (red region). On the other hand, there are many pairs that show a very small $\\Delta E$  value (yellow region), but they are not more likely to be TLS. Ultimately we find a `sweet spot' (green region), where TLS are more frequent. The ML model also captures this feature, as we can see from the SHAP parameter of $\\Delta E$  in Fig. REF . The next most important feature according to the ML model is the largest particle displacement $\\Delta \\vec{r}_0$ , reported in Fig. REF (c). When it is larger than $\\sim 0.8\\sigma $  we rarely find TLS and DW, but we do not find them also when $\\Delta \\vec{r}_0<0.3\\sigma $ . The SHAP parameter in Fig. REF  confirms that the ML model has discovered this feature. Finally in Fig. REF (d) we report the total displacement $d$ . If $d>0.9\\sigma $  the pair is so different that it is not likely to be a TLS or DW, while this probability increases for smaller $d$ . Overall, if it is necessary to identify TLS with a `quick and dirty' method, we propose to use a transition matrix to filter DW from non-DW, and then select a sweet spot for the energy splitting and the displacement for selecting optimal TLS candidates. Figure: Microscopic features of TLS and DW for ultrastable glasses at T f =0.062T_f=0.062. We report the probability distribution functions of: (a) number of transitions between the two inherent structures T ij T_{ij} and T ji T_{ji}, (b) classical energy splitting ΔE\\Delta E, (c) largest particle displacement Δr → 0 \\Delta \\vec{r}_0 and (d) total displacement dd. We color coded in red the regions of parameters where we do not expect to find TLS, which instead concentrate in the green regions. These green regions could serve as an alternative to rapidly identify TLS. Conclusion In this paper we have introduced a machine-learning approach to explore complex energy landscapes, with the goal of efficiently locating double wells (DW) and two-level systems (TLS). We demonstrate that it is possible to use ML to rapidly estimate the quantum splitting of a pair of inherent structures (IS) and accurately predict if a DW is a TLS or not. We also show that our ML approach can be used to predict very accurately the energy barrier between pairs of IS. Overall, this approach allows us to collect a TLS library of unprecedented size that would be impossible to obtain without the support of ML. The ML model uses as input information calculated from a pair of inherent structures. After just a few minutes of supervised training it is able to infer with high accuracy the quantum splitting of any new pair of inherent structures. We establish that the ML model that we develop using the Autogluon library is fast and precise. Its efficiency allows us to introduce an iterative training procedure, where we perform a small batch of prediction and then retrain the model. After performing statistical analysis over the unprecedented number of TLS collected with our method, we have discovered that many DW and TLS are not consecutively explored during the dynamics. We then reanalyzed the data of Ref.  finding that more than half of the TLS had been missed, because the analysis of Ref.  was based on this dynamic assumption. Our ML approach not only finds more than twice the number of TLS from the same data, but it also requires significantly fewer calculations. Overall we conclude that ML significantly improves the existing approaches. It also shows that if it is not possible to use the ML procedure that we outline, an effective `quick and dirty' way to predict TLS should be: a) use $T_{ij},T_{ji}$  for predicting DW; b) for predicted DW use energy splitting between two IS for predicting TLS. We also discuss the microscopic nature of DW and TLS. We perform a Shapley analysis to dissect the ML model and understand what it learns, and we compare this with the extended statistics of TLS that we are able to collect. We find that the quantum splitting is mostly related to the classical energy splitting and the displacements of the particles. Overall, the Shapley analysis suggests that TLS are characterized by one particle that displaces between $0.3$  and $0.9$  of its size, while the total displacement and the energy difference between the two states remains small. The local structure around the particle is not as important, nor is the number of times we actually see this transition during the exploration dynamics. Lastly we investigate the effect that glass stability (equivalent to the preparation temperature in our simulations) has on double wells and TLS. The ML model learns that at higher temperatures all the pairs are characterized by more collective rearrangements, but TLS are similar for any preparation temperature. Ultimately, since our ML approach is extremely efficient in exploring the energy landscape and is easy to generalize to target any type of state-to-state transition (as we show for the energy barriers), we hope that our method will be used in the future to analyze not only TLS, but also many other examples of phenomena related to specific transitions between states in complex classical and quantum settings. We thank E. Flenner, G. Folena, M. Ozawa, J. Sethna, S. Elliott, G. Ruocco and W. Schirmacher for useful discussions. This work was granted access to the HPC resources of MesoPSL financed by the Region Ile de France and the project Equip@Meso (reference ANR-10-EQPX-29-01) of the programme Investissements d’Avenir supervised by the Agence Nationale pour la Recherche. This project received funding from the European Research Council (ERC) under the European Union’s Horizon 2020 research and innovation program, Grant No. 723955 – GlassUniversality (FZ), and from the Simons Foundation (#454933, LB, #454955, FZ, #454951 DR) and by a Visiting Professorship from the Leverhulme Trust (VP1-2019-029, LB). CS acknowledges support from the Herchel Smith Fund and Sidney Sussex College, University of Cambridge. Model We study a three-dimensional polydisperse mixture of particles interacting via the potential $v(r_{ij}) = \\epsilon (\\sigma _{ij}/r_{ij})^{12}+\\epsilon F(r_{ij}/\\sigma _{ij}),$  only if $r_{ij}=1.25\\sigma _{ij}$ . We use non-additive interactions $\\sigma _{ij} = 0.5(\\sigma _i + \\sigma _j ) (1 - 0.2|\\sigma _i - \\sigma _j |)$ . The function $F$  is a fourth-order polynomial ensuring continuity of $v(r_{ij})$ , its first and second derivatives, at the interaction cutoff. The particle diameters $\\sigma _i$  are drawn from the normalized distribution $P(0.73 < \\sigma <1.62) \\propto 1/\\sigma ^3$ . This model glassformer is efficiently equilibrated with the particle-swap Monte Carlo algorithm, with a fluid robust against fractionation and crystallization down to low temperature . We employ the swap Monte Carlo algorithm implemented in the LAMMPS package, with the optimal parameters provided in . With such parameters supercooled liquid configurations can be generated down to $T =$  0.062, below the experimental glass transition $T_g=0.067$ . In this work, we use configurations prepared in equilibrium conditions at temperatures $T_f = 0.092, 0.07, 0.062$ . MD dynamics To explore the energy landscape of glasses generated at $T_f$ , we use classical Molecular Dynamics (MD) simulations. Glasses are first thermalized to $T_{MD}$  = 0.04 using a Berendsen thermostat. Such temperature is low enough to suppress diffusion, but sufficiently high for quick exploration of the energy landscape. We then run MD simulations in the NVE ensemble, using an integration time step of $dt =$  0.01 (LJ time units). Configurations along the MD trajectory are used as the starting point for energy minimization via a conjugate gradient algorithm, which brings them to their inherent structure (IS). MD configurations are minimized every 20, 10, 5 time steps for $T_f$  = 0.062, 0.07, 0.092 respectively. The values are a compromise between too frequent minimization which sample the same IS many times consecutively, and too infrequent which would miss intermediate IS. For each initial configuration we perform 100,100,200 MD runs with different initial velocities for $T_f$  = 0.062, 0.07, 0.092, respectively. Each run lasts 40000, 100000, 10000 time steps. We performed our calculations on 200, 50, 5 glasses for $T_f$  = 0.062, 0.07, 0.092. For $T_f=0.092$  glasses we used a subset of the original data obtained in . NEB To analyze the transition between two IS we compute the multi-dimensional minimum energy path separating them, along with its one-dimensional potential energy profile. This is done by the nudged elastic band (NEB) method ,  implemented in the LAMMPS package. We use 40 images to interpolate the minimal energy path, that are connected by springs of constant $\\kappa =1.0$  $\\epsilon \\sigma ^{-2}$ , and use the FIRE algorithm in the minimization procedure , . Comparison between ensembled and non-ensembled models In the main manuscript we discuss how to transform the problem of computing the quantum splitting of two inherent structures into a supervised learning regression problem. Still, many machine learning models can in principle answer this question. We opted for the AutoGluon library  which is based on model ensembling to find an optimal solution. Model ensembling consists in averaging the output of different ML models and create an ensemble-averaged model that outperforms each one of its components. In particular, the AutoGluon library performs model ensembling in the form of stacking (stacking different layers of models that are applied in series) and bagging (training multiple instances of the same model on different subsets of data and then combining their prediction). In Fig. REF (a) we report the most performant ensembling after 20 hours of training, at $T_f=0.062$ , $M=5$  and $N_\\mathrm {samples}=10^4$ . The R2-score reported in the figure shows that the most performant model is a bagged ensemble of CatBoost, a gradient boosting method , which then corresponds to the final WeightedEnsemble for this training instance. Notice that in the main paper, we always use the best ensembled model. In addition to CatBoost, very good performances are also usually achieved by LightGBM, another gradient boosting method , and ExtraTrees, which is an extremely randomized tree ensemble . Overall, gradient boosting methods typically achieve the highest R2-score. Thus, in the main text when we report results obtained with the `best models', we refer to ensembles of gradient boosting methods assembled with Autogluon. Figure: Comparison of different ML model ensembles to predict the quantum splitting at T f =0.062T_f=0.062 using M=5M=5 and N  samples  =7·10 4 N_\\mathrm {samples}=7\\cdot 10^4. (a) R2-score of the test set obtained for an ensemble of models. The full collection was trained for 20 hours of CPU time. In the main manuscript we use the WeightedEnsemble. (b) An optimal Neural Network (MLP with 10 hidden layers of size 1.1·N  features  1.1\\cdot N_\\mathrm {features}) does not produce good predictions (R2<0R2<0).We motivate our choice of complex ensemble model constructed with Autogluon by showing that simpler models do not perform at the same level. For the system at $T_f=0.062$  and in the optimal condition corresponding to $M=5$  and $N_\\mathrm {samples}=7000$  the simplest model to at least capture some correlation was a Multi-layer-perceptron (MLP) with 10 hidden layers of size $1.1\\cdot N_\\mathrm {features}$ , using the same input and output structure as in the main text. A first disadvantage of this approach is that several additional hyperparameters have to be selected, such as learning rate, number and size of hidden layers, activation function, etc. We varied these parameters and report in Fig. REF (b) results for the model with the best performance. This optimal MLP was trained until the iteration per epoch was consistently below $10^{-10}$ , corresponding to $\\sim 10$  hours. Still, the results in Fig. REF (b) demonstrate that the MLP performance is much worse than that of the ensemble models used in the main text. In addition, it is much slower to train. While in this case the predictions could be scaled by a constant to achieve significant improvement, the problem with such an approach is that it is not reliable, and this scaling constant is not known a priori adding an additional layer of complication. For this reason we prefer to use more complex machine-learning models. Parameters to predict the quantum splitting A significant advantage of the model ensemble approach is that most of the hyperparameters are automatically optimized by the ensembling. Still, some degrees of freedom have to be fixed. In particular we need to fix the number of particles to be considered ($M$ ), how many samples to use ($N_\\mathrm {samples}$ ) and for how long to train the ensemble. In the next sections we motivate the choices reported in the main text. Figure: Optimizing the Quantum splitting predictor. Effect of (a) the number of particles MM, (b) number of samples N  samples  N_\\mathrm {samples} and (c) training time on performance, while other parameters assume their optimal value. We report the R2-score after training on the data of Ref. . Overall we conclude that optimal performances can be achieved for M=3M=3, ∼10 4 \\sim 10^4 samples in just 10 minutes of training. $M$ : input particles – In Fig. REF (a) we report the effect of $M$ , the number of particles considered in the ML procedure. A large $M$  hinders the performance of the ML model because it has to process a larger input vector. The peak performance is achieved at $M=3$ , which is a relatively low number of particles to define a TLS. This confirms that the participation rate in TLS is low . $N_\\mathrm {samples}$ : number of samples – To evaluate the optimal value of $N_\\mathrm {samples}$  we report in Fig. REF (b) the variation of the R2-score upon using more samples for training. Notice that we keep the other parameters in the optimal condition for each temperature. The shape of the curves let us conclude that a good number of training samples is $7000, 10000, 30000$  over a total of $14202, 23535, 117370$  for $T_f=0.062, 0.07, 0.092$  respectively. Notice that the results depend on the specific quality of the samples provided for training. It is possible to achieve better predictions by selecting better initial samples, more uniformly distributed, which are different from the random choice we make for simplicity. Still, the values reported consistently produce good results, independently from the initial sample composition. Training time – In Fig. REF (c) we report the effect of the training time on performance. The R2-score approaches the plateau in 10 minutes of training on a single CPU and reaches the final value after $10^3$  minutes ($\\sim 16$  hours). Overall, we find that in iterative training, where we retrain the model several times, a good balance between performance and speed can be reached by training for $\\sim 10$  minutes. Instead, in a more classic single training approach we suggest to train for $>10^3$  minutes. Performing the training in parallel can further reduce the training time. Parameters to identify double wells The ML model has to process a large number of unfiltered minima obtained during the exploration simulation. While we can anticipate that only a small fraction of inherent structure pairs will be a TLS, we also know that the minimum energy path connecting any pair of IS will not form a DW and as it is likely to contain intermediate minima. To exclude those non-DW pairs, we train a classifier using as input the same quantities that we measure to predict the QS, as described in the main text. In Fig. REF  we report the results of a hyperparameter scan at $T_f=0.062$ . These results are similar to those obtained for the QS prediction suggesting that at $T_f=0.062$  we can use (a) $M=3$ , (b) $10^2$ s of training and (c) $\\sim 10^4$  samples in order to achieve $>95\\%$  accuracy, measured over a validation set. We conclude that we can use the same hyperparameters for the DW classifier and the QS predictor. Removing irrelevant features After finding the optimal parameters to train the ML model, we can extract the specific effect and overall importance of each input parameter, as well as the score drop when each of them is removed. Since each additional feature corresponds to additional computational time and memory, we investigate which features are crucial, and which ones can be excluded to make the ML pipeline faster and more efficient without affecting performance. Figure: Importance of different features on the quantum splitting predictor. (a) Performance drop when each specific feature is removed, normalized to the first most important. Different colors correspond to different glass preparation temperatures. (b) Shapley values calculated at T f =0.062T_f=0.062. In general, at any temperature the most important feature is the energy difference (ΔE\\Delta E) of the two IS, which is followed by the value of the displacement of the MM particles. The Shapley values on the right also report the effect of the specific features: the splitting ΔE\\Delta E has to be small (green) in order to predict low QS (i.e. negative SHAP), while instead the displacements have to be large (red) in order to point towards low QS.In Fig. REF (a) we report the feature importance (in log scale) after a single iteration of training containing all the data from Ref. . Different colors refer to glasses prepared at different temperatures. Independently from the temperature, the most important features are the energy difference $\\Delta E$  between the two IS, followed by the total displacement of particles $\\sum _i \\Delta \\vec{r}_i$ . The third most important information is the positions of the particles that displaced the most $\\sum _i | \\vec{r}_0 -\\vec{r}_i|$ . After them, we find that the arrangement of the first shell of neighbors represented by the $q$  parameters and the particle sizes $\\sigma _i$  are insignificant (and so is their variation between the two IS of the pair $\\Delta \\cdot $  reported in Fig. REF ), since their importance is more than two orders of magnitude smaller than the one of the energy and displacements. Since the $q$  parameters are also the slowest to compute, we decided to not include them in the final pipeline reported in the main manuscript. Next, to quantify the role of those inputs, we calculate their Shapley values  shown in Fig. REF (b) for $T_f=0.062$ . The color codes for the value taken by the specific feature (red when the value is high, green when low) and reports its scaled impact on the model output. The x axis reports the Shapley value, where SHAP$>0$  implies that the specific feature is pushing towards predicting a high QS, while SHAP$<0$  indicates that the feature promotes low QS values. While no input feature alone can predict a TLS (because $|$ SHAP$|$  is small) there are some visible trends: (i) the energy asymmetry is the most important feature and should be small in order to predict a low QS, (ii) the particle displacements have to be larger than a threshold in order to predict low QS. According to these results we rationalize the ML prediction. The energy difference between two IS is the main predictor for the quantum splitting, which is never too large for DW, then the displacements are necessary to understand if the two IS are similar and what their stability is (see the temperature classification section in the SI). The displacement nucleus size complements the information contained in the displacements and gives local information about the participation ratio. Finally, all the other features do not provide any improvement. Since the bond order parameters are computationally expensive to calculate, we do not include them in the final ML approach we propose in the main text. The performance reported in the main text confirms that our choice is justified. Reducing the number of features We justify the choice of features discussed in sec. IVa and Fig. 2 by presenting the performance of our ML model as a function of feature number. In Tab. REF  we rank the features according to their Shapley values (see Fig. 6). We report (blue line) in Fig. REF  the accuracy of the DW classifier (a) and the QS predictor (b) as a function of number of features, following the Shapley ranking of Tab. REF . The DW classifier reaches its best accuracy with six features. In the initial study Ref. , , the matrix $\\mathbf {T}$  was the only information used to analyze the pair of IS. Here, the classifier already reaches $\\sim 90\\%$  accuracy using the transition matrix only (two features). We add (orange line) the performances when we exclude $\\Delta E, T_{ij}$  and $T_{ji}$ , which are the most important features overall. Surprisingly, the DW classifier still reaches its maximum accuracy with two features ($\\Delta \\vec{r}_2$  and $|\\vec{r}_0-\\vec{r}_2|$ ) as highlighted in the inset. The QS predictor shows instead very good predictions even using a single feature ($\\Delta E$  in the blue curve, or $\\Delta \\vec{r}_0$  in the orange). Table: Ranking the features used in the main manuscript by their Shapley values.Figure: Effect of the number of features. Accuracy of the DW classifier (a) and Pearson correlation score of the QS predictor (b), as a function of the number of features used. Along the blue curves the features are ranked by their importance using their Shapley values, so the first features to be used are the most important. For the orange curve we exclude ΔE\\Delta E, T ij T_{ij} and T ji T_{ji} to evaluate the performances of the ML model without the features with the best Shapley values. Iterative training The standard supervised learning approach consists in collecting a significant amount of data, then using them to train the ML model and finally use the model predictions. While overall very powerful, this scheme is not optimal when the goal of the model is to drive the exploration in a space much larger than the available data. This is the case of our main study where we employ the ML model to identify IS pairs with low QS. In order to make our approach more efficient and generalizable we developed the iterative training procedure. Figure: Performance of the iterative training procedure at T f =0.062T_f=0.062 for the data collected in the main manuscript. Starting from K 0 =5000K_0=5000 samples we perform iterations of K i =500K_i=500 predictions for 11 iterations. In (a) we report the cumulative number of double wells (DW), two-level systems (TLS) and non double-wells (Non-DW). In (b) we break down this result by color coding the confusion matrix of the ML model at each iteration of iterative training.We start the iterative training procedure from a sample of $K_0$  pairs for which we calculate the QS. We empirically find that $K_0\\sim 5000$  achieves a good balance between precision and time. This sample has to be balanced between DW and non-DW, but there is no need to include TLS in the initial sample. It is possible to start from a smaller $K_0$ , at the cost of a performance drop in the first few iterations of training. From the initial sample we perform a first training that takes 10 minutes for the DW classifier and 10 minutes for the QS predictor. We then use the model to predict the $K_i=500$  IS pairs with the lowest QS. We report the cumulative number of TLS, DW, and non-DW at each iteration in Fig. REF (a). From Fig. REF (b) we see that during the first iteration most of the $K_i=500$  best candidates are actually non-DW, so the model is performing poorly. This is the reason why we suggest to perform only $K_i=500$  NEBs for each iterations, otherwise the first iteration would lead to wasting time to perform calculations over non-DW. On the other hand, in the first iteration 72 TLS are already found by running 500 NEBs. This is to be compared with Ref.  in which 61 TLS were found by running $>14000$  NEBs. After the first retraining (during iteration 2) the performance of the ML model is already excellent and less than $30\\%$  of the 500 best pairs are non-DW, while 134 are newly found TLS. We also used our iterative training to reprocess the data of Ref. , and we report the results in Table 1 (main text). We show in Fig. REF  a detailed analysis of the procedure at $T_f=0.062$ . While the standard approach was able to identify 61 TLS running $>14000$  NEB+Schrödinger calculations, iterative training finds 156 TLS, by running only 2500 NEBs. This confirms that more than half of the total TLS were hidden among the pairs discarded by Ref. . In details, Fig. REF (a) shows the cumulative number of DW and TLS that we find, while Fig. REF (b) reports the confusion matrix for the different steps of iterative training. Figure: Reprocessing the data of Ref. at T f =0.062T_f=0.062 with iterative training. In (a) we report the cumulative number of double wells (DW), two-level systems (TLS) and non double-wells (Non-DW). While Ref. (red dashed line) identified 61 TLS running >14000>14000 NEBs, iterative training finds 156 TLS from 2500 NEBs. In (b) we report the confusion matrix of the 5 steps of iterative training that we run.Overall, these results demonstrate that the ML approach is not only faster than a manual filtering rule based on the transition matrix, but also much more effective. The pool of IS pairs excluded from the analysis in the original approach effectively contains a significant number of TLS. In conclusion, a ML driven exploration is not only more efficient than manual filtering, but also necessary to capture the correct statistics. Temperature and stability Our ML approach is able to predict the quantum splitting of a pair of IS from static information. It is known that TLS have different features depending on their preparation temperature, or glass stability . Here we address the following questions: (i) how difficult is it for a machine to distinguish data corresponding to different temperatures, (ii) what are the most important features for this task, and (iii) does our ML approach learn temperature-independent features? Temperature classification To understand how TLS and IS features evolve with temperature, we trained a multi-layer-perceptron (MLP) to classify the temperature corresponding to a specific pair. We find that it is possible to rapidly train a classifier reaching accuracy $>95\\%$ . Depending on the value of $M$ , the input layer is composed by $N_\\mathrm {features}$  neurons, where the features are explained in the main text. After the input layer, there are $n$  hidden fully connected layers, all of the same size $s$ . The activation function for each neuron-neuron connection is a ReLu function. The last layer is composed of 3 neurons that represent the probability that a given input belongs to one of the three classes: $T_f=0.062$  or $T_f=0.07$  or $T_f=0.092$ . The performance of the MLP are evaluated by measuring the accuracy, which is the percentage of the corrected predictions. In Fig. REF (a) we report the effect of increasing the size of the hidden layers, while in Fig. REF (b) we report the effect of a larger number of hidden layers. Figure: Temperature classifier accuracy as a function of number of hidden layers (a) and their size (b). We report results for M=5M=5. Results suggest that already with 2 hidden layers of 60 neurons it is possible to achieve an accuracy above 95%95\\%.Our results show that a MLP with 2 hidden layers of 60 neurons has a $95\\%$  accuracy after less than 1h of supervised training. The answer to question (i), is that one can identify the temperature at which a pair of IS was obtained. Static signatures of glass stability We answer question (ii) by measuring the Shapley (SHAP) values from the temperature classifier. In Fig. REF  (a) we report the SHAP value of the most important input features for the MLP prediction that considers only the $M=5$  particles that displaced the most. The smallest ($i=(M-1)=4$ ) and the largest ($i=0$ ) particle displacements emerge as the most important input, noted $\\Delta \\vec{r}_4$  and $\\Delta \\vec{r}_0$ , respectively. Their average effect on the model prediction is shown in Fig.REF (b,c). The particle that displaced the most ($i=0$ ) shows a large displacement at low preparation temperature, while the particle that displaced the least ($i=M-1$ ) exhibits a large displacement at high temperature. Our results suggest that higher temperatures are characterized by more collective rearrangements, leading to large $\\Delta \\vec{r}_4$ . In glasses prepared at low temperature instead, transitions are characterized by a particle displacing significantly more than the others. Figure: Microscopic differences originating from different temperature preparation/glass stability, quantified using the Shapley values for the temperature classifier. (a) Summary of the SHAP values for the 9 most important input features, measured for a subset of 1000 pairs processed by the TT-classifier. Predicted glass preparation temperature as a function of (b) Δr → 4 \\Delta \\vec{r}_4, and (c) Δr → 0 \\Delta \\vec{r}_0, with all other inputs taking their average value. Transferability and crossvalidation We address question (iii) by testing how our model performs when trained at $T_\\mathrm {train}$  and deployed to predict the quantum splitting of pairs at $T_\\mathrm {predict}\\ne T_\\mathrm {train}$ . In Fig. REF  each column corresponds to a training temperature: $T_\\mathrm {train}=0.092$  (left), $0.07$  (middle) and $0.062$  (right). Then, computed quantum splittings are compared with ML prediction made on samples obtained at $T_\\mathrm {predict}$ , decreasing from top to bottom. We see in Fig. REF  that the predictive power of a model trained at a different temperature is slightly lower compared to the model trained at the same temperature (Fig. REF  of main). Still, the transferability of the model is good, especially when the model is trained at $T_\\mathrm {train}>T_\\mathrm {predict}$ . This situation is the most useful, since it is easier to obtain data at higher $T$ . We conclude that if not enough data is available at low temperature, it is possible to rely on model transferability. This relies on the fact that TLS share some general temperature-independent features. In summary, we have seen that IS and TLS have different microscopic features when generated from different stabilities. It is then optimal to train the ML model at a fixed temperature only. If no easier way to measure $T$ /stability are available, it is possible to use ML to classify the stability of each IS by adding another block to the workflow, as reported in the main manuscript. Alternatively, at the cost of a finite accuracy drop it is even possible to transfer the model predictions at different temperatures and thus train where data collection is fast and easy and apply it where it is not. Figure: Transferability and crossvalidation of the machine learning model. Exact quantum splitting against ML prediction. The quantum splitting predictor is trained at T  train  T_\\mathrm {train} and deployed at T  prediction  ≠T  train  T_\\mathrm {prediction}\\ne T_\\mathrm {train}. From left to right: T  train  =0.092,0.07,0.062T_\\mathrm {train} = 0.092, 0.07, 0.062. From top to bottom, decreasing T  prediction  T_\\mathrm {prediction}. Performances are not on par with Fig. , but good in particular when T  train  >T  predict  T_\\mathrm {train}>T_\\mathrm {predict}, which is of practical interest.\n",
      "11/09/2024 20:36:18 - INFO - \t Start Positions: [259, 568, 573, 578, 583, 588, 593, 598, 603, 608, 614, 727, 748, 754, 1297, 1441, 1447, 1453, 1459, 1465, 1526, 1532, 1671, 1677, 1683, 1689, 1695, 1701, 1707, 1713, 1719, 1725, 1860, 1866, 1968, 1973, 1978, 1984, 1990, 2101, 2107, 2255, 2342, 2348, 2354, 2372, 2378, 3084, 3090, 3096, 3626, 3632, 3638, 3644, 3650, 3656, 3662, 3668, 3674, 3680, 4046, 4868, 4874, 4880, 6679, 6685, 6691, 7347, 7655, 7794, 10747, 10753, 11264, 13283, 15130, 15135, 15140, 15146, 15152, 15158, 15939, 16584, 17555, 18081, 18189, 19202, 23653, 25104, 25195, 25931, 28196, 28321, 29526, 30119, 30768, 30773, 30778, 30784, 30790, 30796, 30802, 31745, 34444, 34449, 34454, 34460, 34466, 34575, 34581, 34587, 35925, 39432, 39522, 43096, 43218, 44737, 44968, 44974, 45211, 45217, 45559, 46338, 46616, 46685, 50122, 53880, 54884, 56906, 56912, 60361, 60674, 61067, 62325]\n",
      "11/09/2024 20:36:18 - INFO - \t End Positions: [262, 571, 576, 581, 586, 591, 596, 601, 606, 612, 618, 731, 752, 758, 1301, 1445, 1451, 1457, 1463, 1469, 1530, 1536, 1675, 1681, 1687, 1693, 1699, 1705, 1711, 1717, 1723, 1729, 1864, 1870, 1971, 1976, 1982, 1988, 1994, 2105, 2111, 2259, 2346, 2352, 2358, 2376, 2382, 3088, 3094, 3100, 3630, 3636, 3642, 3648, 3654, 3660, 3666, 3672, 3678, 3684, 4050, 4872, 4878, 4884, 6683, 6689, 6695, 7351, 7659, 7798, 10751, 10757, 11268, 13287, 15133, 15138, 15144, 15150, 15156, 15162, 15943, 16588, 17559, 18085, 18193, 19206, 23657, 25108, 25199, 25935, 28200, 28325, 29530, 30123, 30771, 30776, 30782, 30788, 30794, 30800, 30806, 31749, 34447, 34452, 34458, 34464, 34470, 34579, 34585, 34591, 35929, 39436, 39526, 43100, 43222, 44741, 44972, 44978, 45215, 45221, 45563, 46342, 46620, 46689, 50126, 53884, 54888, 56910, 56916, 60365, 60678, 61071, 62329]\n",
      "11/09/2024 20:36:18 - INFO - \t --------------------------------------------------\n",
      "11/09/2024 20:36:18 - DEBUG - \t 1 abbreviations detected and kept (0 omitted)\n",
      "11/09/2024 20:36:18 - INFO - \t Action: Abbreviation replacement\n",
      "11/09/2024 20:36:18 - INFO - \t Line: 0\n",
      "11/09/2024 20:36:18 - INFO - \t Old Text: Abstract  Two-level systems (TLS) are rare quantum tunneling defects which govern the physics of glasses at very low temperature.\n",
      "11/09/2024 20:36:18 - INFO - \t New Text: Abstract  Two-level systems (Two-level systems) are rare quantum tunneling defects which govern the physics of glasses at very low temperature.\n",
      "11/09/2024 20:36:18 - INFO - \t Start Positions: [29]\n",
      "11/09/2024 20:36:18 - INFO - \t End Positions: [32]\n",
      "11/09/2024 20:36:18 - INFO - \t --------------------------------------------------\n",
      "11/09/2024 20:36:18 - DEBUG - \t 0 Error processing sentence Introduction When a glass-forming liquid is cooled rapidly, its viscosity increases dramatically and it eventually transforms into an amorphous solid, called a glass, whose physical properties are profoundly different from those of ordered crystalline solids . At even lower temperature, around 1K, the specific heat of a disordered solid is much larger than that of its crystalline counterpart as it scales linearly rather than cubically with temperature. Similarly, the temperature evolution of the thermal conductivity in glasses is quadratic, rather than cubic , , , , , , , , , . A theoretical framework rationalizing such anomalous behavior was provided by Anderson, Halperin and Varma  and by Phillips , . They argued that the energy landscape of amorphous solids contains many nearly-degenerate minima, connected by localized motions of a few atoms, that can act as tunneling defects, called two-level systems (TLS). Understanding the microscopic origin of TLS and how to control their density and physical properties has gained significant interest, not only because they provide a large contribution to the specific heat and to the thermal conductivity, but also because their presence may impact the performance of certain quantum devices . Unfortunately, understanding their microscopic nature and their almost ubiquitous presence in low-temperature glasses remains a challenge , , , , . With the development of the swap Monte Carlo algorithm ,  it has become possible to create computer glasses at unprecedentedly low temperatures. Combined with landscape exploration algorithms , , , , , , , , ,  this provides a method to investigate the nature of TLS in materials prepared under conditions comparable to experimental studies , . These tools have enabled computational studies that have confirmed the experimental observation , , , ,  that as the kinetic stability of a glass increases, the density of tunneling defects is strongly depleted , . The direct detection of TLS revealed some of their microscopic features, namely that the participation ratio decreases in more stable glasses , and that TLS do not seem to be in a one-to-one correspondence with soft harmonic , ,  or localized ,  modes. The main issue that limits the applicability of the direct landscape exploration method is that it remains computationally very expensive, and it is thus hard to construct the large library of TLS needed to enable a robust statistical analysis of their physical properties. After accumulating a large number of inherent structures (IS), one must run an expensive algorithm to find the relaxation pathway connecting pairs of IS to then determine if this pair forms a proper TLS (namely, has an energy splitting within thermal energy at 1K). Due to the large number of IS pairs detected, it is impossible to characterize all of them. In previous work, some ad hoc filtering rules were introduced , ,  but the success rate of such filters is poor. After a significant exploration effort, it was possible to identify about 60 TLS starting from the identification of $\\sim 10^8$  IS. It is then obvious that most of the computational effort has been wasted in the study of pairs that form defects which do not tunnel at low temperatures. In this paper we show that it is possible to predict with enhanced accuracy whether a pair of inherent structures forms a TLS by using machine learning techniques. Recently, machine learning , , , , , , , , ,  has been shown to be extremely effective in using structural indicators to predict structural, dynamical or mechanical properties of glassy systems. In a similar vein, we use supervised learning to streamline the process of TLS identification. Our study has two goals: (i) develop a faster way to identify TLS compared to the standard approach outlined in Ref.  and described in Sec. , in order to collect a statistically significant number of TLS; (ii) understand what are the structural and dynamical features characterizing TLS, and if/how they change with the preparation temperature. To address (i) we show that our machine learning model can be trained in a few minutes using a small quantity of data, after which the model is able to identify candidate TLS with high speed and accuracy. To address (ii) we show which static features are the most important for the model prediction and we show that states which are dynamically distant can nevertheless be TLS. We conclude by explaining how the ML model distinguishes TLS from non-TLS and how it is able to identify glasses prepared at different temperatures. Standard approach to TLS identification The standard procedure , ,  to identify TLS is sketched in Fig. REF . It consists of the following first steps which aim at identifying potential candidates for TLS:  Equilibrate the system at the preparation temperature $T_f$ . Glasses with lower $T_f$  have a larger glass stability. Run molecular dynamics to sample configurations along a dynamical trajectory at the exploration temperature $T<T_f$ . Perform energy minimization from the sampled configurations to produce a time series of energy minima, or inherent structures (IS). Analyze the transition rates between pairs of IS, and select the pairs of IS that are explored consecutively. Step 4 was necessary because it is computationally impossible to analyze all pairs of IS, as the number of pairs scales quadratically with the number of minima. The filter defined in step 4 was physically motivated by the fact that TLS tend to originate from IS that are not too distinct in order to have a reasonable tunneling probability. As such it is likely that those pairs of IS get explored one after the other during the exploration dynamics in step 2. Overall, given $N_{IS}$  inherent structures, this procedure selects for $\\mathcal {O}(N_{IS})$  pairs to be analyzed. However, many pairs of IS can be close but not sampled consecutively during the dynamics, owing to the complex structure of the potential energy landscape. Instead, our machine learning approach allows to consider all pairs of IS. As shown below, our approach is able to detect TLS which are otherwise excluded by the above step 4. Once potential candidates are selected, the procedure continues as follows:  For each selected pair of IS, look for the minimum energy path and the classical barrier between them by running a minimum energy path finding algorithm, such as nudge elastic band (NEB) , , . This provides the value of the potential along the minimum energy path between the pair $V(\\xi )$ , where $0\\le \\xi \\le 1$  is the reaction coordinate. Select pairs whose energy profile $V(\\xi )$  has the form of a double well (DW), i.e., exclude paths with multiple wells. Solve the one-dimensional Schrödinger equation: $-\\frac{\\hbar ^2}{2md^2\\epsilon }\\partial ^2_{\\xi } \\Psi (\\xi )+V(\\xi )\\Psi (\\xi )=\\mathcal {E}\\Psi (\\xi ),$  where $\\xi $  is a normalized distance along the reaction path $\\xi =x/d$  and energy is normalized by a Lennard-Jones energy scale $\\epsilon $ , the effective mass $m$  and the distance $d$  are calculated as in Ref. . We obtain the quantum splitting (QS) $E_{qs}=\\mathcal {E}_2-\\mathcal {E}_1$  from the first two energy levels $\\mathcal {E}_1$  and $\\mathcal {E}_2$ . The quantum splitting is the most relevant parameter because when $E_{qs} \\sim T$  the system can go from one state to the other via quantum tunneling , creating what is called a two-level system (TLS). In particular since we choose to report the data in units that correspond to Argon , a double well excitation will be an active TLS at $T=1$ K when $E_{qs} < 0.0015 \\epsilon $ , where $\\epsilon $  sets the energy scale of the pair interactions in the simulated model. Overall, since at low temperature the landscape exploration dynamics is slow, one would like to spend most of the computational time by doing steps 2-3 to construct a large library of pairs of IS. A first problem is that when the library of IS grows larger it takes a lot of time to perform steps 5-7. Moreover, the main bottleneck lies in the fact that most of the pairs that go through the full procedure are not found to be TLS in the end, and so this large computational time is effectively wasted. Machine learning approach to TLS identification As in any machine learning (ML) approach, we distinguish two phases: training and deployment. Our supervised training approach, detailed in the next section, takes just a few hours of training on a single CPU. It requires an initial dataset of $\\mathcal {O}(10^4)$  full NEB calculations, whose collection is the most time consuming part of the training phase. Once training is complete, the ML model can be deployed to identify new TLS. Its workflow is similar to the standard one, with some major improvements. It proceeds with the following steps:  - 3. The first 3 steps are similar to the standard procedure to obtain a collection of inherent structures from a dynamical exploration. Apply the ML model to all possible pairs of IS to predict which pairs form a DW potential. Apply the ML model to predict the quantum splitting (QS) for all predicted DW and filter out the configurations that are not predicted to be TLS by the ML model. - 8. Run NEB, select pairs that form DW potential and solve the one-dimensional Schrödinger equation for the TLS candidates only in order to obtain the exact value of the quantum splitting. It is possible to use steps 4-5 as a single shot or as an iterative training approach (see Sec. REF ). In the Supplementary Information (SI) we provide details on how steps 1-3 are performed: glass preparation, exploration of the potential energy landscape via molecular dynamics simulations and minimization procedure, as well as NEB computation, see Sec. . Noticeably, if the model is well-trained then the ML approach has two significant advantages over the standard approach. First, $\\mathcal {O}(N_{IS}^2)$  pairs of IS are scanned to identify TLS, compared to a much smaller number $\\mathcal {O}(N_{IS})$  in the standard procedure. Second, if a pair of IS passes step 5 and goes through the full procedure it is very likely to be a real TLS. As a consequence, by using the ML approach one can spend more time doing steps 2-3 to produce new IS, since fewer pairs pass step 5. At the same time, for any given number of IS, the ML approach can analyze all possible pairs and is therefore able to identify many more TLS, as we demonstrate below. Machine learning model In Refs. , , the authors analyze a library of 14202, 23535 and 117370 pairs of inherent structures for a continuously polydisperse system of soft repulsive particles, equilibrated at reduced temperatures $T = 0.062$ , 0.07, and 0.092, respectively. The standard approach (Sec. ) leads to the identification of 61, 291 and 1008 TLS for the three temperatures, respectively. Notice that this approach uses pairs of IS that are selected by the dynamical information contained in the transition matrix between pairs of IS . This was done to filter out all non double well potential. For all pairs in this small subset, the quantum splitting was then calculated. Instead, the ML approach starts by independently evaluating the relevant information contained in each IS and constructs all possible combinations, even for pairs that are not dynamically connected. Following the steps discussed in Sec. the model is then able to predict the quantum splitting of all the pairs, that were predicted to form a DW, very accurately. From a quantitative perspective, this means that the same trajectories now contain many more TLS candidates in the ML approach compared to the standard approach. Figure: Flowchart of the machine learning approach. The first block represents the construction of the dataset by comparing all the pairs of inherent structures, focusing on the MM particles that displace the most. Then, required features are extracted to construct the input vector XX. We then train a classifier to predict whether a pair of IS is a DW or not. The DW are finally processed using a multi-layer stacking strategy to predict the quantum splitting energy. Our pipeline analyses a given pair of IS in about ∼10 -4 \\sim 10^{-4}s.In this section we describe the flowchart of the model summarised in Fig. REF . We first discuss how we construct the dataset and extract the relevant features (Sec. REF ). We then explain the two main blocks consisting in a DW classifier followed by a quantum splitting predictor, both of which have similar model architecture and inputs. Finally we evaluate the performance of this model by showing its ability to accurately predict the quantum splitting. We conclude by introducing the iterative training technique that represents the optimal way to efficiently expand the library of TLS. Dataset and features construction The first step is the evaluation of a set of static quantities for all the available IS. This set consists of: energy, particle positions, averaged bond-orientational order parameters  determined via a Voronoi tessellation, from $q_2$  to $q_{12}$ , and finally particle radii. The cost of this operation scales as the number of available states $N_{IS}$ , but we use these quantities to calculate the features of $\\sim N_{IS}^2$  pairs. A detailed analysis (see Sec. in the SI) shows that the bond orientational parameters and the particle sizes are not very useful for the ML model. Since their calculation is slower than all the other features, we do not include them in the final version of the ML approach. To construct the input features for each pair of IS we combine the information of the two states evaluating the following:  Energy splitting $\\Delta E$ : energy difference between the two IS. Displacements $\\Delta \\vec{r}_i$ : displacement vector of particle $i$  between the two configurations. Total displacement $d$ : total distance between the two IS defined as $d^2=\\sum _i |\\Delta \\vec{r}_i|^2$ . Participation ratio $PR$ : defined as $PR=(d^2)^2/\\left( \\sum _i |\\Delta \\vec{r}_i|^4\\right)$ . Distance from the displacement center $|\\vec{r}_0-\\vec{r}_i|$ : we measure the average distance of particle $i$  from the center of displacement $\\vec{r}_0$ , identified as the average position of the particle that moves the most. This quantity identifies the typical size of the region of particles that rearrange. Transition matrix $T_{ij}$  (and $T_{ji}$ ): number of times that the exploration dynamics traverses IS$_j$  just after IS$_i$  (and vice versa). The crucial step of the feature construction is that we can reduce the number of features by considering only the $M$  particles whose displacement is the largest between pairs of IS. We make this assumption because we expect that the low temperature dynamics is characterized by localised rearrangements involving only a small fraction of the particles , , , , , . In Sec. of the SI, we confirm this assumption by showing that the ML model achieves optimal performances even when $M$  is very small. So, the choice of $M \\ll N$  makes the ML model computationally effective without any performance drop. Double well classifier A necessary condition in order for a pair of IS to be a TLS is that the transition between the pair forms a double well (DW) potential. A DW is defined when the minimum energy path between the two IS resembles a quartic potential, as sketched in Fig. REF (c). The final goal of the ML model is to predict the quantum splitting of the pair to identify pairs with low values of the QS. The first obstacle in the identification of TLS is that DW represent only a small subgroup of all IS pairs. For instance in Ref. , only $\\sim 0.5\\%$  of all the IS pairs are DW at the lowest temperature. It is then mandatory to filter out pairs that are not likely to be a DW. In the machine learning field there are usually many different models that can be trained to achieve similar performances, with complexity ranging from polynomial regression to deep neural networks. Here, we perform model ensembling and use ensembles both for DW classification and QS prediction. Model ensembling consists in averaging the output of different ML models to achieve better predictions compared to each of them separately. We do so using the publicly available AutoGluon library . In this approach, we train in a few minutes a single-stack ensemble that is able to classify DW with $>95\\%$  accuracy. In Sec. of the SI we justify this choice of ML model and provide details on performances and hyperparameters. Overall, since the DW classifier is accurate and rapid, we use it to filter out the pairs that do not require the attention of the QS predictor because they cannot be TLS anyway. Quantum splitting predictor We want to predict the quantum splitting of a pair of IS for which the features discussed in Sec. REF  have been computed. We need this prediction to be very precise, because we know that a pair can be considered a TLS when $E_{qs}<0.0015 \\epsilon $ , but $E_{qs}$  can vary significantly so errors may be large. In the SI (see Sec. ) we show that models such as deep neural networks and regression are not stable or powerful enough to achieve satisfying results. We thus perform model ensembling by using the AutoGluon library . This achieves superior performances while also allowing us to compare and rank single models. We perform two types of model ensembling: (i) stacking: different layers of models are applied in series creating a stack (schematized in Fig. REF ), and (ii) bagging: we divide the data in subsets that we use to train multiple instances of the same models and we later combine all their predictions. As shown in the SI, the best results are obtained while using ensembles of gradient boosting methods (in particular CatBoost ), which have proven to be the optimal choice in similar semi-empirical quantum-mechanical calculations . Overall, the ensemble structure of our final model relieves us from hyperparameter optimization and makes the results more stable. Figure: (a)-(c) Quantum splitting and (d)-(f) energy barrier predicted by the ML model compared to the exact value, reported using a double logarithmic scale. We report predictions for IS pairs that the ML model has not seen during the training. (a) and (d) correspond to T f =0.062T_f=0.062, with training for 7000 samples and information of the M=3M=3 particles with largest displacements. (b) and (e) correspond to T f =0.07T_f=0.07, 10000 samples and M=3M=3. (c) and (f) correspond to T f =0.092T_f=0.092, 30000 samples and M=3M=3. All models have been trained for ∼10\\sim 10 hours of single CPU time.In order to train the model we first collect a set of $E_{qs}$  examples. The size of this training set is discussed in the SI (see Fig. REF ) where we find that the minimum number is around $10^4$ . We can use some of the data already collected in previous work in Ref.  for the training. Moreover, since we are interested in estimating with more precision the lowest values of $E_{qs}$  we train the model to minimize the following loss function $\\mathcal {L} = \\frac{ \\sum _{i=1}^n w_i \\left( E_{qs,\\mathrm {true}} -E_{qs,\\mathrm {predicted}} \\right)^2 }{n \\sum _{i=1}^n w_i },$  which is a weighted mean-squared error. The weights correspond to $w_i = 1/E_{qs,\\mathrm {true}}$  in order to give more importance to low $E_{qs}$  values. We thus train our model to provide a very accurate prediction of the value $E_{qs}$  for any given pair. Once the model is trained it takes only $\\sim 10^{-4}$ s to predict the QS of a new pair (compared to 1 minute to run the standard procedure). If we predict a value $E_{qs}<0.0015\\epsilon $ , then we have identified a TLS much faster. To showcase the performance of the ML model we report in Fig. REF (a)-(c) the exact quantum splitting calculated from the NEB procedure, compared with the value predicted by the model. We have trained three independent models to work at the three different temperatures. As explained before (Sec. REF ), the model needs the information about only the $M \\ll N$  particles that are displaced the most to achieve the excellent precision demonstrated in Fig. REF . In the Fig. REF  (SI) we find that the optimal value is $M=3$ , confirming that the participation ratio in TLS is quite low, because only three particles are needed for the model to identify TLS. Furthermore, the models have been trained using the smallest number of samples, randomly selected from all the IS pairs available, that allows the model to reach its top performance. We have also performed an analysis of the optimal training time. Details on these points are provided in Sec. of the SI. The performances presented in Fig. REF  are achieved by training the model for $\\sim 10$  hours of single CPU time, but we also show in the SI that it is possible to already achieve $>90\\%$  of this performance by training the ensemble for only 10 minutes. The ML approach that we have just introduced is also easily generalizable to target any state-to-state transition, like excitations and higher energy effects. Here we modified the quantum splitting predictor to instead predict the classical energy barrier between two IS states. If the minimal energy path between two IS forms a DW, we define the classical energy barrier as the maximum value of the energy along this path. In Fig. REF (d)-(f) we report the value of the energy barrier predicted by the ML model (y-axis) compared to the exact value calculated from the NEB procedure (x-axis). The hyperparameters and the features are the ones used for the quantum splitting predictor. Such a high performance demonstrates that our ML approach can predict other types of transitions between states. Iterative training procedure We finally introduce an approach to optimally employ our ML model to process new data: the iterative training procedure. In previous sections and in Fig. REF  we trained the model once using a subset of the already available data. This is a natural way to proceed when the goal is to process new data that are very similar to the training set, and the training set is itself large enough. However, since the goal of the proposed ML model is to ultimately drive the landscape exploration and collect new samples, the single-training approach may encounter two types of problems. First, at the beginning there may be not enough data and second the findings of the model do not provide any additional feedback. To solve both problems we introduce the iterative training procedure. The idea of iterative training is to use the predictive power of ML to create and expand its own training set, consequently enhancing its performance by iteratively retraining over the new data. Details on the method and parameters are discussed in Sec. of the SI. In practice, we start from a training set of $K_0 \\sim 10^3-10^4$  randomly selected pairs to have an initial idea of the relation between input and output. We then use the ML approach outlined in Fig. REF  to predict the $K_i=500$  pairs with the lowest QS. For these TLS candidates, we perform the full procedure to calculate the true QS and determine whether the pair is a DW or a TLS. In Fig. REF  (SI), we report the result of this procedure when we process a new set of trajectories from the same polydisperse soft sphere potential as in Ref. . In general the first few iterations of iterative training have a poor performance. In fact we find that $>70\\%$  of the first $K_i$  pairs are actually non-DW. After collecting additional $K_i$  measurements, we retrain the model. We report in Tab. REF  the average time for each step of the ML procedure. The retraining can be done in $\\sim 10$  min, after which the model is able to predict the next $K_i$  pairs with lowest QS. Overall, to process $N_\\mathrm {IS pairs}$  we estimate that the computational time of the iterative approach is $t_{i}=\\left[ K_0\\cdot 10^{2} + N_{iter}\\left(K_i\\cdot 10^{2} + 10^{3} + N_\\mathrm {IS pairs}\\cdot 10^{-5} \\right) \\right]s$ . If $N_\\mathrm {IS pairs}>10^{9}$  it is possible to significantly reduce $t_i$  by permanently discarding the worst pairs, but this is not needed here. We iterate this procedure $N_{iter}$  times, until the last batch of $K_i$  candidates contains less than $1\\%$  of the total number of TLS. We believe that continuing this iterative procedure would lead to the identification of even more TLS/DW, but this is out of the scope of this paper. Table: Computational time needed to perform our ML approach, on a standard laptop. Results and discussion We now use ML in order to speed up the TLS search. This highly efficient method allows us to collect a library of TLS of unprecedented size, generated from numerical simulations with the same interaction potential as in Ref. . First of all, we reprocess the data produced to obtain the results presented in Ref.  with our new ML method, obtaining new information about the connection between TLS and dynamics. Next, we perform ML-guided exploration to collect as many TLS as possible. This sizable library of TLS allows us to perform for the first time a detailed statistical analysis of TLS and compare their distribution to the distribution of double wells. We perform this analysis for glasses of three different stabilities. Finally, we discuss the microscopic features of TLS not only by looking at their statistics, but also by analyzing what the ML model has learned, and how it expresses its prediction. Capturing elusive TLS with machine learning Prior to this paper, it was not possible to evaluate all the IS pairs collected in Ref. . For this reason the authors discarded a priori all pairs where the number of forward and backward jumps between IS is such that $\\min \\left(T_{ij},T_{ji}\\right)<4$ , based on the assumption that high transition rates during the dynamic landscape exploration is a good indicator that two IS form a DW. This reduced the number of pairs to 14202, 21109 and 117339 for glasses prepared at $T_f=0.062$ , 0.07 and $0.092$  respectively. In order to have comparable data at the three temperatures, for $T_f=0.092$  we only consider a subset of glasses corresponding to 30920 IS pairs. The results of the TLS search are summarized in the red columns of Tab. REF . Overall the standard procedure reaches a rate of TLS found over calculations performed of $4 \\cdot 10^{-3}$ , $13\\cdot 10^{-3}$ , and $8\\cdot 10^{-3}$  for the three temperatures, respectively. We compare this with our iterative training procedure applied on the same data, whose results are reported in the green columns of Tab. REF . We immediately notice two major improvements: the overall number of TLS that we find from the same data set is more than twice larger and the ratio of TLS per calculation is more than 15 times larger, corresponding to $62\\cdot 10^{-3}$ , $211\\cdot 10^{-3}$ , and $194\\cdot 10^{-3}$  for the same three temperatures. We conclude that the iterative ML approach is much more efficient than the standard procedure, and also that TLS do not necessarily have a large dynamical transition rate, since the dynamical-filtering approach misses more than half of them. Table: Analysis of data collected in Ref. . We compare the standard procedure with our ML approach using iterative training. We report results for different glass stabilities, decreasing from top to bottom, using Argon units (Ar)  and NiP metallic glass parameters (NiP) . The standard procedure finds less than half of the TLS and is computationally much more expensive. Differences between DW and TLS With our ML-driven exploration of the energy landscape we can restrict the numerical effort to DW and favorable TLS candidates, while processing a larger number and/or longer exploration trajectories. This allows us to consider a larger set of independent glasses of the same type as those treated in Ref. , which is particularly relevant for ultrastable glasses generated at the lowest temperature $T_f=0.062$ . While in Ref.  the collection of 61 TLS required $>14000$  NEB calculations, we are able to identify 864 TLS running 11 iterations of iterative training using only a total of 5500 NEB calculations in addition to the $\\sim 6000$  used for pretraining. In the next section we analyze these results to discuss the nature of TLS. Figure: Results of ML driven exploration, leading to a library of TLS of unprecedented size. (a) We compare the model predictions to the calculated values at the end of our iterative training, at T f =0.062T_f=0.062. We color coded in the background the confusion matrix. The black dashed horizontal lines report the percentage of TLS that are predicted below that value of quantum splitting, showing that more than 95%95\\% of the TLS are within twice the TLS threshold of 0.00150.0015. (b) Cumulative distribution of energy splitting n(E qs )n(E_{qs}) divided by E qs E_{qs}. (c) Histograms of the number of TLS and DW per glass, at the three preparation temperatures. In total we have considered 237, 30, 5 glasses equilibrated at T f =0.062T_f=0.062, 0.07, and 0.092 respectively.The database of glasses that we analyze with iterative training contains 5 times more minima than in Ref. , but we are able to find 15 times more TLS running around half of the NEB calculations. Furthermore, in Fig. REF (a) we have color-coded the confusion matrix and we highlight that we can capture almost all of the TLS if we consider only the pairs that are predicted to be within twice the quantum splitting threshold of TLS. In Fig. REF (b) we report the saturation plateau of the cumulative density of TLS quantum splitting $n(E_{qs})$ , which scales as $n(E_{qs}) \\sim n_0 E_{qs}$  at low $E_{qs}$ . The ML approach allows us to collect significantly better statistics compared to Ref. , confirming that the TLS density $n_0$  decreases by several orders of magnitude from hyperquenched to ultrastable glasses. Lastly, in Fig. REF (c) we report the histograms of the number of TLS and DW per glass at the three temperatures. We see that when the glasses are ultrastable ($T_f=0.062$ ) most of the glasses have very few TLS. Conversely, poorly annealed ($T_f=0.092$ ) glasses show a very unbalanced distribution, with few glasses that contains most of the DW and TLS. Interpretation of the ML model It is possible to use the ML model to gain information about the distinctive structure of TLS. First, the present and previous works , , , , , ,  find that the density of TLS decreases upon increasing glass stability, which in our simulations is controlled by the preparation temperature. Thus, one may also expect temperature-dependent TLS features. In the SI we show that when the ML model is trained at $T_\\mathrm {train}$  and deployed at $T_\\mathrm {prediction} \\ne T_\\mathrm {train}$  there is only a minor performance drop and the model is able to perform reasonably well. This implies that the model captures distinctive signatures of TLS that do not depend strongly on the preparation temperature. Yet, we also show in the SI that it is very easy to train another ML model to predict the temperature itself and eventually add it to the pipeline. Overall, the ML model is not only able to capture the different microscopic features of TLS, but it can also suggest what is the specific influence of each feature. To interpret this information we calculate their Shapley values  and we report them in Fig. REF . The features are ranked from the most important (top) to the less important (bottom) reporting the impact that they have on the model output (SHAP value), so that a positive SHAP value predicts on average a high value of the QS. We see that the most important feature is the classical energy splitting $\\Delta E$  and that a large splitting (red) corresponds to a large QS. The second most important feature is the largest single particle displacement $\\Delta \\vec{r}_0$ , which has to be larger than a threshold corresponding to $0.3\\sigma $  in order to predict a low QS. The total displacement $d$  is the third most important and shows a similar effect. All the remaining features have a less clear and much smaller effect on the model prediction and they only collaborate collectively to the final QS prediction. In the SI we show that it is possible to obtain very good performance even when removing some of the features with the largest Shapley values, which means that the ML interpretation is not unique. Figure: Importance of the different features for the ML model for the prediction of the quantum splitting, evaluated from the Shapley values  at T f =0.062T_f=0.062. The features are ranked from the most important (top) to the least important, where the impact that they have on the model output corresponds to their SHAP value. Each point corresponds to a single IS pair. The color coding shows the impact from high (red) to low (green) values of each specific input.According to this Shapley analysis we explain the ML prediction in the following way: the energy difference $\\Delta E$  between two IS is the main predictor for the quantum splitting, and it has to be small for TLS. Then, the largest particle displacement $\\Delta \\vec{r}_0$  is necessary to understand if the two IS are similar and what is their stability (we show in the SI that $\\Delta \\vec{r}_0$  is the most important feature to identify the glass stability). Then the total displacement $d$  complements this information and gives local information about the displacements of the other particles. Lastly, all the other inputs provide fine tuning to refine the final prediction and are discussed in more detail in the SI. Interestingly, in the SI we also show that even without the two most important features, the ML approach can still reasonably reveal TLS candidates. Microscopic features of TLS We have shown that by following a ML-driven approach it is possible to collect a significant library of TLS for any preparation temperature. However it may be useful to discuss alternative strategies to rapidly identify TLS. In general, since TLS are extremely rare objects , , , ,  a filtering rule is necessary in order to reduce the number of possible candidates. In particular, Ref. , ,  proposed to use the transition matrix to exclude pairs that do not get explored consecutively. This is based on the assumption that DW (and consequently TLS) correspond to IS pairs that are close to each other and therefore during an exploration run should be detected successively, hence have a non zero transition rates. Instead, we prove in Fig. REF  that a filter based on dynamical information only is a poor predictor. In Fig. REF (a) we report the distribution of dynamical transitions between two inherent structures $i$  and $j$  for TLS, DW and all the pairs, measured at $T_f=0.062$ . While the slowly decaying tail of TLS and DW suggests that they often exhibit a large transition rate, actually most of the TLS and DW are characterized by no transition at all between them. Our interpretation is that even though the transition towards the other side of the double well is favourable, the landscape has such a large dimensionality ($3N$ ) that even very favorable transitions may never take place in a finite exploration time. This issue can become more severe when the trajectories are shorter, for example if the exploration is performed in parallel. We confirmed this observation by the results that are already reported in Tab. REF , where we have used our iterative training approach to re-analyze the data of Ref. , including pairs with no transition, thus finding many more TLS. We conclude that even though the transition rates are the most important single characteristics for DW prediction (see Fig. REF ), a filter based solely on them is still missing a lot of interesting pairs and therefore is not the most efficient. In Fig. REF (b) we focus on the distribution of classical splitting $\\Delta E$ . When $\\Delta E$  is large we rarely see DW and TLS (red region). On the other hand, there are many pairs that show a very small $\\Delta E$  value (yellow region), but they are not more likely to be TLS. Ultimately we find a `sweet spot' (green region), where TLS are more frequent. The ML model also captures this feature, as we can see from the SHAP parameter of $\\Delta E$  in Fig. REF . The next most important feature according to the ML model is the largest particle displacement $\\Delta \\vec{r}_0$ , reported in Fig. REF (c). When it is larger than $\\sim 0.8\\sigma $  we rarely find TLS and DW, but we do not find them also when $\\Delta \\vec{r}_0<0.3\\sigma $ . The SHAP parameter in Fig. REF  confirms that the ML model has discovered this feature. Finally in Fig. REF (d) we report the total displacement $d$ . If $d>0.9\\sigma $  the pair is so different that it is not likely to be a TLS or DW, while this probability increases for smaller $d$ . Overall, if it is necessary to identify TLS with a `quick and dirty' method, we propose to use a transition matrix to filter DW from non-DW, and then select a sweet spot for the energy splitting and the displacement for selecting optimal TLS candidates. Figure: Microscopic features of TLS and DW for ultrastable glasses at T f =0.062T_f=0.062. We report the probability distribution functions of: (a) number of transitions between the two inherent structures T ij T_{ij} and T ji T_{ji}, (b) classical energy splitting ΔE\\Delta E, (c) largest particle displacement Δr → 0 \\Delta \\vec{r}_0 and (d) total displacement dd. We color coded in red the regions of parameters where we do not expect to find TLS, which instead concentrate in the green regions. These green regions could serve as an alternative to rapidly identify TLS. Conclusion In this paper we have introduced a machine-learning approach to explore complex energy landscapes, with the goal of efficiently locating double wells (DW) and two-level systems (TLS). We demonstrate that it is possible to use ML to rapidly estimate the quantum splitting of a pair of inherent structures (IS) and accurately predict if a DW is a TLS or not. We also show that our ML approach can be used to predict very accurately the energy barrier between pairs of IS. Overall, this approach allows us to collect a TLS library of unprecedented size that would be impossible to obtain without the support of ML. The ML model uses as input information calculated from a pair of inherent structures. After just a few minutes of supervised training it is able to infer with high accuracy the quantum splitting of any new pair of inherent structures. We establish that the ML model that we develop using the Autogluon library is fast and precise. Its efficiency allows us to introduce an iterative training procedure, where we perform a small batch of prediction and then retrain the model. After performing statistical analysis over the unprecedented number of TLS collected with our method, we have discovered that many DW and TLS are not consecutively explored during the dynamics. We then reanalyzed the data of Ref.  finding that more than half of the TLS had been missed, because the analysis of Ref.  was based on this dynamic assumption. Our ML approach not only finds more than twice the number of TLS from the same data, but it also requires significantly fewer calculations. Overall we conclude that ML significantly improves the existing approaches. It also shows that if it is not possible to use the ML procedure that we outline, an effective `quick and dirty' way to predict TLS should be: a) use $T_{ij},T_{ji}$  for predicting DW; b) for predicted DW use energy splitting between two IS for predicting TLS. We also discuss the microscopic nature of DW and TLS. We perform a Shapley analysis to dissect the ML model and understand what it learns, and we compare this with the extended statistics of TLS that we are able to collect. We find that the quantum splitting is mostly related to the classical energy splitting and the displacements of the particles. Overall, the Shapley analysis suggests that TLS are characterized by one particle that displaces between $0.3$  and $0.9$  of its size, while the total displacement and the energy difference between the two states remains small. The local structure around the particle is not as important, nor is the number of times we actually see this transition during the exploration dynamics. Lastly we investigate the effect that glass stability (equivalent to the preparation temperature in our simulations) has on double wells and TLS. The ML model learns that at higher temperatures all the pairs are characterized by more collective rearrangements, but TLS are similar for any preparation temperature. Ultimately, since our ML approach is extremely efficient in exploring the energy landscape and is easy to generalize to target any type of state-to-state transition (as we show for the energy barriers), we hope that our method will be used in the future to analyze not only TLS, but also many other examples of phenomena related to specific transitions between states in complex classical and quantum settings. We thank E. Flenner, G. Folena, M. Ozawa, J. Sethna, S. Elliott, G. Ruocco and W. Schirmacher for useful discussions. This work was granted access to the HPC resources of MesoPSL financed by the Region Ile de France and the project Equip@Meso (reference ANR-10-EQPX-29-01) of the programme Investissements d’Avenir supervised by the Agence Nationale pour la Recherche. This project received funding from the European Research Council (ERC) under the European Union’s Horizon 2020 research and innovation program, Grant No. 723955 – GlassUniversality (FZ), and from the Simons Foundation (#454933, LB, #454955, FZ, #454951 DR) and by a Visiting Professorship from the Leverhulme Trust (VP1-2019-029, LB). CS acknowledges support from the Herchel Smith Fund and Sidney Sussex College, University of Cambridge. Model We study a three-dimensional polydisperse mixture of particles interacting via the potential $v(r_{ij}) = \\epsilon (\\sigma _{ij}/r_{ij})^{12}+\\epsilon F(r_{ij}/\\sigma _{ij}),$  only if $r_{ij}=1.25\\sigma _{ij}$ . We use non-additive interactions $\\sigma _{ij} = 0.5(\\sigma _i + \\sigma _j ) (1 - 0.2|\\sigma _i - \\sigma _j |)$ . The function $F$  is a fourth-order polynomial ensuring continuity of $v(r_{ij})$ , its first and second derivatives, at the interaction cutoff. The particle diameters $\\sigma _i$  are drawn from the normalized distribution $P(0.73 < \\sigma <1.62) \\propto 1/\\sigma ^3$ . This model glassformer is efficiently equilibrated with the particle-swap Monte Carlo algorithm, with a fluid robust against fractionation and crystallization down to low temperature . We employ the swap Monte Carlo algorithm implemented in the LAMMPS package, with the optimal parameters provided in . With such parameters supercooled liquid configurations can be generated down to $T =$  0.062, below the experimental glass transition $T_g=0.067$ . In this work, we use configurations prepared in equilibrium conditions at temperatures $T_f = 0.092, 0.07, 0.062$ . MD dynamics To explore the energy landscape of glasses generated at $T_f$ , we use classical Molecular Dynamics (MD) simulations. Glasses are first thermalized to $T_{MD}$  = 0.04 using a Berendsen thermostat. Such temperature is low enough to suppress diffusion, but sufficiently high for quick exploration of the energy landscape. We then run MD simulations in the NVE ensemble, using an integration time step of $dt =$  0.01 (LJ time units). Configurations along the MD trajectory are used as the starting point for energy minimization via a conjugate gradient algorithm, which brings them to their inherent structure (IS). MD configurations are minimized every 20, 10, 5 time steps for $T_f$  = 0.062, 0.07, 0.092 respectively. The values are a compromise between too frequent minimization which sample the same IS many times consecutively, and too infrequent which would miss intermediate IS. For each initial configuration we perform 100,100,200 MD runs with different initial velocities for $T_f$  = 0.062, 0.07, 0.092, respectively. Each run lasts 40000, 100000, 10000 time steps. We performed our calculations on 200, 50, 5 glasses for $T_f$  = 0.062, 0.07, 0.092. For $T_f=0.092$  glasses we used a subset of the original data obtained in . NEB To analyze the transition between two IS we compute the multi-dimensional minimum energy path separating them, along with its one-dimensional potential energy profile. This is done by the nudged elastic band (NEB) method ,  implemented in the LAMMPS package. We use 40 images to interpolate the minimal energy path, that are connected by springs of constant $\\kappa =1.0$  $\\epsilon \\sigma ^{-2}$ , and use the FIRE algorithm in the minimization procedure , . Comparison between ensembled and non-ensembled models In the main manuscript we discuss how to transform the problem of computing the quantum splitting of two inherent structures into a supervised learning regression problem. Still, many machine learning models can in principle answer this question. We opted for the AutoGluon library  which is based on model ensembling to find an optimal solution. Model ensembling consists in averaging the output of different ML models and create an ensemble-averaged model that outperforms each one of its components. In particular, the AutoGluon library performs model ensembling in the form of stacking (stacking different layers of models that are applied in series) and bagging (training multiple instances of the same model on different subsets of data and then combining their prediction). In Fig. REF (a) we report the most performant ensembling after 20 hours of training, at $T_f=0.062$ , $M=5$  and $N_\\mathrm {samples}=10^4$ . The R2-score reported in the figure shows that the most performant model is a bagged ensemble of CatBoost, a gradient boosting method , which then corresponds to the final WeightedEnsemble for this training instance. Notice that in the main paper, we always use the best ensembled model. In addition to CatBoost, very good performances are also usually achieved by LightGBM, another gradient boosting method , and ExtraTrees, which is an extremely randomized tree ensemble . Overall, gradient boosting methods typically achieve the highest R2-score. Thus, in the main text when we report results obtained with the `best models', we refer to ensembles of gradient boosting methods assembled with Autogluon. Figure: Comparison of different ML model ensembles to predict the quantum splitting at T f =0.062T_f=0.062 using M=5M=5 and N  samples  =7·10 4 N_\\mathrm {samples}=7\\cdot 10^4. (a) R2-score of the test set obtained for an ensemble of models. The full collection was trained for 20 hours of CPU time. In the main manuscript we use the WeightedEnsemble. (b) An optimal Neural Network (MLP with 10 hidden layers of size 1.1·N  features  1.1\\cdot N_\\mathrm {features}) does not produce good predictions (R2<0R2<0).We motivate our choice of complex ensemble model constructed with Autogluon by showing that simpler models do not perform at the same level. For the system at $T_f=0.062$  and in the optimal condition corresponding to $M=5$  and $N_\\mathrm {samples}=7000$  the simplest model to at least capture some correlation was a Multi-layer-perceptron (MLP) with 10 hidden layers of size $1.1\\cdot N_\\mathrm {features}$ , using the same input and output structure as in the main text. A first disadvantage of this approach is that several additional hyperparameters have to be selected, such as learning rate, number and size of hidden layers, activation function, etc. We varied these parameters and report in Fig. REF (b) results for the model with the best performance. This optimal MLP was trained until the iteration per epoch was consistently below $10^{-10}$ , corresponding to $\\sim 10$  hours. Still, the results in Fig. REF (b) demonstrate that the MLP performance is much worse than that of the ensemble models used in the main text. In addition, it is much slower to train. While in this case the predictions could be scaled by a constant to achieve significant improvement, the problem with such an approach is that it is not reliable, and this scaling constant is not known a priori adding an additional layer of complication. For this reason we prefer to use more complex machine-learning models. Parameters to predict the quantum splitting A significant advantage of the model ensemble approach is that most of the hyperparameters are automatically optimized by the ensembling. Still, some degrees of freedom have to be fixed. In particular we need to fix the number of particles to be considered ($M$ ), how many samples to use ($N_\\mathrm {samples}$ ) and for how long to train the ensemble. In the next sections we motivate the choices reported in the main text. Figure: Optimizing the Quantum splitting predictor. Effect of (a) the number of particles MM, (b) number of samples N  samples  N_\\mathrm {samples} and (c) training time on performance, while other parameters assume their optimal value. We report the R2-score after training on the data of Ref. . Overall we conclude that optimal performances can be achieved for M=3M=3, ∼10 4 \\sim 10^4 samples in just 10 minutes of training. $M$ : input particles – In Fig. REF (a) we report the effect of $M$ , the number of particles considered in the ML procedure. A large $M$  hinders the performance of the ML model because it has to process a larger input vector. The peak performance is achieved at $M=3$ , which is a relatively low number of particles to define a TLS. This confirms that the participation rate in TLS is low . $N_\\mathrm {samples}$ : number of samples – To evaluate the optimal value of $N_\\mathrm {samples}$  we report in Fig. REF (b) the variation of the R2-score upon using more samples for training. Notice that we keep the other parameters in the optimal condition for each temperature. The shape of the curves let us conclude that a good number of training samples is $7000, 10000, 30000$  over a total of $14202, 23535, 117370$  for $T_f=0.062, 0.07, 0.092$  respectively. Notice that the results depend on the specific quality of the samples provided for training. It is possible to achieve better predictions by selecting better initial samples, more uniformly distributed, which are different from the random choice we make for simplicity. Still, the values reported consistently produce good results, independently from the initial sample composition. Training time – In Fig. REF (c) we report the effect of the training time on performance. The R2-score approaches the plateau in 10 minutes of training on a single CPU and reaches the final value after $10^3$  minutes ($\\sim 16$  hours). Overall, we find that in iterative training, where we retrain the model several times, a good balance between performance and speed can be reached by training for $\\sim 10$  minutes. Instead, in a more classic single training approach we suggest to train for $>10^3$  minutes. Performing the training in parallel can further reduce the training time. Parameters to identify double wells The ML model has to process a large number of unfiltered minima obtained during the exploration simulation. While we can anticipate that only a small fraction of inherent structure pairs will be a TLS, we also know that the minimum energy path connecting any pair of IS will not form a DW and as it is likely to contain intermediate minima. To exclude those non-DW pairs, we train a classifier using as input the same quantities that we measure to predict the QS, as described in the main text. In Fig. REF  we report the results of a hyperparameter scan at $T_f=0.062$ . These results are similar to those obtained for the QS prediction suggesting that at $T_f=0.062$  we can use (a) $M=3$ , (b) $10^2$ s of training and (c) $\\sim 10^4$  samples in order to achieve $>95\\%$  accuracy, measured over a validation set. We conclude that we can use the same hyperparameters for the DW classifier and the QS predictor. Removing irrelevant features After finding the optimal parameters to train the ML model, we can extract the specific effect and overall importance of each input parameter, as well as the score drop when each of them is removed. Since each additional feature corresponds to additional computational time and memory, we investigate which features are crucial, and which ones can be excluded to make the ML pipeline faster and more efficient without affecting performance. Figure: Importance of different features on the quantum splitting predictor. (a) Performance drop when each specific feature is removed, normalized to the first most important. Different colors correspond to different glass preparation temperatures. (b) Shapley values calculated at T f =0.062T_f=0.062. In general, at any temperature the most important feature is the energy difference (ΔE\\Delta E) of the two IS, which is followed by the value of the displacement of the MM particles. The Shapley values on the right also report the effect of the specific features: the splitting ΔE\\Delta E has to be small (green) in order to predict low QS (i.e. negative SHAP), while instead the displacements have to be large (red) in order to point towards low QS.In Fig. REF (a) we report the feature importance (in log scale) after a single iteration of training containing all the data from Ref. . Different colors refer to glasses prepared at different temperatures. Independently from the temperature, the most important features are the energy difference $\\Delta E$  between the two IS, followed by the total displacement of particles $\\sum _i \\Delta \\vec{r}_i$ . The third most important information is the positions of the particles that displaced the most $\\sum _i | \\vec{r}_0 -\\vec{r}_i|$ . After them, we find that the arrangement of the first shell of neighbors represented by the $q$  parameters and the particle sizes $\\sigma _i$  are insignificant (and so is their variation between the two IS of the pair $\\Delta \\cdot $  reported in Fig. REF ), since their importance is more than two orders of magnitude smaller than the one of the energy and displacements. Since the $q$  parameters are also the slowest to compute, we decided to not include them in the final pipeline reported in the main manuscript. Next, to quantify the role of those inputs, we calculate their Shapley values  shown in Fig. REF (b) for $T_f=0.062$ . The color codes for the value taken by the specific feature (red when the value is high, green when low) and reports its scaled impact on the model output. The x axis reports the Shapley value, where SHAP$>0$  implies that the specific feature is pushing towards predicting a high QS, while SHAP$<0$  indicates that the feature promotes low QS values. While no input feature alone can predict a TLS (because $|$ SHAP$|$  is small) there are some visible trends: (i) the energy asymmetry is the most important feature and should be small in order to predict a low QS, (ii) the particle displacements have to be larger than a threshold in order to predict low QS. According to these results we rationalize the ML prediction. The energy difference between two IS is the main predictor for the quantum splitting, which is never too large for DW, then the displacements are necessary to understand if the two IS are similar and what their stability is (see the temperature classification section in the SI). The displacement nucleus size complements the information contained in the displacements and gives local information about the participation ratio. Finally, all the other features do not provide any improvement. Since the bond order parameters are computationally expensive to calculate, we do not include them in the final ML approach we propose in the main text. The performance reported in the main text confirms that our choice is justified. Reducing the number of features We justify the choice of features discussed in sec. IVa and Fig. 2 by presenting the performance of our ML model as a function of feature number. In Tab. REF  we rank the features according to their Shapley values (see Fig. 6). We report (blue line) in Fig. REF  the accuracy of the DW classifier (a) and the QS predictor (b) as a function of number of features, following the Shapley ranking of Tab. REF . The DW classifier reaches its best accuracy with six features. In the initial study Ref. , , the matrix $\\mathbf {T}$  was the only information used to analyze the pair of IS. Here, the classifier already reaches $\\sim 90\\%$  accuracy using the transition matrix only (two features). We add (orange line) the performances when we exclude $\\Delta E, T_{ij}$  and $T_{ji}$ , which are the most important features overall. Surprisingly, the DW classifier still reaches its maximum accuracy with two features ($\\Delta \\vec{r}_2$  and $|\\vec{r}_0-\\vec{r}_2|$ ) as highlighted in the inset. The QS predictor shows instead very good predictions even using a single feature ($\\Delta E$  in the blue curve, or $\\Delta \\vec{r}_0$  in the orange). Table: Ranking the features used in the main manuscript by their Shapley values.Figure: Effect of the number of features. Accuracy of the DW classifier (a) and Pearson correlation score of the QS predictor (b), as a function of the number of features used. Along the blue curves the features are ranked by their importance using their Shapley values, so the first features to be used are the most important. For the orange curve we exclude ΔE\\Delta E, T ij T_{ij} and T ji T_{ji} to evaluate the performances of the ML model without the features with the best Shapley values. Iterative training The standard supervised learning approach consists in collecting a significant amount of data, then using them to train the ML model and finally use the model predictions. While overall very powerful, this scheme is not optimal when the goal of the model is to drive the exploration in a space much larger than the available data. This is the case of our main study where we employ the ML model to identify IS pairs with low QS. In order to make our approach more efficient and generalizable we developed the iterative training procedure. Figure: Performance of the iterative training procedure at T f =0.062T_f=0.062 for the data collected in the main manuscript. Starting from K 0 =5000K_0=5000 samples we perform iterations of K i =500K_i=500 predictions for 11 iterations. In (a) we report the cumulative number of double wells (DW), two-level systems (TLS) and non double-wells (Non-DW). In (b) we break down this result by color coding the confusion matrix of the ML model at each iteration of iterative training.We start the iterative training procedure from a sample of $K_0$  pairs for which we calculate the QS. We empirically find that $K_0\\sim 5000$  achieves a good balance between precision and time. This sample has to be balanced between DW and non-DW, but there is no need to include TLS in the initial sample. It is possible to start from a smaller $K_0$ , at the cost of a performance drop in the first few iterations of training. From the initial sample we perform a first training that takes 10 minutes for the DW classifier and 10 minutes for the QS predictor. We then use the model to predict the $K_i=500$  IS pairs with the lowest QS. We report the cumulative number of TLS, DW, and non-DW at each iteration in Fig. REF (a). From Fig. REF (b) we see that during the first iteration most of the $K_i=500$  best candidates are actually non-DW, so the model is performing poorly. This is the reason why we suggest to perform only $K_i=500$  NEBs for each iterations, otherwise the first iteration would lead to wasting time to perform calculations over non-DW. On the other hand, in the first iteration 72 TLS are already found by running 500 NEBs. This is to be compared with Ref.  in which 61 TLS were found by running $>14000$  NEBs. After the first retraining (during iteration 2) the performance of the ML model is already excellent and less than $30\\%$  of the 500 best pairs are non-DW, while 134 are newly found TLS. We also used our iterative training to reprocess the data of Ref. , and we report the results in Table 1 (main text). We show in Fig. REF  a detailed analysis of the procedure at $T_f=0.062$ . While the standard approach was able to identify 61 TLS running $>14000$  NEB+Schrödinger calculations, iterative training finds 156 TLS, by running only 2500 NEBs. This confirms that more than half of the total TLS were hidden among the pairs discarded by Ref. . In details, Fig. REF (a) shows the cumulative number of DW and TLS that we find, while Fig. REF (b) reports the confusion matrix for the different steps of iterative training. Figure: Reprocessing the data of Ref. at T f =0.062T_f=0.062 with iterative training. In (a) we report the cumulative number of double wells (DW), two-level systems (TLS) and non double-wells (Non-DW). While Ref. (red dashed line) identified 61 TLS running >14000>14000 NEBs, iterative training finds 156 TLS from 2500 NEBs. In (b) we report the confusion matrix of the 5 steps of iterative training that we run.Overall, these results demonstrate that the ML approach is not only faster than a manual filtering rule based on the transition matrix, but also much more effective. The pool of IS pairs excluded from the analysis in the original approach effectively contains a significant number of TLS. In conclusion, a ML driven exploration is not only more efficient than manual filtering, but also necessary to capture the correct statistics. Temperature and stability Our ML approach is able to predict the quantum splitting of a pair of IS from static information. It is known that TLS have different features depending on their preparation temperature, or glass stability . Here we address the following questions: (i) how difficult is it for a machine to distinguish data corresponding to different temperatures, (ii) what are the most important features for this task, and (iii) does our ML approach learn temperature-independent features? Temperature classification To understand how TLS and IS features evolve with temperature, we trained a multi-layer-perceptron (MLP) to classify the temperature corresponding to a specific pair. We find that it is possible to rapidly train a classifier reaching accuracy $>95\\%$ . Depending on the value of $M$ , the input layer is composed by $N_\\mathrm {features}$  neurons, where the features are explained in the main text. After the input layer, there are $n$  hidden fully connected layers, all of the same size $s$ . The activation function for each neuron-neuron connection is a ReLu function. The last layer is composed of 3 neurons that represent the probability that a given input belongs to one of the three classes: $T_f=0.062$  or $T_f=0.07$  or $T_f=0.092$ . The performance of the MLP are evaluated by measuring the accuracy, which is the percentage of the corrected predictions. In Fig. REF (a) we report the effect of increasing the size of the hidden layers, while in Fig. REF (b) we report the effect of a larger number of hidden layers. Figure: Temperature classifier accuracy as a function of number of hidden layers (a) and their size (b). We report results for M=5M=5. Results suggest that already with 2 hidden layers of 60 neurons it is possible to achieve an accuracy above 95%95\\%.Our results show that a MLP with 2 hidden layers of 60 neurons has a $95\\%$  accuracy after less than 1h of supervised training. The answer to question (i), is that one can identify the temperature at which a pair of IS was obtained. Static signatures of glass stability We answer question (ii) by measuring the Shapley (SHAP) values from the temperature classifier. In Fig. REF  (a) we report the SHAP value of the most important input features for the MLP prediction that considers only the $M=5$  particles that displaced the most. The smallest ($i=(M-1)=4$ ) and the largest ($i=0$ ) particle displacements emerge as the most important input, noted $\\Delta \\vec{r}_4$  and $\\Delta \\vec{r}_0$ , respectively. Their average effect on the model prediction is shown in Fig.REF (b,c). The particle that displaced the most ($i=0$ ) shows a large displacement at low preparation temperature, while the particle that displaced the least ($i=M-1$ ) exhibits a large displacement at high temperature. Our results suggest that higher temperatures are characterized by more collective rearrangements, leading to large $\\Delta \\vec{r}_4$ . In glasses prepared at low temperature instead, transitions are characterized by a particle displacing significantly more than the others. Figure: Microscopic differences originating from different temperature preparation/glass stability, quantified using the Shapley values for the temperature classifier. (a) Summary of the SHAP values for the 9 most important input features, measured for a subset of 1000 pairs processed by the TT-classifier. Predicted glass preparation temperature as a function of (b) Δr → 4 \\Delta \\vec{r}_4, and (c) Δr → 0 \\Delta \\vec{r}_0, with all other inputs taking their average value. Transferability and crossvalidation We address question (iii) by testing how our model performs when trained at $T_\\mathrm {train}$  and deployed to predict the quantum splitting of pairs at $T_\\mathrm {predict}\\ne T_\\mathrm {train}$ . In Fig. REF  each column corresponds to a training temperature: $T_\\mathrm {train}=0.092$  (left), $0.07$  (middle) and $0.062$  (right). Then, computed quantum splittings are compared with ML prediction made on samples obtained at $T_\\mathrm {predict}$ , decreasing from top to bottom. We see in Fig. REF  that the predictive power of a model trained at a different temperature is slightly lower compared to the model trained at the same temperature (Fig. REF  of main). Still, the transferability of the model is good, especially when the model is trained at $T_\\mathrm {train}>T_\\mathrm {predict}$ . This situation is the most useful, since it is easier to obtain data at higher $T$ . We conclude that if not enough data is available at low temperature, it is possible to rely on model transferability. This relies on the fact that TLS share some general temperature-independent features. In summary, we have seen that IS and TLS have different microscopic features when generated from different stabilities. It is then optimal to train the ML model at a fixed temperature only. If no easier way to measure $T$ /stability are available, it is possible to use ML to classify the stability of each IS by adding another block to the workflow, as reported in the main manuscript. Alternatively, at the cost of a finite accuracy drop it is even possible to transfer the model predictions at different temperatures and thus train where data collection is fast and easy and apply it where it is not. Figure: Transferability and crossvalidation of the machine learning model. Exact quantum splitting against ML prediction. The quantum splitting predictor is trained at T  train  T_\\mathrm {train} and deployed at T  prediction  ≠T  train  T_\\mathrm {prediction}\\ne T_\\mathrm {train}. From left to right: T  train  =0.092,0.07,0.062T_\\mathrm {train} = 0.092, 0.07, 0.062. From top to bottom, decreasing T  prediction  T_\\mathrm {prediction}. Performances are not on par with Fig. , but good in particular when T  train  >T  predict  T_\\mathrm {train}>T_\\mathrm {predict}, which is of practical interest.: Unbalanced parentheses: Introduction When a glass-forming liquid is cooled rapidly, its viscosity increases dramatically and it eventually transforms into an amorphous solid, called a glass, whose physical properties are profoundly different from those of ordered crystalline solids . At even lower temperature, around 1K, the specific heat of a disordered solid is much larger than that of its crystalline counterpart as it scales linearly rather than cubically with temperature. Similarly, the temperature evolution of the thermal conductivity in glasses is quadratic, rather than cubic , , , , , , , , , . A theoretical framework rationalizing such anomalous behavior was provided by Anderson, Halperin and Varma  and by Phillips , . They argued that the energy landscape of amorphous solids contains many nearly-degenerate minima, connected by localized motions of a few atoms, that can act as tunneling defects, called two-level systems (TLS). Understanding the microscopic origin of TLS and how to control their density and physical properties has gained significant interest, not only because they provide a large contribution to the specific heat and to the thermal conductivity, but also because their presence may impact the performance of certain quantum devices . Unfortunately, understanding their microscopic nature and their almost ubiquitous presence in low-temperature glasses remains a challenge , , , , . With the development of the swap Monte Carlo algorithm ,  it has become possible to create computer glasses at unprecedentedly low temperatures. Combined with landscape exploration algorithms , , , , , , , , ,  this provides a method to investigate the nature of TLS in materials prepared under conditions comparable to experimental studies , . These tools have enabled computational studies that have confirmed the experimental observation , , , ,  that as the kinetic stability of a glass increases, the density of tunneling defects is strongly depleted , . The direct detection of TLS revealed some of their microscopic features, namely that the participation ratio decreases in more stable glasses , and that TLS do not seem to be in a one-to-one correspondence with soft harmonic , ,  or localized ,  modes. The main issue that limits the applicability of the direct landscape exploration method is that it remains computationally very expensive, and it is thus hard to construct the large library of TLS needed to enable a robust statistical analysis of their physical properties. After accumulating a large number of inherent structures (IS), one must run an expensive algorithm to find the relaxation pathway connecting pairs of IS to then determine if this pair forms a proper TLS (namely, has an energy splitting within thermal energy at 1K). Due to the large number of IS pairs detected, it is impossible to characterize all of them. In previous work, some ad hoc filtering rules were introduced , ,  but the success rate of such filters is poor. After a significant exploration effort, it was possible to identify about 60 TLS starting from the identification of $\\sim 10^8$  IS. It is then obvious that most of the computational effort has been wasted in the study of pairs that form defects which do not tunnel at low temperatures. In this paper we show that it is possible to predict with enhanced accuracy whether a pair of inherent structures forms a TLS by using machine learning techniques. Recently, machine learning , , , , , , , , ,  has been shown to be extremely effective in using structural indicators to predict structural, dynamical or mechanical properties of glassy systems. In a similar vein, we use supervised learning to streamline the process of TLS identification. Our study has two goals: (i) develop a faster way to identify TLS compared to the standard approach outlined in Ref.  and described in Sec. , in order to collect a statistically significant number of TLS; (ii) understand what are the structural and dynamical features characterizing TLS, and if/how they change with the preparation temperature. To address (i) we show that our machine learning model can be trained in a few minutes using a small quantity of data, after which the model is able to identify candidate TLS with high speed and accuracy. To address (ii) we show which static features are the most important for the model prediction and we show that states which are dynamically distant can nevertheless be TLS. We conclude by explaining how the ML model distinguishes TLS from non-TLS and how it is able to identify glasses prepared at different temperatures. Standard approach to TLS identification The standard procedure , ,  to identify TLS is sketched in Fig. REF . It consists of the following first steps which aim at identifying potential candidates for TLS:  Equilibrate the system at the preparation temperature $T_f$ . Glasses with lower $T_f$  have a larger glass stability. Run molecular dynamics to sample configurations along a dynamical trajectory at the exploration temperature $T<T_f$ . Perform energy minimization from the sampled configurations to produce a time series of energy minima, or inherent structures (IS). Analyze the transition rates between pairs of IS, and select the pairs of IS that are explored consecutively. Step 4 was necessary because it is computationally impossible to analyze all pairs of IS, as the number of pairs scales quadratically with the number of minima. The filter defined in step 4 was physically motivated by the fact that TLS tend to originate from IS that are not too distinct in order to have a reasonable tunneling probability. As such it is likely that those pairs of IS get explored one after the other during the exploration dynamics in step 2. Overall, given $N_{IS}$  inherent structures, this procedure selects for $\\mathcal {O}(N_{IS})$  pairs to be analyzed. However, many pairs of IS can be close but not sampled consecutively during the dynamics, owing to the complex structure of the potential energy landscape. Instead, our machine learning approach allows to consider all pairs of IS. As shown below, our approach is able to detect TLS which are otherwise excluded by the above step 4. Once potential candidates are selected, the procedure continues as follows:  For each selected pair of IS, look for the minimum energy path and the classical barrier between them by running a minimum energy path finding algorithm, such as nudge elastic band (NEB) , , . This provides the value of the potential along the minimum energy path between the pair $V(\\xi )$ , where $0\\le \\xi \\le 1$  is the reaction coordinate. Select pairs whose energy profile $V(\\xi )$  has the form of a double well (DW), i.e., exclude paths with multiple wells. Solve the one-dimensional Schrödinger equation: $-\\frac{\\hbar ^2}{2md^2\\epsilon }\\partial ^2_{\\xi } \\Psi (\\xi )+V(\\xi )\\Psi (\\xi )=\\mathcal {E}\\Psi (\\xi ),$  where $\\xi $  is a normalized distance along the reaction path $\\xi =x/d$  and energy is normalized by a Lennard-Jones energy scale $\\epsilon $ , the effective mass $m$  and the distance $d$  are calculated as in Ref. . We obtain the quantum splitting (QS) $E_{qs}=\\mathcal {E}_2-\\mathcal {E}_1$  from the first two energy levels $\\mathcal {E}_1$  and $\\mathcal {E}_2$ . The quantum splitting is the most relevant parameter because when $E_{qs} \\sim T$  the system can go from one state to the other via quantum tunneling , creating what is called a two-level system (TLS). In particular since we choose to report the data in units that correspond to Argon , a double well excitation will be an active TLS at $T=1$ K when $E_{qs} < 0.0015 \\epsilon $ , where $\\epsilon $  sets the energy scale of the pair interactions in the simulated model. Overall, since at low temperature the landscape exploration dynamics is slow, one would like to spend most of the computational time by doing steps 2-3 to construct a large library of pairs of IS. A first problem is that when the library of IS grows larger it takes a lot of time to perform steps 5-7. Moreover, the main bottleneck lies in the fact that most of the pairs that go through the full procedure are not found to be TLS in the end, and so this large computational time is effectively wasted. Machine learning approach to TLS identification As in any machine learning (ML) approach, we distinguish two phases: training and deployment. Our supervised training approach, detailed in the next section, takes just a few hours of training on a single CPU. It requires an initial dataset of $\\mathcal {O}(10^4)$  full NEB calculations, whose collection is the most time consuming part of the training phase. Once training is complete, the ML model can be deployed to identify new TLS. Its workflow is similar to the standard one, with some major improvements. It proceeds with the following steps:  - 3. The first 3 steps are similar to the standard procedure to obtain a collection of inherent structures from a dynamical exploration. Apply the ML model to all possible pairs of IS to predict which pairs form a DW potential. Apply the ML model to predict the quantum splitting (QS) for all predicted DW and filter out the configurations that are not predicted to be TLS by the ML model. - 8. Run NEB, select pairs that form DW potential and solve the one-dimensional Schrödinger equation for the TLS candidates only in order to obtain the exact value of the quantum splitting. It is possible to use steps 4-5 as a single shot or as an iterative training approach (see Sec. REF ). In the Supplementary Information (SI) we provide details on how steps 1-3 are performed: glass preparation, exploration of the potential energy landscape via molecular dynamics simulations and minimization procedure, as well as NEB computation, see Sec. . Noticeably, if the model is well-trained then the ML approach has two significant advantages over the standard approach. First, $\\mathcal {O}(N_{IS}^2)$  pairs of IS are scanned to identify TLS, compared to a much smaller number $\\mathcal {O}(N_{IS})$  in the standard procedure. Second, if a pair of IS passes step 5 and goes through the full procedure it is very likely to be a real TLS. As a consequence, by using the ML approach one can spend more time doing steps 2-3 to produce new IS, since fewer pairs pass step 5. At the same time, for any given number of IS, the ML approach can analyze all possible pairs and is therefore able to identify many more TLS, as we demonstrate below. Machine learning model In Refs. , , the authors analyze a library of 14202, 23535 and 117370 pairs of inherent structures for a continuously polydisperse system of soft repulsive particles, equilibrated at reduced temperatures $T = 0.062$ , 0.07, and 0.092, respectively. The standard approach (Sec. ) leads to the identification of 61, 291 and 1008 TLS for the three temperatures, respectively. Notice that this approach uses pairs of IS that are selected by the dynamical information contained in the transition matrix between pairs of IS . This was done to filter out all non double well potential. For all pairs in this small subset, the quantum splitting was then calculated. Instead, the ML approach starts by independently evaluating the relevant information contained in each IS and constructs all possible combinations, even for pairs that are not dynamically connected. Following the steps discussed in Sec. the model is then able to predict the quantum splitting of all the pairs, that were predicted to form a DW, very accurately. From a quantitative perspective, this means that the same trajectories now contain many more TLS candidates in the ML approach compared to the standard approach. Figure: Flowchart of the machine learning approach. The first block represents the construction of the dataset by comparing all the pairs of inherent structures, focusing on the MM particles that displace the most. Then, required features are extracted to construct the input vector XX. We then train a classifier to predict whether a pair of IS is a DW or not. The DW are finally processed using a multi-layer stacking strategy to predict the quantum splitting energy. Our pipeline analyses a given pair of IS in about ∼10 -4 \\sim 10^{-4}s.In this section we describe the flowchart of the model summarised in Fig. REF . We first discuss how we construct the dataset and extract the relevant features (Sec. REF ). We then explain the two main blocks consisting in a DW classifier followed by a quantum splitting predictor, both of which have similar model architecture and inputs. Finally we evaluate the performance of this model by showing its ability to accurately predict the quantum splitting. We conclude by introducing the iterative training technique that represents the optimal way to efficiently expand the library of TLS. Dataset and features construction The first step is the evaluation of a set of static quantities for all the available IS. This set consists of: energy, particle positions, averaged bond-orientational order parameters  determined via a Voronoi tessellation, from $q_2$  to $q_{12}$ , and finally particle radii. The cost of this operation scales as the number of available states $N_{IS}$ , but we use these quantities to calculate the features of $\\sim N_{IS}^2$  pairs. A detailed analysis (see Sec. in the SI) shows that the bond orientational parameters and the particle sizes are not very useful for the ML model. Since their calculation is slower than all the other features, we do not include them in the final version of the ML approach. To construct the input features for each pair of IS we combine the information of the two states evaluating the following:  Energy splitting $\\Delta E$ : energy difference between the two IS. Displacements $\\Delta \\vec{r}_i$ : displacement vector of particle $i$  between the two configurations. Total displacement $d$ : total distance between the two IS defined as $d^2=\\sum _i |\\Delta \\vec{r}_i|^2$ . Participation ratio $PR$ : defined as $PR=(d^2)^2/\\left( \\sum _i |\\Delta \\vec{r}_i|^4\\right)$ . Distance from the displacement center $|\\vec{r}_0-\\vec{r}_i|$ : we measure the average distance of particle $i$  from the center of displacement $\\vec{r}_0$ , identified as the average position of the particle that moves the most. This quantity identifies the typical size of the region of particles that rearrange. Transition matrix $T_{ij}$  (and $T_{ji}$ ): number of times that the exploration dynamics traverses IS$_j$  just after IS$_i$  (and vice versa). The crucial step of the feature construction is that we can reduce the number of features by considering only the $M$  particles whose displacement is the largest between pairs of IS. We make this assumption because we expect that the low temperature dynamics is characterized by localised rearrangements involving only a small fraction of the particles , , , , , . In Sec. of the SI, we confirm this assumption by showing that the ML model achieves optimal performances even when $M$  is very small. So, the choice of $M \\ll N$  makes the ML model computationally effective without any performance drop. Double well classifier A necessary condition in order for a pair of IS to be a TLS is that the transition between the pair forms a double well (DW) potential. A DW is defined when the minimum energy path between the two IS resembles a quartic potential, as sketched in Fig. REF (c). The final goal of the ML model is to predict the quantum splitting of the pair to identify pairs with low values of the QS. The first obstacle in the identification of TLS is that DW represent only a small subgroup of all IS pairs. For instance in Ref. , only $\\sim 0.5\\%$  of all the IS pairs are DW at the lowest temperature. It is then mandatory to filter out pairs that are not likely to be a DW. In the machine learning field there are usually many different models that can be trained to achieve similar performances, with complexity ranging from polynomial regression to deep neural networks. Here, we perform model ensembling and use ensembles both for DW classification and QS prediction. Model ensembling consists in averaging the output of different ML models to achieve better predictions compared to each of them separately. We do so using the publicly available AutoGluon library . In this approach, we train in a few minutes a single-stack ensemble that is able to classify DW with $>95\\%$  accuracy. In Sec. of the SI we justify this choice of ML model and provide details on performances and hyperparameters. Overall, since the DW classifier is accurate and rapid, we use it to filter out the pairs that do not require the attention of the QS predictor because they cannot be TLS anyway. Quantum splitting predictor We want to predict the quantum splitting of a pair of IS for which the features discussed in Sec. REF  have been computed. We need this prediction to be very precise, because we know that a pair can be considered a TLS when $E_{qs}<0.0015 \\epsilon $ , but $E_{qs}$  can vary significantly so errors may be large. In the SI (see Sec. ) we show that models such as deep neural networks and regression are not stable or powerful enough to achieve satisfying results. We thus perform model ensembling by using the AutoGluon library . This achieves superior performances while also allowing us to compare and rank single models. We perform two types of model ensembling: (i) stacking: different layers of models are applied in series creating a stack (schematized in Fig. REF ), and (ii) bagging: we divide the data in subsets that we use to train multiple instances of the same models and we later combine all their predictions. As shown in the SI, the best results are obtained while using ensembles of gradient boosting methods (in particular CatBoost ), which have proven to be the optimal choice in similar semi-empirical quantum-mechanical calculations . Overall, the ensemble structure of our final model relieves us from hyperparameter optimization and makes the results more stable. Figure: (a)-(c) Quantum splitting and (d)-(f) energy barrier predicted by the ML model compared to the exact value, reported using a double logarithmic scale. We report predictions for IS pairs that the ML model has not seen during the training. (a) and (d) correspond to T f =0.062T_f=0.062, with training for 7000 samples and information of the M=3M=3 particles with largest displacements. (b) and (e) correspond to T f =0.07T_f=0.07, 10000 samples and M=3M=3. (c) and (f) correspond to T f =0.092T_f=0.092, 30000 samples and M=3M=3. All models have been trained for ∼10\\sim 10 hours of single CPU time.In order to train the model we first collect a set of $E_{qs}$  examples. The size of this training set is discussed in the SI (see Fig. REF ) where we find that the minimum number is around $10^4$ . We can use some of the data already collected in previous work in Ref.  for the training. Moreover, since we are interested in estimating with more precision the lowest values of $E_{qs}$  we train the model to minimize the following loss function $\\mathcal {L} = \\frac{ \\sum _{i=1}^n w_i \\left( E_{qs,\\mathrm {true}} -E_{qs,\\mathrm {predicted}} \\right)^2 }{n \\sum _{i=1}^n w_i },$  which is a weighted mean-squared error. The weights correspond to $w_i = 1/E_{qs,\\mathrm {true}}$  in order to give more importance to low $E_{qs}$  values. We thus train our model to provide a very accurate prediction of the value $E_{qs}$  for any given pair. Once the model is trained it takes only $\\sim 10^{-4}$ s to predict the QS of a new pair (compared to 1 minute to run the standard procedure). If we predict a value $E_{qs}<0.0015\\epsilon $ , then we have identified a TLS much faster. To showcase the performance of the ML model we report in Fig. REF (a)-(c) the exact quantum splitting calculated from the NEB procedure, compared with the value predicted by the model. We have trained three independent models to work at the three different temperatures. As explained before (Sec. REF ), the model needs the information about only the $M \\ll N$  particles that are displaced the most to achieve the excellent precision demonstrated in Fig. REF . In the Fig. REF  (SI) we find that the optimal value is $M=3$ , confirming that the participation ratio in TLS is quite low, because only three particles are needed for the model to identify TLS. Furthermore, the models have been trained using the smallest number of samples, randomly selected from all the IS pairs available, that allows the model to reach its top performance. We have also performed an analysis of the optimal training time. Details on these points are provided in Sec. of the SI. The performances presented in Fig. REF  are achieved by training the model for $\\sim 10$  hours of single CPU time, but we also show in the SI that it is possible to already achieve $>90\\%$  of this performance by training the ensemble for only 10 minutes. The ML approach that we have just introduced is also easily generalizable to target any state-to-state transition, like excitations and higher energy effects. Here we modified the quantum splitting predictor to instead predict the classical energy barrier between two IS states. If the minimal energy path between two IS forms a DW, we define the classical energy barrier as the maximum value of the energy along this path. In Fig. REF (d)-(f) we report the value of the energy barrier predicted by the ML model (y-axis) compared to the exact value calculated from the NEB procedure (x-axis). The hyperparameters and the features are the ones used for the quantum splitting predictor. Such a high performance demonstrates that our ML approach can predict other types of transitions between states. Iterative training procedure We finally introduce an approach to optimally employ our ML model to process new data: the iterative training procedure. In previous sections and in Fig. REF  we trained the model once using a subset of the already available data. This is a natural way to proceed when the goal is to process new data that are very similar to the training set, and the training set is itself large enough. However, since the goal of the proposed ML model is to ultimately drive the landscape exploration and collect new samples, the single-training approach may encounter two types of problems. First, at the beginning there may be not enough data and second the findings of the model do not provide any additional feedback. To solve both problems we introduce the iterative training procedure. The idea of iterative training is to use the predictive power of ML to create and expand its own training set, consequently enhancing its performance by iteratively retraining over the new data. Details on the method and parameters are discussed in Sec. of the SI. In practice, we start from a training set of $K_0 \\sim 10^3-10^4$  randomly selected pairs to have an initial idea of the relation between input and output. We then use the ML approach outlined in Fig. REF  to predict the $K_i=500$  pairs with the lowest QS. For these TLS candidates, we perform the full procedure to calculate the true QS and determine whether the pair is a DW or a TLS. In Fig. REF  (SI), we report the result of this procedure when we process a new set of trajectories from the same polydisperse soft sphere potential as in Ref. . In general the first few iterations of iterative training have a poor performance. In fact we find that $>70\\%$  of the first $K_i$  pairs are actually non-DW. After collecting additional $K_i$  measurements, we retrain the model. We report in Tab. REF  the average time for each step of the ML procedure. The retraining can be done in $\\sim 10$  min, after which the model is able to predict the next $K_i$  pairs with lowest QS. Overall, to process $N_\\mathrm {IS pairs}$  we estimate that the computational time of the iterative approach is $t_{i}=\\left[ K_0\\cdot 10^{2} + N_{iter}\\left(K_i\\cdot 10^{2} + 10^{3} + N_\\mathrm {IS pairs}\\cdot 10^{-5} \\right) \\right]s$ . If $N_\\mathrm {IS pairs}>10^{9}$  it is possible to significantly reduce $t_i$  by permanently discarding the worst pairs, but this is not needed here. We iterate this procedure $N_{iter}$  times, until the last batch of $K_i$  candidates contains less than $1\\%$  of the total number of TLS. We believe that continuing this iterative procedure would lead to the identification of even more TLS/DW, but this is out of the scope of this paper. Table: Computational time needed to perform our ML approach, on a standard laptop. Results and discussion We now use ML in order to speed up the TLS search. This highly efficient method allows us to collect a library of TLS of unprecedented size, generated from numerical simulations with the same interaction potential as in Ref. . First of all, we reprocess the data produced to obtain the results presented in Ref.  with our new ML method, obtaining new information about the connection between TLS and dynamics. Next, we perform ML-guided exploration to collect as many TLS as possible. This sizable library of TLS allows us to perform for the first time a detailed statistical analysis of TLS and compare their distribution to the distribution of double wells. We perform this analysis for glasses of three different stabilities. Finally, we discuss the microscopic features of TLS not only by looking at their statistics, but also by analyzing what the ML model has learned, and how it expresses its prediction. Capturing elusive TLS with machine learning Prior to this paper, it was not possible to evaluate all the IS pairs collected in Ref. . For this reason the authors discarded a priori all pairs where the number of forward and backward jumps between IS is such that $\\min \\left(T_{ij},T_{ji}\\right)<4$ , based on the assumption that high transition rates during the dynamic landscape exploration is a good indicator that two IS form a DW. This reduced the number of pairs to 14202, 21109 and 117339 for glasses prepared at $T_f=0.062$ , 0.07 and $0.092$  respectively. In order to have comparable data at the three temperatures, for $T_f=0.092$  we only consider a subset of glasses corresponding to 30920 IS pairs. The results of the TLS search are summarized in the red columns of Tab. REF . Overall the standard procedure reaches a rate of TLS found over calculations performed of $4 \\cdot 10^{-3}$ , $13\\cdot 10^{-3}$ , and $8\\cdot 10^{-3}$  for the three temperatures, respectively. We compare this with our iterative training procedure applied on the same data, whose results are reported in the green columns of Tab. REF . We immediately notice two major improvements: the overall number of TLS that we find from the same data set is more than twice larger and the ratio of TLS per calculation is more than 15 times larger, corresponding to $62\\cdot 10^{-3}$ , $211\\cdot 10^{-3}$ , and $194\\cdot 10^{-3}$  for the same three temperatures. We conclude that the iterative ML approach is much more efficient than the standard procedure, and also that TLS do not necessarily have a large dynamical transition rate, since the dynamical-filtering approach misses more than half of them. Table: Analysis of data collected in Ref. . We compare the standard procedure with our ML approach using iterative training. We report results for different glass stabilities, decreasing from top to bottom, using Argon units (Ar)  and NiP metallic glass parameters (NiP) . The standard procedure finds less than half of the TLS and is computationally much more expensive. Differences between DW and TLS With our ML-driven exploration of the energy landscape we can restrict the numerical effort to DW and favorable TLS candidates, while processing a larger number and/or longer exploration trajectories. This allows us to consider a larger set of independent glasses of the same type as those treated in Ref. , which is particularly relevant for ultrastable glasses generated at the lowest temperature $T_f=0.062$ . While in Ref.  the collection of 61 TLS required $>14000$  NEB calculations, we are able to identify 864 TLS running 11 iterations of iterative training using only a total of 5500 NEB calculations in addition to the $\\sim 6000$  used for pretraining. In the next section we analyze these results to discuss the nature of TLS. Figure: Results of ML driven exploration, leading to a library of TLS of unprecedented size. (a) We compare the model predictions to the calculated values at the end of our iterative training, at T f =0.062T_f=0.062. We color coded in the background the confusion matrix. The black dashed horizontal lines report the percentage of TLS that are predicted below that value of quantum splitting, showing that more than 95%95\\% of the TLS are within twice the TLS threshold of 0.00150.0015. (b) Cumulative distribution of energy splitting n(E qs )n(E_{qs}) divided by E qs E_{qs}. (c) Histograms of the number of TLS and DW per glass, at the three preparation temperatures. In total we have considered 237, 30, 5 glasses equilibrated at T f =0.062T_f=0.062, 0.07, and 0.092 respectively.The database of glasses that we analyze with iterative training contains 5 times more minima than in Ref. , but we are able to find 15 times more TLS running around half of the NEB calculations. Furthermore, in Fig. REF (a) we have color-coded the confusion matrix and we highlight that we can capture almost all of the TLS if we consider only the pairs that are predicted to be within twice the quantum splitting threshold of TLS. In Fig. REF (b) we report the saturation plateau of the cumulative density of TLS quantum splitting $n(E_{qs})$ , which scales as $n(E_{qs}) \\sim n_0 E_{qs}$  at low $E_{qs}$ . The ML approach allows us to collect significantly better statistics compared to Ref. , confirming that the TLS density $n_0$  decreases by several orders of magnitude from hyperquenched to ultrastable glasses. Lastly, in Fig. REF (c) we report the histograms of the number of TLS and DW per glass at the three temperatures. We see that when the glasses are ultrastable ($T_f=0.062$ ) most of the glasses have very few TLS. Conversely, poorly annealed ($T_f=0.092$ ) glasses show a very unbalanced distribution, with few glasses that contains most of the DW and TLS. Interpretation of the ML model It is possible to use the ML model to gain information about the distinctive structure of TLS. First, the present and previous works , , , , , ,  find that the density of TLS decreases upon increasing glass stability, which in our simulations is controlled by the preparation temperature. Thus, one may also expect temperature-dependent TLS features. In the SI we show that when the ML model is trained at $T_\\mathrm {train}$  and deployed at $T_\\mathrm {prediction} \\ne T_\\mathrm {train}$  there is only a minor performance drop and the model is able to perform reasonably well. This implies that the model captures distinctive signatures of TLS that do not depend strongly on the preparation temperature. Yet, we also show in the SI that it is very easy to train another ML model to predict the temperature itself and eventually add it to the pipeline. Overall, the ML model is not only able to capture the different microscopic features of TLS, but it can also suggest what is the specific influence of each feature. To interpret this information we calculate their Shapley values  and we report them in Fig. REF . The features are ranked from the most important (top) to the less important (bottom) reporting the impact that they have on the model output (SHAP value), so that a positive SHAP value predicts on average a high value of the QS. We see that the most important feature is the classical energy splitting $\\Delta E$  and that a large splitting (red) corresponds to a large QS. The second most important feature is the largest single particle displacement $\\Delta \\vec{r}_0$ , which has to be larger than a threshold corresponding to $0.3\\sigma $  in order to predict a low QS. The total displacement $d$  is the third most important and shows a similar effect. All the remaining features have a less clear and much smaller effect on the model prediction and they only collaborate collectively to the final QS prediction. In the SI we show that it is possible to obtain very good performance even when removing some of the features with the largest Shapley values, which means that the ML interpretation is not unique. Figure: Importance of the different features for the ML model for the prediction of the quantum splitting, evaluated from the Shapley values  at T f =0.062T_f=0.062. The features are ranked from the most important (top) to the least important, where the impact that they have on the model output corresponds to their SHAP value. Each point corresponds to a single IS pair. The color coding shows the impact from high (red) to low (green) values of each specific input.According to this Shapley analysis we explain the ML prediction in the following way: the energy difference $\\Delta E$  between two IS is the main predictor for the quantum splitting, and it has to be small for TLS. Then, the largest particle displacement $\\Delta \\vec{r}_0$  is necessary to understand if the two IS are similar and what is their stability (we show in the SI that $\\Delta \\vec{r}_0$  is the most important feature to identify the glass stability). Then the total displacement $d$  complements this information and gives local information about the displacements of the other particles. Lastly, all the other inputs provide fine tuning to refine the final prediction and are discussed in more detail in the SI. Interestingly, in the SI we also show that even without the two most important features, the ML approach can still reasonably reveal TLS candidates. Microscopic features of TLS We have shown that by following a ML-driven approach it is possible to collect a significant library of TLS for any preparation temperature. However it may be useful to discuss alternative strategies to rapidly identify TLS. In general, since TLS are extremely rare objects , , , ,  a filtering rule is necessary in order to reduce the number of possible candidates. In particular, Ref. , ,  proposed to use the transition matrix to exclude pairs that do not get explored consecutively. This is based on the assumption that DW (and consequently TLS) correspond to IS pairs that are close to each other and therefore during an exploration run should be detected successively, hence have a non zero transition rates. Instead, we prove in Fig. REF  that a filter based on dynamical information only is a poor predictor. In Fig. REF (a) we report the distribution of dynamical transitions between two inherent structures $i$  and $j$  for TLS, DW and all the pairs, measured at $T_f=0.062$ . While the slowly decaying tail of TLS and DW suggests that they often exhibit a large transition rate, actually most of the TLS and DW are characterized by no transition at all between them. Our interpretation is that even though the transition towards the other side of the double well is favourable, the landscape has such a large dimensionality ($3N$ ) that even very favorable transitions may never take place in a finite exploration time. This issue can become more severe when the trajectories are shorter, for example if the exploration is performed in parallel. We confirmed this observation by the results that are already reported in Tab. REF , where we have used our iterative training approach to re-analyze the data of Ref. , including pairs with no transition, thus finding many more TLS. We conclude that even though the transition rates are the most important single characteristics for DW prediction (see Fig. REF ), a filter based solely on them is still missing a lot of interesting pairs and therefore is not the most efficient. In Fig. REF (b) we focus on the distribution of classical splitting $\\Delta E$ . When $\\Delta E$  is large we rarely see DW and TLS (red region). On the other hand, there are many pairs that show a very small $\\Delta E$  value (yellow region), but they are not more likely to be TLS. Ultimately we find a `sweet spot' (green region), where TLS are more frequent. The ML model also captures this feature, as we can see from the SHAP parameter of $\\Delta E$  in Fig. REF . The next most important feature according to the ML model is the largest particle displacement $\\Delta \\vec{r}_0$ , reported in Fig. REF (c). When it is larger than $\\sim 0.8\\sigma $  we rarely find TLS and DW, but we do not find them also when $\\Delta \\vec{r}_0<0.3\\sigma $ . The SHAP parameter in Fig. REF  confirms that the ML model has discovered this feature. Finally in Fig. REF (d) we report the total displacement $d$ . If $d>0.9\\sigma $  the pair is so different that it is not likely to be a TLS or DW, while this probability increases for smaller $d$ . Overall, if it is necessary to identify TLS with a `quick and dirty' method, we propose to use a transition matrix to filter DW from non-DW, and then select a sweet spot for the energy splitting and the displacement for selecting optimal TLS candidates. Figure: Microscopic features of TLS and DW for ultrastable glasses at T f =0.062T_f=0.062. We report the probability distribution functions of: (a) number of transitions between the two inherent structures T ij T_{ij} and T ji T_{ji}, (b) classical energy splitting ΔE\\Delta E, (c) largest particle displacement Δr → 0 \\Delta \\vec{r}_0 and (d) total displacement dd. We color coded in red the regions of parameters where we do not expect to find TLS, which instead concentrate in the green regions. These green regions could serve as an alternative to rapidly identify TLS. Conclusion In this paper we have introduced a machine-learning approach to explore complex energy landscapes, with the goal of efficiently locating double wells (DW) and two-level systems (TLS). We demonstrate that it is possible to use ML to rapidly estimate the quantum splitting of a pair of inherent structures (IS) and accurately predict if a DW is a TLS or not. We also show that our ML approach can be used to predict very accurately the energy barrier between pairs of IS. Overall, this approach allows us to collect a TLS library of unprecedented size that would be impossible to obtain without the support of ML. The ML model uses as input information calculated from a pair of inherent structures. After just a few minutes of supervised training it is able to infer with high accuracy the quantum splitting of any new pair of inherent structures. We establish that the ML model that we develop using the Autogluon library is fast and precise. Its efficiency allows us to introduce an iterative training procedure, where we perform a small batch of prediction and then retrain the model. After performing statistical analysis over the unprecedented number of TLS collected with our method, we have discovered that many DW and TLS are not consecutively explored during the dynamics. We then reanalyzed the data of Ref.  finding that more than half of the TLS had been missed, because the analysis of Ref.  was based on this dynamic assumption. Our ML approach not only finds more than twice the number of TLS from the same data, but it also requires significantly fewer calculations. Overall we conclude that ML significantly improves the existing approaches. It also shows that if it is not possible to use the ML procedure that we outline, an effective `quick and dirty' way to predict TLS should be: a) use $T_{ij},T_{ji}$  for predicting DW; b) for predicted DW use energy splitting between two IS for predicting TLS. We also discuss the microscopic nature of DW and TLS. We perform a Shapley analysis to dissect the ML model and understand what it learns, and we compare this with the extended statistics of TLS that we are able to collect. We find that the quantum splitting is mostly related to the classical energy splitting and the displacements of the particles. Overall, the Shapley analysis suggests that TLS are characterized by one particle that displaces between $0.3$  and $0.9$  of its size, while the total displacement and the energy difference between the two states remains small. The local structure around the particle is not as important, nor is the number of times we actually see this transition during the exploration dynamics. Lastly we investigate the effect that glass stability (equivalent to the preparation temperature in our simulations) has on double wells and TLS. The ML model learns that at higher temperatures all the pairs are characterized by more collective rearrangements, but TLS are similar for any preparation temperature. Ultimately, since our ML approach is extremely efficient in exploring the energy landscape and is easy to generalize to target any type of state-to-state transition (as we show for the energy barriers), we hope that our method will be used in the future to analyze not only TLS, but also many other examples of phenomena related to specific transitions between states in complex classical and quantum settings. We thank E. Flenner, G. Folena, M. Ozawa, J. Sethna, S. Elliott, G. Ruocco and W. Schirmacher for useful discussions. This work was granted access to the HPC resources of MesoPSL financed by the Region Ile de France and the project Equip@Meso (reference ANR-10-EQPX-29-01) of the programme Investissements d’Avenir supervised by the Agence Nationale pour la Recherche. This project received funding from the European Research Council (ERC) under the European Union’s Horizon 2020 research and innovation program, Grant No. 723955 – GlassUniversality (FZ), and from the Simons Foundation (#454933, LB, #454955, FZ, #454951 DR) and by a Visiting Professorship from the Leverhulme Trust (VP1-2019-029, LB). CS acknowledges support from the Herchel Smith Fund and Sidney Sussex College, University of Cambridge. Model We study a three-dimensional polydisperse mixture of particles interacting via the potential $v(r_{ij}) = \\epsilon (\\sigma _{ij}/r_{ij})^{12}+\\epsilon F(r_{ij}/\\sigma _{ij}),$  only if $r_{ij}=1.25\\sigma _{ij}$ . We use non-additive interactions $\\sigma _{ij} = 0.5(\\sigma _i + \\sigma _j ) (1 - 0.2|\\sigma _i - \\sigma _j |)$ . The function $F$  is a fourth-order polynomial ensuring continuity of $v(r_{ij})$ , its first and second derivatives, at the interaction cutoff. The particle diameters $\\sigma _i$  are drawn from the normalized distribution $P(0.73 < \\sigma <1.62) \\propto 1/\\sigma ^3$ . This model glassformer is efficiently equilibrated with the particle-swap Monte Carlo algorithm, with a fluid robust against fractionation and crystallization down to low temperature . We employ the swap Monte Carlo algorithm implemented in the LAMMPS package, with the optimal parameters provided in . With such parameters supercooled liquid configurations can be generated down to $T =$  0.062, below the experimental glass transition $T_g=0.067$ . In this work, we use configurations prepared in equilibrium conditions at temperatures $T_f = 0.092, 0.07, 0.062$ . MD dynamics To explore the energy landscape of glasses generated at $T_f$ , we use classical Molecular Dynamics (MD) simulations. Glasses are first thermalized to $T_{MD}$  = 0.04 using a Berendsen thermostat. Such temperature is low enough to suppress diffusion, but sufficiently high for quick exploration of the energy landscape. We then run MD simulations in the NVE ensemble, using an integration time step of $dt =$  0.01 (LJ time units). Configurations along the MD trajectory are used as the starting point for energy minimization via a conjugate gradient algorithm, which brings them to their inherent structure (IS). MD configurations are minimized every 20, 10, 5 time steps for $T_f$  = 0.062, 0.07, 0.092 respectively. The values are a compromise between too frequent minimization which sample the same IS many times consecutively, and too infrequent which would miss intermediate IS. For each initial configuration we perform 100,100,200 MD runs with different initial velocities for $T_f$  = 0.062, 0.07, 0.092, respectively. Each run lasts 40000, 100000, 10000 time steps. We performed our calculations on 200, 50, 5 glasses for $T_f$  = 0.062, 0.07, 0.092. For $T_f=0.092$  glasses we used a subset of the original data obtained in . NEB To analyze the transition between two IS we compute the multi-dimensional minimum energy path separating them, along with its one-dimensional potential energy profile. This is done by the nudged elastic band (NEB) method ,  implemented in the LAMMPS package. We use 40 images to interpolate the minimal energy path, that are connected by springs of constant $\\kappa =1.0$  $\\epsilon \\sigma ^{-2}$ , and use the FIRE algorithm in the minimization procedure , . Comparison between ensembled and non-ensembled models In the main manuscript we discuss how to transform the problem of computing the quantum splitting of two inherent structures into a supervised learning regression problem. Still, many machine learning models can in principle answer this question. We opted for the AutoGluon library  which is based on model ensembling to find an optimal solution. Model ensembling consists in averaging the output of different ML models and create an ensemble-averaged model that outperforms each one of its components. In particular, the AutoGluon library performs model ensembling in the form of stacking (stacking different layers of models that are applied in series) and bagging (training multiple instances of the same model on different subsets of data and then combining their prediction). In Fig. REF (a) we report the most performant ensembling after 20 hours of training, at $T_f=0.062$ , $M=5$  and $N_\\mathrm {samples}=10^4$ . The R2-score reported in the figure shows that the most performant model is a bagged ensemble of CatBoost, a gradient boosting method , which then corresponds to the final WeightedEnsemble for this training instance. Notice that in the main paper, we always use the best ensembled model. In addition to CatBoost, very good performances are also usually achieved by LightGBM, another gradient boosting method , and ExtraTrees, which is an extremely randomized tree ensemble . Overall, gradient boosting methods typically achieve the highest R2-score. Thus, in the main text when we report results obtained with the `best models', we refer to ensembles of gradient boosting methods assembled with Autogluon. Figure: Comparison of different ML model ensembles to predict the quantum splitting at T f =0.062T_f=0.062 using M=5M=5 and N  samples  =7·10 4 N_\\mathrm {samples}=7\\cdot 10^4. (a) R2-score of the test set obtained for an ensemble of models. The full collection was trained for 20 hours of CPU time. In the main manuscript we use the WeightedEnsemble. (b) An optimal Neural Network (MLP with 10 hidden layers of size 1.1·N  features  1.1\\cdot N_\\mathrm {features}) does not produce good predictions (R2<0R2<0).We motivate our choice of complex ensemble model constructed with Autogluon by showing that simpler models do not perform at the same level. For the system at $T_f=0.062$  and in the optimal condition corresponding to $M=5$  and $N_\\mathrm {samples}=7000$  the simplest model to at least capture some correlation was a Multi-layer-perceptron (MLP) with 10 hidden layers of size $1.1\\cdot N_\\mathrm {features}$ , using the same input and output structure as in the main text. A first disadvantage of this approach is that several additional hyperparameters have to be selected, such as learning rate, number and size of hidden layers, activation function, etc. We varied these parameters and report in Fig. REF (b) results for the model with the best performance. This optimal MLP was trained until the iteration per epoch was consistently below $10^{-10}$ , corresponding to $\\sim 10$  hours. Still, the results in Fig. REF (b) demonstrate that the MLP performance is much worse than that of the ensemble models used in the main text. In addition, it is much slower to train. While in this case the predictions could be scaled by a constant to achieve significant improvement, the problem with such an approach is that it is not reliable, and this scaling constant is not known a priori adding an additional layer of complication. For this reason we prefer to use more complex machine-learning models. Parameters to predict the quantum splitting A significant advantage of the model ensemble approach is that most of the hyperparameters are automatically optimized by the ensembling. Still, some degrees of freedom have to be fixed. In particular we need to fix the number of particles to be considered ($M$ ), how many samples to use ($N_\\mathrm {samples}$ ) and for how long to train the ensemble. In the next sections we motivate the choices reported in the main text. Figure: Optimizing the Quantum splitting predictor. Effect of (a) the number of particles MM, (b) number of samples N  samples  N_\\mathrm {samples} and (c) training time on performance, while other parameters assume their optimal value. We report the R2-score after training on the data of Ref. . Overall we conclude that optimal performances can be achieved for M=3M=3, ∼10 4 \\sim 10^4 samples in just 10 minutes of training. $M$ : input particles – In Fig. REF (a) we report the effect of $M$ , the number of particles considered in the ML procedure. A large $M$  hinders the performance of the ML model because it has to process a larger input vector. The peak performance is achieved at $M=3$ , which is a relatively low number of particles to define a TLS. This confirms that the participation rate in TLS is low . $N_\\mathrm {samples}$ : number of samples – To evaluate the optimal value of $N_\\mathrm {samples}$  we report in Fig. REF (b) the variation of the R2-score upon using more samples for training. Notice that we keep the other parameters in the optimal condition for each temperature. The shape of the curves let us conclude that a good number of training samples is $7000, 10000, 30000$  over a total of $14202, 23535, 117370$  for $T_f=0.062, 0.07, 0.092$  respectively. Notice that the results depend on the specific quality of the samples provided for training. It is possible to achieve better predictions by selecting better initial samples, more uniformly distributed, which are different from the random choice we make for simplicity. Still, the values reported consistently produce good results, independently from the initial sample composition. Training time – In Fig. REF (c) we report the effect of the training time on performance. The R2-score approaches the plateau in 10 minutes of training on a single CPU and reaches the final value after $10^3$  minutes ($\\sim 16$  hours). Overall, we find that in iterative training, where we retrain the model several times, a good balance between performance and speed can be reached by training for $\\sim 10$  minutes. Instead, in a more classic single training approach we suggest to train for $>10^3$  minutes. Performing the training in parallel can further reduce the training time. Parameters to identify double wells The ML model has to process a large number of unfiltered minima obtained during the exploration simulation. While we can anticipate that only a small fraction of inherent structure pairs will be a TLS, we also know that the minimum energy path connecting any pair of IS will not form a DW and as it is likely to contain intermediate minima. To exclude those non-DW pairs, we train a classifier using as input the same quantities that we measure to predict the QS, as described in the main text. In Fig. REF  we report the results of a hyperparameter scan at $T_f=0.062$ . These results are similar to those obtained for the QS prediction suggesting that at $T_f=0.062$  we can use (a) $M=3$ , (b) $10^2$ s of training and (c) $\\sim 10^4$  samples in order to achieve $>95\\%$  accuracy, measured over a validation set. We conclude that we can use the same hyperparameters for the DW classifier and the QS predictor. Removing irrelevant features After finding the optimal parameters to train the ML model, we can extract the specific effect and overall importance of each input parameter, as well as the score drop when each of them is removed. Since each additional feature corresponds to additional computational time and memory, we investigate which features are crucial, and which ones can be excluded to make the ML pipeline faster and more efficient without affecting performance. Figure: Importance of different features on the quantum splitting predictor. (a) Performance drop when each specific feature is removed, normalized to the first most important. Different colors correspond to different glass preparation temperatures. (b) Shapley values calculated at T f =0.062T_f=0.062. In general, at any temperature the most important feature is the energy difference (ΔE\\Delta E) of the two IS, which is followed by the value of the displacement of the MM particles. The Shapley values on the right also report the effect of the specific features: the splitting ΔE\\Delta E has to be small (green) in order to predict low QS (i.e. negative SHAP), while instead the displacements have to be large (red) in order to point towards low QS.In Fig. REF (a) we report the feature importance (in log scale) after a single iteration of training containing all the data from Ref. . Different colors refer to glasses prepared at different temperatures. Independently from the temperature, the most important features are the energy difference $\\Delta E$  between the two IS, followed by the total displacement of particles $\\sum _i \\Delta \\vec{r}_i$ . The third most important information is the positions of the particles that displaced the most $\\sum _i | \\vec{r}_0 -\\vec{r}_i|$ . After them, we find that the arrangement of the first shell of neighbors represented by the $q$  parameters and the particle sizes $\\sigma _i$  are insignificant (and so is their variation between the two IS of the pair $\\Delta \\cdot $  reported in Fig. REF ), since their importance is more than two orders of magnitude smaller than the one of the energy and displacements. Since the $q$  parameters are also the slowest to compute, we decided to not include them in the final pipeline reported in the main manuscript. Next, to quantify the role of those inputs, we calculate their Shapley values  shown in Fig. REF (b) for $T_f=0.062$ . The color codes for the value taken by the specific feature (red when the value is high, green when low) and reports its scaled impact on the model output. The x axis reports the Shapley value, where SHAP$>0$  implies that the specific feature is pushing towards predicting a high QS, while SHAP$<0$  indicates that the feature promotes low QS values. While no input feature alone can predict a TLS (because $|$ SHAP$|$  is small) there are some visible trends: (i) the energy asymmetry is the most important feature and should be small in order to predict a low QS, (ii) the particle displacements have to be larger than a threshold in order to predict low QS. According to these results we rationalize the ML prediction. The energy difference between two IS is the main predictor for the quantum splitting, which is never too large for DW, then the displacements are necessary to understand if the two IS are similar and what their stability is (see the temperature classification section in the SI). The displacement nucleus size complements the information contained in the displacements and gives local information about the participation ratio. Finally, all the other features do not provide any improvement. Since the bond order parameters are computationally expensive to calculate, we do not include them in the final ML approach we propose in the main text. The performance reported in the main text confirms that our choice is justified. Reducing the number of features We justify the choice of features discussed in sec. IVa and Fig. 2 by presenting the performance of our ML model as a function of feature number. In Tab. REF  we rank the features according to their Shapley values (see Fig. 6). We report (blue line) in Fig. REF  the accuracy of the DW classifier (a) and the QS predictor (b) as a function of number of features, following the Shapley ranking of Tab. REF . The DW classifier reaches its best accuracy with six features. In the initial study Ref. , , the matrix $\\mathbf {T}$  was the only information used to analyze the pair of IS. Here, the classifier already reaches $\\sim 90\\%$  accuracy using the transition matrix only (two features). We add (orange line) the performances when we exclude $\\Delta E, T_{ij}$  and $T_{ji}$ , which are the most important features overall. Surprisingly, the DW classifier still reaches its maximum accuracy with two features ($\\Delta \\vec{r}_2$  and $|\\vec{r}_0-\\vec{r}_2|$ ) as highlighted in the inset. The QS predictor shows instead very good predictions even using a single feature ($\\Delta E$  in the blue curve, or $\\Delta \\vec{r}_0$  in the orange). Table: Ranking the features used in the main manuscript by their Shapley values.Figure: Effect of the number of features. Accuracy of the DW classifier (a) and Pearson correlation score of the QS predictor (b), as a function of the number of features used. Along the blue curves the features are ranked by their importance using their Shapley values, so the first features to be used are the most important. For the orange curve we exclude ΔE\\Delta E, T ij T_{ij} and T ji T_{ji} to evaluate the performances of the ML model without the features with the best Shapley values. Iterative training The standard supervised learning approach consists in collecting a significant amount of data, then using them to train the ML model and finally use the model predictions. While overall very powerful, this scheme is not optimal when the goal of the model is to drive the exploration in a space much larger than the available data. This is the case of our main study where we employ the ML model to identify IS pairs with low QS. In order to make our approach more efficient and generalizable we developed the iterative training procedure. Figure: Performance of the iterative training procedure at T f =0.062T_f=0.062 for the data collected in the main manuscript. Starting from K 0 =5000K_0=5000 samples we perform iterations of K i =500K_i=500 predictions for 11 iterations. In (a) we report the cumulative number of double wells (DW), two-level systems (TLS) and non double-wells (Non-DW). In (b) we break down this result by color coding the confusion matrix of the ML model at each iteration of iterative training.We start the iterative training procedure from a sample of $K_0$  pairs for which we calculate the QS. We empirically find that $K_0\\sim 5000$  achieves a good balance between precision and time. This sample has to be balanced between DW and non-DW, but there is no need to include TLS in the initial sample. It is possible to start from a smaller $K_0$ , at the cost of a performance drop in the first few iterations of training. From the initial sample we perform a first training that takes 10 minutes for the DW classifier and 10 minutes for the QS predictor. We then use the model to predict the $K_i=500$  IS pairs with the lowest QS. We report the cumulative number of TLS, DW, and non-DW at each iteration in Fig. REF (a). From Fig. REF (b) we see that during the first iteration most of the $K_i=500$  best candidates are actually non-DW, so the model is performing poorly. This is the reason why we suggest to perform only $K_i=500$  NEBs for each iterations, otherwise the first iteration would lead to wasting time to perform calculations over non-DW. On the other hand, in the first iteration 72 TLS are already found by running 500 NEBs. This is to be compared with Ref.  in which 61 TLS were found by running $>14000$  NEBs. After the first retraining (during iteration 2) the performance of the ML model is already excellent and less than $30\\%$  of the 500 best pairs are non-DW, while 134 are newly found TLS. We also used our iterative training to reprocess the data of Ref. , and we report the results in Table 1 (main text). We show in Fig. REF  a detailed analysis of the procedure at $T_f=0.062$ . While the standard approach was able to identify 61 TLS running $>14000$  NEB+Schrödinger calculations, iterative training finds 156 TLS, by running only 2500 NEBs. This confirms that more than half of the total TLS were hidden among the pairs discarded by Ref. . In details, Fig. REF (a) shows the cumulative number of DW and TLS that we find, while Fig. REF (b) reports the confusion matrix for the different steps of iterative training. Figure: Reprocessing the data of Ref. at T f =0.062T_f=0.062 with iterative training. In (a) we report the cumulative number of double wells (DW), two-level systems (TLS) and non double-wells (Non-DW). While Ref. (red dashed line) identified 61 TLS running >14000>14000 NEBs, iterative training finds 156 TLS from 2500 NEBs. In (b) we report the confusion matrix of the 5 steps of iterative training that we run.Overall, these results demonstrate that the ML approach is not only faster than a manual filtering rule based on the transition matrix, but also much more effective. The pool of IS pairs excluded from the analysis in the original approach effectively contains a significant number of TLS. In conclusion, a ML driven exploration is not only more efficient than manual filtering, but also necessary to capture the correct statistics. Temperature and stability Our ML approach is able to predict the quantum splitting of a pair of IS from static information. It is known that TLS have different features depending on their preparation temperature, or glass stability . Here we address the following questions: (i) how difficult is it for a machine to distinguish data corresponding to different temperatures, (ii) what are the most important features for this task, and (iii) does our ML approach learn temperature-independent features? Temperature classification To understand how TLS and IS features evolve with temperature, we trained a multi-layer-perceptron (MLP) to classify the temperature corresponding to a specific pair. We find that it is possible to rapidly train a classifier reaching accuracy $>95\\%$ . Depending on the value of $M$ , the input layer is composed by $N_\\mathrm {features}$  neurons, where the features are explained in the main text. After the input layer, there are $n$  hidden fully connected layers, all of the same size $s$ . The activation function for each neuron-neuron connection is a ReLu function. The last layer is composed of 3 neurons that represent the probability that a given input belongs to one of the three classes: $T_f=0.062$  or $T_f=0.07$  or $T_f=0.092$ . The performance of the MLP are evaluated by measuring the accuracy, which is the percentage of the corrected predictions. In Fig. REF (a) we report the effect of increasing the size of the hidden layers, while in Fig. REF (b) we report the effect of a larger number of hidden layers. Figure: Temperature classifier accuracy as a function of number of hidden layers (a) and their size (b). We report results for M=5M=5. Results suggest that already with 2 hidden layers of 60 neurons it is possible to achieve an accuracy above 95%95\\%.Our results show that a MLP with 2 hidden layers of 60 neurons has a $95\\%$  accuracy after less than 1h of supervised training. The answer to question (i), is that one can identify the temperature at which a pair of IS was obtained. Static signatures of glass stability We answer question (ii) by measuring the Shapley (SHAP) values from the temperature classifier. In Fig. REF  (a) we report the SHAP value of the most important input features for the MLP prediction that considers only the $M=5$  particles that displaced the most. The smallest ($i=(M-1)=4$ ) and the largest ($i=0$ ) particle displacements emerge as the most important input, noted $\\Delta \\vec{r}_4$  and $\\Delta \\vec{r}_0$ , respectively. Their average effect on the model prediction is shown in Fig.REF (b,c). The particle that displaced the most ($i=0$ ) shows a large displacement at low preparation temperature, while the particle that displaced the least ($i=M-1$ ) exhibits a large displacement at high temperature. Our results suggest that higher temperatures are characterized by more collective rearrangements, leading to large $\\Delta \\vec{r}_4$ . In glasses prepared at low temperature instead, transitions are characterized by a particle displacing significantly more than the others. Figure: Microscopic differences originating from different temperature preparation/glass stability, quantified using the Shapley values for the temperature classifier. (a) Summary of the SHAP values for the 9 most important input features, measured for a subset of 1000 pairs processed by the TT-classifier. Predicted glass preparation temperature as a function of (b) Δr → 4 \\Delta \\vec{r}_4, and (c) Δr → 0 \\Delta \\vec{r}_0, with all other inputs taking their average value. Transferability and crossvalidation We address question (iii) by testing how our model performs when trained at $T_\\mathrm {train}$  and deployed to predict the quantum splitting of pairs at $T_\\mathrm {predict}\\ne T_\\mathrm {train}$ . In Fig. REF  each column corresponds to a training temperature: $T_\\mathrm {train}=0.092$  (left), $0.07$  (middle) and $0.062$  (right). Then, computed quantum splittings are compared with ML prediction made on samples obtained at $T_\\mathrm {predict}$ , decreasing from top to bottom. We see in Fig. REF  that the predictive power of a model trained at a different temperature is slightly lower compared to the model trained at the same temperature (Fig. REF  of main). Still, the transferability of the model is good, especially when the model is trained at $T_\\mathrm {train}>T_\\mathrm {predict}$ . This situation is the most useful, since it is easier to obtain data at higher $T$ . We conclude that if not enough data is available at low temperature, it is possible to rely on model transferability. This relies on the fact that TLS share some general temperature-independent features. In summary, we have seen that IS and TLS have different microscopic features when generated from different stabilities. It is then optimal to train the ML model at a fixed temperature only. If no easier way to measure $T$ /stability are available, it is possible to use ML to classify the stability of each IS by adding another block to the workflow, as reported in the main manuscript. Alternatively, at the cost of a finite accuracy drop it is even possible to transfer the model predictions at different temperatures and thus train where data collection is fast and easy and apply it where it is not. Figure: Transferability and crossvalidation of the machine learning model. Exact quantum splitting against ML prediction. The quantum splitting predictor is trained at T  train  T_\\mathrm {train} and deployed at T  prediction  ≠T  train  T_\\mathrm {prediction}\\ne T_\\mathrm {train}. From left to right: T  train  =0.092,0.07,0.062T_\\mathrm {train} = 0.092, 0.07, 0.062. From top to bottom, decreasing T  prediction  T_\\mathrm {prediction}. Performances are not on par with Fig. , but good in particular when T  train  >T  predict  T_\\mathrm {train}>T_\\mathrm {predict}, which is of practical interest.\n",
      "11/09/2024 20:36:18 - DEBUG - \t 0 abbreviations detected and kept (0 omitted)\n",
      "11/09/2024 20:36:18 - INFO - \t Tokenize 1 inputs...\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 20.30 examples/s]\n",
      "11/09/2024 20:36:23 - INFO - \t ***** Running Inference on 1 texts *****\n",
      "Inference:   0%|          | 0/1 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Numpy is not available",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m processed_df \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_dataframe\u001b[49m\u001b[43m(\u001b[49m\u001b[43marxiv_df\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[7], line 272\u001b[0m, in \u001b[0;36mprocess_dataframe\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m    269\u001b[0m abstract, body \u001b[38;5;241m=\u001b[39m abstract[\u001b[38;5;241m0\u001b[39m], body[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    271\u001b[0m \u001b[38;5;66;03m# Apply coreference resolution\u001b[39;00m\n\u001b[0;32m--> 272\u001b[0m doc_abstract \u001b[38;5;241m=\u001b[39m \u001b[43mnlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mabstract\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    273\u001b[0m doc_body \u001b[38;5;241m=\u001b[39m nlp(body)\n\u001b[1;32m    274\u001b[0m clusters_abstract \u001b[38;5;241m=\u001b[39m doc_abstract\u001b[38;5;241m.\u001b[39m_\u001b[38;5;241m.\u001b[39mcoref_clusters\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/triplets/lib/python3.11/site-packages/spacy/language.py:1057\u001b[0m, in \u001b[0;36mLanguage.__call__\u001b[0;34m(self, text, disable, component_cfg)\u001b[0m\n\u001b[1;32m   1055\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(Errors\u001b[38;5;241m.\u001b[39mE109\u001b[38;5;241m.\u001b[39mformat(name\u001b[38;5;241m=\u001b[39mname)) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m   1056\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m-> 1057\u001b[0m     \u001b[43merror_handler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43me\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1058\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(doc, Doc):\n\u001b[1;32m   1059\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(Errors\u001b[38;5;241m.\u001b[39mE005\u001b[38;5;241m.\u001b[39mformat(name\u001b[38;5;241m=\u001b[39mname, returned_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mtype\u001b[39m(doc)))\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/triplets/lib/python3.11/site-packages/spacy/util.py:1722\u001b[0m, in \u001b[0;36mraise_error\u001b[0;34m(proc_name, proc, docs, e)\u001b[0m\n\u001b[1;32m   1721\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mraise_error\u001b[39m(proc_name, proc, docs, e):\n\u001b[0;32m-> 1722\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/triplets/lib/python3.11/site-packages/spacy/language.py:1052\u001b[0m, in \u001b[0;36mLanguage.__call__\u001b[0;34m(self, text, disable, component_cfg)\u001b[0m\n\u001b[1;32m   1050\u001b[0m     error_handler \u001b[38;5;241m=\u001b[39m proc\u001b[38;5;241m.\u001b[39mget_error_handler()\n\u001b[1;32m   1051\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1052\u001b[0m     doc \u001b[38;5;241m=\u001b[39m \u001b[43mproc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcomponent_cfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m   1053\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1054\u001b[0m     \u001b[38;5;66;03m# This typically happens if a component is not initialized\u001b[39;00m\n\u001b[1;32m   1055\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(Errors\u001b[38;5;241m.\u001b[39mE109\u001b[38;5;241m.\u001b[39mformat(name\u001b[38;5;241m=\u001b[39mname)) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/triplets/lib/python3.11/site-packages/fastcoref/spacy_component/spacy_component.py:119\u001b[0m, in \u001b[0;36mFastCorefResolver.__call__\u001b[0;34m(self, doc, resolve_text)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, doc: Doc, resolve_text\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Doc:\n\u001b[1;32m    113\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;124;03m    The function takes a doc object and returns a doc object\u001b[39;00m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;124;03m    :param doc: Doc\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;124;03m    :type doc: Doc\u001b[39;00m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;124;03m    :return: A Doc object with the resolved text and coreference clusters added as attributes.\u001b[39;00m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 119\u001b[0m     preds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcoref_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mdoc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    120\u001b[0m     clusters \u001b[38;5;241m=\u001b[39m preds[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget_clusters(as_strings\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m resolve_text:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/triplets/lib/python3.11/site-packages/fastcoref/modeling.py:265\u001b[0m, in \u001b[0;36mCorefModel.predict\u001b[0;34m(self, texts, is_split_into_words, max_tokens_in_batch, output_file)\u001b[0m\n\u001b[1;32m    262\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_dataset(texts, is_split_into_words)\n\u001b[1;32m    263\u001b[0m dataloader \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_batches(dataset, max_tokens_in_batch)\n\u001b[0;32m--> 265\u001b[0m preds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    267\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(output_file, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/triplets/lib/python3.11/site-packages/fastcoref/modeling.py:200\u001b[0m, in \u001b[0;36mCorefModel._inference\u001b[0;34m(self, dataloader)\u001b[0m\n\u001b[1;32m    198\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m tqdm(desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInference\u001b[39m\u001b[38;5;124m\"\u001b[39m, total\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(dataloader\u001b[38;5;241m.\u001b[39mdataset)) \u001b[38;5;28;01mas\u001b[39;00m progress_bar:\n\u001b[1;32m    199\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m dataloader:\n\u001b[0;32m--> 200\u001b[0m             results\u001b[38;5;241m.\u001b[39mextend(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_batch_inference\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    201\u001b[0m             progress_bar\u001b[38;5;241m.\u001b[39mupdate(n\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m]))\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/triplets/lib/python3.11/site-packages/fastcoref/modeling.py:167\u001b[0m, in \u001b[0;36mCorefModel._batch_inference\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m    165\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(batch, return_all_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m--> 167\u001b[0m outputs_np \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    169\u001b[0m span_starts, span_ends, mention_logits, coref_logits \u001b[38;5;241m=\u001b[39m outputs_np\n\u001b[1;32m    170\u001b[0m doc_indices, mention_to_antecedent \u001b[38;5;241m=\u001b[39m create_mention_to_antecedent(span_starts, span_ends, coref_logits)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/triplets/lib/python3.11/site-packages/fastcoref/modeling.py:167\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m    165\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(batch, return_all_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m--> 167\u001b[0m outputs_np \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(\u001b[43mtensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m tensor \u001b[38;5;129;01min\u001b[39;00m outputs)\n\u001b[1;32m    169\u001b[0m span_starts, span_ends, mention_logits, coref_logits \u001b[38;5;241m=\u001b[39m outputs_np\n\u001b[1;32m    170\u001b[0m doc_indices, mention_to_antecedent \u001b[38;5;241m=\u001b[39m create_mention_to_antecedent(span_starts, span_ends, coref_logits)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Numpy is not available"
     ]
    }
   ],
   "source": [
    "processed_df = process_dataframe(arxiv_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "triplets",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
